#!/usr/bin/env python3.13
"""
Script pour rÃ©gÃ©nÃ©rer complÃ¨tement la base vectorielle RAG avec TOUS les PDFs
"""

import os
import sys
import pickle
import numpy as np

# Configuration
SETRAF_PATH = os.path.dirname(os.path.abspath(__file__))
RAG_DOCUMENTS_PATH = os.path.join(SETRAF_PATH, "rag_documents")
VECTOR_DB_PATH = os.path.join(SETRAF_PATH, "vector_db")

print("=" * 80)
print("ğŸ”„ RÃ‰GÃ‰NÃ‰RATION COMPLÃˆTE DE LA BASE VECTORIELLE RAG")
print("=" * 80)

# 1. Extraction des PDFs
print("\nğŸ“– Ã‰TAPE 1: Extraction des PDFs")
print("-" * 80)

try:
    from pypdf import PdfReader
    
    pdf_files = [f for f in os.listdir(RAG_DOCUMENTS_PATH) if f.endswith('.pdf')]
    print(f"ğŸ“„ {len(pdf_files)} PDF(s) trouvÃ©(s): {', '.join(pdf_files)}")
    
    all_texts = []
    
    for pdf_file in pdf_files:
        pdf_path = os.path.join(RAG_DOCUMENTS_PATH, pdf_file)
        print(f"\nğŸ“– Traitement: {pdf_file}")
        
        reader = PdfReader(pdf_path)
        text = ""
        
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            page_text = page.extract_text()
            if len(page_text.strip()) > 50:
                text += page_text + "\n\n"
        
        all_texts.append({
            "title": f"PDF: {pdf_file}",
            "content": text,
            "pages": len(reader.pages),
            "source": pdf_file
        })
        print(f"  âœ… {len(reader.pages)} pages, {len(text)} caractÃ¨res")
    
except Exception as e:
    print(f"âŒ Erreur extraction: {e}")
    sys.exit(1)

# 2. Documents par dÃ©faut
print("\nğŸ“š Ã‰TAPE 2: Ajout documents par dÃ©faut")
print("-" * 80)

default_docs = [
    {
        "title": "RÃ©sistivitÃ© ERT - Ã‰chelle rapide",
        "content": """
        Ã‰CHELLE RÃ‰SISTIVITÃ‰ ERT:
        0.01-1 Î©Â·m : EAU DE MER / MINÃ‰RAUX CONDUCTEURS
        1-10 Î©Â·m : EAU SAUMÃ‚TRE / ARGILES SATURÃ‰ES
        10-100 Î©Â·m : EAU DOUCE / SOLS FINS / AQUIFÃˆRE ARGILEUX
        100-1000 Î©Â·m : SABLES SATURÃ‰S / GRAVIERS / AQUIFÃˆRE PRODUCTIF
        1000-10000 Î©Â·m : ROCHES SÃ‰DIMENTAIRES / SOCLE ALTÃ‰RÃ‰
        >10000 Î©Â·m : SOCLE CRISTALLIN / GRANITE / GNEISS
        
        MÃ‰THODES D'ACQUISITION:
        - Wenner: Bonne pÃ©nÃ©tration verticale
        - Schlumberger: Compromis rÃ©solution/profondeur
        - DipÃ´le-dipÃ´le: Haute rÃ©solution latÃ©rale
        - PÃ´le-pÃ´le: Grande profondeur d'investigation
        """,
        "source": "default"
    },
    {
        "title": "InterprÃ©tation gÃ©ophysique ERT",
        "content": """
        ANALYSE DES PSEUDO-SECTIONS:
        - ReprÃ©sentation 2D des rÃ©sistivitÃ©s apparentes
        - Identification des anomalies conductrices/rÃ©sistantes
        - CorrÃ©lation avec la gÃ©ologie locale
        
        INVERSION DE DONNÃ‰ES:
        - Transformation pseudo-section â†’ vraie rÃ©sistivitÃ©
        - ModÃ¨le 2D/3D du sous-sol
        - Contraintes gÃ©ologiques et hydrogÃ©ologiques
        
        APPLICATIONS HYDROGÃ‰OLOGIQUES:
        - DÃ©tection d'aquifÃ¨res (10-100 Î©Â·m)
        - Cartographie du socle rocheux (>1000 Î©Â·m)
        - Identification des argiles (1-10 Î©Â·m)
        - Ã‰valuation de la profondeur des formations
        """,
        "source": "default"
    }
]

all_texts.extend(default_docs)
print(f"âœ… {len(default_docs)} documents par dÃ©faut ajoutÃ©s")
print(f"ğŸ“Š TOTAL: {len(all_texts)} documents Ã  chunker")

# 3. DÃ©coupage en chunks
print("\nâœ‚ï¸ Ã‰TAPE 3: DÃ©coupage en chunks (512 caractÃ¨res)")
print("-" * 80)

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=50,
        length_function=len
    )
    
    texts = []
    metadatas = []
    
    for doc in all_texts:
        chunks = text_splitter.split_text(doc["content"])
        for chunk in chunks:
            if len(chunk.strip()) > 50:
                texts.append(chunk.strip())
                metadatas.append({
                    "title": doc["title"],
                    "source": doc.get("source", "unknown")
                })
    
    print(f"âœ… {len(texts)} chunks gÃ©nÃ©rÃ©s")
    print(f"ğŸ“ Longueur moyenne: {sum(len(t) for t in texts) // len(texts)} caractÃ¨res")
    
except Exception as e:
    print(f"âŒ Erreur chunking: {e}")
    sys.exit(1)

# 4. GÃ©nÃ©ration des embeddings
print("\nğŸ§  Ã‰TAPE 4: GÃ©nÃ©ration des embeddings (all-MiniLM-L6-v2)")
print("-" * 80)

try:
    from sentence_transformers import SentenceTransformer
    
    embeddings_path = os.path.join(SETRAF_PATH, "models/embeddings/sentence-transformers--all-MiniLM-L6-v2")
    
    if not os.path.exists(embeddings_path):
        print(f"âŒ ModÃ¨le d'embeddings non trouvÃ©: {embeddings_path}")
        sys.exit(1)
    
    print(f"ğŸ“‚ Chargement depuis: {embeddings_path}")
    embeddings_model = SentenceTransformer(embeddings_path, device='cpu')
    embeddings_model.eval()
    
    print(f"ğŸ”„ Encodage de {len(texts)} chunks...")
    
    # Traitement par batch pour la mÃ©moire
    batch_size = 32
    embeddings_list = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = embeddings_model.encode(batch_texts, show_progress_bar=True, convert_to_numpy=True)
        embeddings_list.append(batch_embeddings)
        print(f"  âœ… Batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")
    
    embeddings_array = np.vstack(embeddings_list)
    print(f"âœ… Embeddings gÃ©nÃ©rÃ©s: shape {embeddings_array.shape}")
    
except Exception as e:
    print(f"âŒ Erreur embeddings: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 5. CrÃ©ation de l'index FAISS
print("\nğŸ—„ï¸ Ã‰TAPE 5: CrÃ©ation index FAISS")
print("-" * 80)

try:
    import faiss
    
    dimension = embeddings_array.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings_array.astype('float32'))
    
    print(f"âœ… Index FAISS crÃ©Ã©: {index.ntotal} vecteurs, dimension {dimension}")
    
except Exception as e:
    print(f"âŒ Erreur FAISS: {e}")
    sys.exit(1)

# 6. Sauvegarde
print("\nğŸ’¾ Ã‰TAPE 6: Sauvegarde de la base vectorielle")
print("-" * 80)

try:
    os.makedirs(VECTOR_DB_PATH, exist_ok=True)
    
    db_file = os.path.join(VECTOR_DB_PATH, "ert_knowledge_light.faiss")
    docs_file = os.path.join(VECTOR_DB_PATH, "ert_documents_light.pkl")
    
    # Sauvegarder FAISS
    faiss.write_index(index, db_file)
    print(f"âœ… Index FAISS sauvegardÃ©: {db_file}")
    
    # Sauvegarder documents
    with open(docs_file, 'wb') as f:
        pickle.dump({
            'texts': texts,
            'metadatas': metadatas
        }, f)
    print(f"âœ… Documents sauvegardÃ©s: {docs_file}")
    
except Exception as e:
    print(f"âŒ Erreur sauvegarde: {e}")
    sys.exit(1)

# 7. VÃ©rification
print("\nâœ… Ã‰TAPE 7: VÃ©rification")
print("-" * 80)

# Test de recherche
test_query = "rÃ©sistivitÃ© de l'eau"
print(f"ğŸ” Test de recherche: '{test_query}'")

query_embedding = embeddings_model.encode([test_query], convert_to_numpy=True)
distances, indices = index.search(query_embedding.astype('float32'), k=3)

print(f"\nğŸ“Š Top 3 rÃ©sultats:")
for i, (idx, dist) in enumerate(zip(indices[0], distances[0]), 1):
    print(f"\n{i}. Distance: {dist:.4f}")
    print(f"   Chunk #{idx}: {texts[idx][:150]}...")

print("\n" + "=" * 80)
print("ğŸ‰ RÃ‰GÃ‰NÃ‰RATION COMPLÃˆTE TERMINÃ‰E!")
print("=" * 80)
print(f"ğŸ“š {len(texts)} chunks indexÃ©s")
print(f"ğŸ“Š {len(pdf_files)} PDFs traitÃ©s")
print(f"ğŸ¯ Dimension: {dimension}")
print(f"ğŸ’¾ Taille index: {os.path.getsize(db_file) / 1024:.1f} KB")
print(f"ğŸ’¾ Taille docs: {os.path.getsize(docs_file) / 1024:.1f} KB")
print("\nâœ… Le systÃ¨me RAG est maintenant prÃªt Ã  Ãªtre utilisÃ© dans Streamlit!")
