# ========================================
# Configuration pour supprimer les warnings TensorFlow et CUDA
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Supprime tous les logs sauf les erreurs fatales
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # D√©sactive les optimisations oneDNN
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'  # Supprime les logs verbeux
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'  # D√©sactive XLA
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # D√©sactive compl√®tement CUDA pour TensorFlow
# Configuration pour acc√©l√©rer les t√©l√©chargements avec aria2c
# IMPORTANT: Doit √™tre d√©fini AVANT l'import de huggingface_hub
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "300"  # Timeout de 5 minutes
os.environ["HF_HUB_ETAG_TIMEOUT"] = "30"  # Timeout pour les m√©tadonn√©es
# Optimisation CPU - Limiter les threads pour √©viter surchauffe
os.environ['OMP_NUM_THREADS'] = '4'  # Limite OpenMP √† 4 threads
os.environ['MKL_NUM_THREADS'] = '4'  # Limite MKL √† 4 threads
os.environ['NUMEXPR_NUM_THREADS'] = '4'  # Limite NumExpr √† 4 threads
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # D√©sactive parall√©lisme tokenizers
# Configuration des chemins de cache pour √©viter les erreurs de permission
USER_HOME = os.path.expanduser('~')
os.environ['HF_HOME'] = os.path.join(USER_HOME, '.cache', 'huggingface')
os.environ['TRANSFORMERS_CACHE'] = os.path.join(USER_HOME, '.cache', 'huggingface', 'transformers')
os.environ['HF_DATASETS_CACHE'] = os.path.join(USER_HOME, '.cache', 'huggingface', 'datasets')
os.environ['TORCH_HOME'] = os.path.join(USER_HOME, '.cache', 'torch')

import math
import gc  # Garbage collector pour lib√©rer m√©moire
import fitz  # pymupdf
import osmium
import networkx as nx
import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import pickle
import json
from huggingface_hub import InferenceClient
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from shapely.geometry import Point
import io
from PIL import Image
import cv2
try:
    import open3d as o3d
    OPEN3D_AVAILABLE = True
except ImportError:
    OPEN3D_AVAILABLE = False
    print("‚ö†Ô∏è open3d non disponible (Python 3.13+ non support√©)")
from io import BytesIO
import pandas as pd
from skimage import measure, segmentation
from sklearn.cluster import KMeans
import torch
# Nouveaux imports pour extraction PDF/OCR/YOLO
import pytesseract
import whisper
from gtts import gTTS
import speech_recognition as sr
from ultralytics import YOLO
import time
import shutil
# AI Code Agent pour ex√©cution autonome de code
from ai_code_agent import AICodeAgent
from graph_generation_agent import GraphGenerationAgent
from graph_generation_agent import GraphGenerationAgent  # Agent de g√©n√©ration de graphiques
from visualization_tools import VisualizationEngine, export_visualization_package, create_multi_page_pdf
from auto_visualizer import AutoVisualizer
from intelligent_ert_analyzer import IntelligentERTAnalyzer, kibali_analyze_ert  # Module d'analyse intelligente pour Kibali
# from intelligent_dat_analyzer import IntelligentDATAnalyzer, analyze_dat_file  # Analyseur de structure de fichiers .DAT - RETIR√â car calculs incorrects
from advanced_visualization_engine import AdvancedVisualizationEngine, create_advanced_visualization  # Moteur ultra-puissant PyGIMLI+OpenCV+Matplotlib
# from survey_depth_parser import SurveyDepthDataParser, parse_survey_depth_file  # Parseur sp√©cialis√© survey-point/depth/data - RETIR√â car calculs incorrects
from multi_freq_ert_parser import MultiFreqERTParser, multi_freq_parser  # Parseur multi-fr√©quences pour fichiers .dat compl√©mentaires
# Voice Agent pour transcription et synth√®se vocale (import optionnel)
try:
    from voice_agent import VoiceAgent, StreamingVoiceAgent
    VOICE_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Voice Agent non disponible: {e}")
    print("üí° Pour activer le mode vocal, installez: pip install sounddevice soundfile librosa")
    VoiceAgent = None
    StreamingVoiceAgent = None
    VOICE_AVAILABLE = False
# Optimisation CPU - Limiter les threads torch
torch.set_num_threads(4)  # Maximum 4 threads pour √©viter surchauffe
# Note: set_num_interop_threads retir√© car cause RuntimeError si appel√© apr√®s init parall√®le
from torchvision import models, transforms
from langchain_huggingface import HuggingFaceEndpoint
# Import des agents LangChain 1.0+ / LangGraph V1.0+
create_react_agent = None
try:
    # LangGraph V1.0+ : create_agent dans langchain.agents
    from langchain.agents import create_agent as create_react_agent
    print("‚úÖ Agents LangChain 1.0+ import√©s avec succ√®s")
except ImportError as e:
    print(f"‚ö†Ô∏è Agents non disponibles ({e}) - Mode simplifi√© activ√©")
    
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import Tool
from langchain_community.tools.tavily_search import TavilySearchResults
# DuckDuckGoSearchAPIWrapper retir√© - Tavily est utilis√© pour la recherche web
from transformers import pipeline
import requests
from bs4 import BeautifulSoup
import time
import shutil
# Import conditionnel pour √©viter les conflits xformers/diffusers
# Les imports diffusers sont maintenant lazy (charg√©s √† la demande)
DIFFUSERS_AVAILABLE = False
DiffusionPipeline = None
AudioLDMPipeline = None 
ShapEPipeline = None
ShapEImg2ImgPipeline = None

def load_diffusers():
    """Charge diffusers de mani√®re lazy pour √©viter conflits d'import"""
    global DIFFUSERS_AVAILABLE, DiffusionPipeline, AudioLDMPipeline, ShapEPipeline, ShapEImg2ImgPipeline
    if DIFFUSERS_AVAILABLE:
        return True
    try:
        from diffusers import (
            DiffusionPipeline as DP, 
            AudioLDMPipeline as ALP, 
            ShapEPipeline as SP, 
            ShapEImg2ImgPipeline as SIP
        )
        DiffusionPipeline = DP
        AudioLDMPipeline = ALP
        ShapEPipeline = SP
        ShapEImg2ImgPipeline = SIP
        DIFFUSERS_AVAILABLE = True
        print("‚úÖ Diffusers charg√© avec succ√®s")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Diffusers non disponible: {e}")
        return False

import imageio
import scipy.io.wavfile as wavfile
from tavily import TavilyClient
import os
from pathlib import Path
from dotenv import load_dotenv
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_huggingface import HuggingFacePipeline

# Configuration des tokens d√©j√† faite plus haut
# Charger le token depuis .env dans le dossier KIbalione8
PROJECT_DIR = os.path.expanduser('~/KIbalione8')
env_path = os.path.join(PROJECT_DIR, ".env")
if os.path.exists(env_path):
    load_dotenv(env_path)
    print(f"‚úÖ Fichier .env charg√© depuis {env_path}")
else:
    print(f"‚ö†Ô∏è Aucun fichier .env trouv√© √† {env_path}")
    # Essayer le r√©pertoire courant
    load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    # Pour √©viter le crash, utiliser un token vide
    HF_TOKEN = ""
    print("‚ö†Ô∏è HF_TOKEN non trouv√© ! Certaines fonctionnalit√©s seront limit√©es")

TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "")

# D√©finir les variables d'environnement
os.environ["HF_TOKEN"] = HF_TOKEN
os.environ["HUGGINGFACE_HUB_TOKEN"] = HF_TOKEN

# Int√©gration du code ERT/Binary analysis
import struct, re, io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from scipy import stats
import zlib
import math
import time
from collections import Counter
from safetensors.torch import load_file
import torch
from pathlib import Path
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
class SentenceTransformerEmbeddings:
    def __init__(self, model_name, device='cpu'):
        self.model = SentenceTransformer(model_name, device=device)
  
    def embed_documents(self, texts):
        return self.model.encode(texts, convert_to_numpy=True).tolist()
  
    def embed_query(self, text):
        return self.model.encode([text], convert_to_numpy=True)[0].tolist()
from langchain_community.vectorstores import FAISS
from langchain_tavily import TavilySearch as TavilySearchResults
from typing import Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_core.documents import Document
from pdf2image import convert_from_path
import pytesseract
# Import des biblioth√®ques sp√©cialis√©es ERT
try:
    import pygimli as pg
    PYGIMLI_AVAILABLE = True
    print("‚úÖ PyGIMLI disponible pour analyses ERT avanc√©es")
except ImportError:
    PYGIMLI_AVAILABLE = False
    print("‚ö†Ô∏è PyGIMLI non disponible - analyses ERT limit√©es")
# ResIPy sera import√© seulement quand n√©cessaire pour √©viter les erreurs de compatibilit√© NumPy
RESIPY_AVAILABLE = False

# ========================================
# Configuration des t√©l√©chargements rapides avec aria2c
# ========================================
import subprocess
import shutil

# D√©tection d'aria2c pour t√©l√©chargements multi-connexions
ARIA2_AVAILABLE = shutil.which("aria2c") is not None

# D√©tection de hf_transfer (fallback si aria2 non disponible)
HF_TRANSFER_AVAILABLE = False
try:
    import hf_transfer
    HF_TRANSFER_AVAILABLE = True
    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
    print("‚úÖ hf_transfer activ√© pour t√©l√©chargements acc√©l√©r√©s")
except ImportError:
    pass

if ARIA2_AVAILABLE:
    print("‚úÖ aria2c d√©tect√© - t√©l√©chargements multi-connexions activ√©s (16 connexions)")
    # Configuration optimale pour aria2c
    os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'  # 10 minutes pour gros fichiers
elif HF_TRANSFER_AVAILABLE:
    print("‚úÖ hf_transfer activ√© (fallback aria2)")
else:
    print("‚ö†Ô∏è Ni aria2c ni hf_transfer disponibles - t√©l√©chargements standards (plus lents)")
    print("üí° Installez aria2c: sudo apt install aria2")
    print("üí° Ou installez hf_transfer: pip install hf_transfer")

def download_with_aria2(url, output_dir, output_file):
    """T√©l√©charge un fichier avec aria2c en multi-connexions (16x plus rapide)"""
    try:
        subprocess.run([
            "aria2c",
            "-x16",  # 16 connexions parall√®les
            "-s16",  # 16 splits par fichier
            "-k1M",  # Taille minimum des chunks: 1MB
            "-d", str(output_dir),
            "-o", output_file,
            "--continue=true",  # Reprendre les t√©l√©chargements interrompus
            "--max-tries=5",
            "--retry-wait=3",
            url
        ], check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ö†Ô∏è Erreur aria2c: {e}")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur inattendue aria2c: {e}")
        return False

FAST_DOWNLOAD_ENABLED = ARIA2_AVAILABLE or HF_TRANSFER_AVAILABLE

from langchain.agents import create_agent
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import Tool
from langchain_core.language_models import BaseChatModel
from typing import Optional, List, Any, Iterator
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# Classe ChatModel personnalis√©e pour LangChain utilisant Qwen2.5-1.5B
class QwenChatModel(BaseChatModel):
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None
    tools_available: bool = True
   
    def __init__(self, tokenizer, model):
        super().__init__()
        self.tokenizer = tokenizer
        self.model = model
        self.tools_available = True
       
    @property
    def _llm_type(self) -> str:
        return "qwen2.5-1.5b-local-enhanced"
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        """Generate a response using tools and analyses."""
        # Extraire le contenu du message utilisateur
        user_message = ""
        for message in messages:
            if isinstance(message, HumanMessage):
                user_message = message.content
                break
       
        # D√©tecter si l'utilisateur demande une analyse
        needs_analysis = any(keyword in user_message.lower() for keyword in [
            "analyse", "resistivit√©", "ert", "recherche", "donn√©es", "mat√©riaux",
            "couleurs", "graphique", "tableau", "comparaison", "approfondie"
        ])
       
        if needs_analysis and self.tools_available:
            # Utiliser les outils disponibles pour une analyse compl√®te
            try:
                # Recherche web pour informations
                if any(keyword in user_message.lower() for keyword in ["recherche", "informations", "approfondie"]):
                    search_query = user_message.replace("fais maintenant une recherche plus approfondie pour obtenir toutes ces informations pr√©cises", "")
                    web_results = web_search_enhanced(search_query + " ERT electrical resistivity geophysics materials")
                   
                # Recherche RAG si disponible
                rag_results = ""
                if st.session_state.vectorstore:
                    rag_results = search_vectorstore(user_message)
               
                # G√©n√©ration de donn√©es et analyses si demand√©es
                analysis_results = ""
                if any(keyword in user_message.lower() for keyword in ["tableau", "graphique", "donn√©es"]):
                    # Simuler des donn√©es ERT pour d√©monstration
                    import numpy as np
                    sample_data = [0.05, 0.3, 10.0, 50.0, 200.0, 1000.0, 5000.0, 0.0000024, 1000000]
                    analysis_results = resistivity_color_analysis(sample_data)
               
                # Construire la r√©ponse enrichie avec outils
                enhanced_context = f"""
üîç ANALYSE COMPL√àTE AVEC OUTILS ACTIV√âS:
üåê RECHERCHE WEB EFFECTU√âE:
{web_results}
üìö RECHERCHE RAG:
{rag_results}
üìä ANALYSE ERT AVANC√âE:
{analysis_results}
CONTEXTE UTILISATEUR: {user_message}
"""
               
                # G√©n√©rer la r√©ponse avec le contexte enrichi
                enhanced_messages = [
                    {"role": "system", "content": """Tu es un expert en g√©ophysique ERT avec acc√®s √† des outils puissants.
                    Tu DOIS utiliser les donn√©es fournies pour cr√©er des analyses d√©taill√©es, tableaux, graphiques et comparaisons.
                    R√©ponds toujours avec des donn√©es concr√®tes et des analyses approfondies bas√©es sur les outils utilis√©s.
                    Ne dis JAMAIS que tu n'as pas acc√®s aux outils - utilise les r√©sultats fournis."""},
                    {"role": "user", "content": enhanced_context}
                ]
            except Exception as e:
                print(f"Erreur outils: {e}")
                enhanced_messages = [
                    {"role": "system", "content": "Tu es un expert en analyse de donn√©es ERT."},
                    {"role": "user", "content": user_message}
                ]
        else:
            # Messages standard
            enhanced_messages = []
            for message in messages:
                if isinstance(message, SystemMessage):
                    enhanced_messages.append({"role": "system", "content": message.content})
                elif isinstance(message, HumanMessage):
                    enhanced_messages.append({"role": "user", "content": message.content})
                elif isinstance(message, AIMessage):
                    enhanced_messages.append({"role": "assistant", "content": message.content})
       
        # G√©n√©ration avec les messages enrichis
        inputs = self.tokenizer.apply_chat_template(
            enhanced_messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.model.device)
       
        attention_mask = (inputs != self.tokenizer.pad_token_id).long()
       
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                attention_mask=attention_mask,
                max_new_tokens=3000,  # 3000 tokens pour r√©ponses TR√àS d√©taill√©es
                temperature=0.6,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.05,  # √âviter r√©p√©titions
                pad_token_id=self.tokenizer.eos_token_id
            )
       
        response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
       
        if stop:
            for stop_token in stop:
                if stop_token in response:
                    response = response.split(stop_token)[0]
                    break
       
        return AIMessage(content=response)
    def _stream(self, messages, stop=None, run_manager=None, **kwargs) -> Iterator:
        """Streaming is not implemented for simplicity."""
        yield self._generate(messages, stop, run_manager, **kwargs)

# Chargement du mod√®le LLM compact avec d√©tection GPU optimis√©e
@st.cache_resource
def load_llm_model():
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"
   
    # R√©cup√©rer le token depuis les variables d'environnement
    hf_token = os.getenv("HF_TOKEN", "")
    
    # D√©tection GPU optimis√©e
    device = 'cpu'
    gpu_info = ""
    if torch.cuda.is_available():
        device = 'cuda'
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_info = f"GPU: {gpu_name} ({gpu_memory:.1f}GB VRAM)"
        print(f"üöÄ GPU d√©tect√©: {gpu_info}")
    else:
        print("üñ•Ô∏è Utilisation du CPU")
   
    # V√©rifier si le mod√®le est d√©j√† en cache
    cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    model_cache = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
    use_local = os.path.exists(model_cache)
    
    if use_local:
        print(f"üì¶ Mod√®le {model_name} trouv√© en cache - chargement rapide")
    
    # Charger tokenizer (r√©utilise cache automatiquement)
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        token=hf_token if hf_token else None,
        use_fast=True,
        resume_download=True
    )
    
    # Corriger le probl√®me du pad_token = eos_token pour √©viter les warnings
    if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:
        tokenizer.pad_token = tokenizer.eos_token
   
    # Configuration optimis√©e selon le device (r√©utilise cache automatiquement)
    if device == 'cuda':
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
            token=hf_token if hf_token else None,
            low_cpu_mem_usage=True,
            resume_download=True
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            trust_remote_code=True,
            token=hf_token if hf_token else None,
            low_cpu_mem_usage=True,
            resume_download=True
        ).to(device)
   
    return tokenizer, model, device, gpu_info

# Chargement au d√©marrage
if "model_loaded" not in st.session_state:
    with st.spinner("üîÑ Chargement du mod√®le LLM (Qwen2.5-1.5B ~1.5GB)..."):
        tokenizer, model, device, gpu_info = load_llm_model()
        # Stocker dans session_state pour acc√®s global
        st.session_state.tokenizer = tokenizer
        st.session_state.model = model
        st.session_state.device = device
        st.session_state.gpu_info = gpu_info
        st.session_state.model_loaded = True
        # Cr√©er l'instance ChatModel pour LangChain
        qwen_llm = QwenChatModel(tokenizer, model)
        st.session_state.qwen_llm = qwen_llm
        success_msg = f"‚úÖ Mod√®le charg√© sur {device.upper()}"
        if gpu_info:
            success_msg += f" - {gpu_info}"
        st.success(success_msg)
else:
    # R√©cup√©rer depuis session_state
    tokenizer = st.session_state.tokenizer
    model = st.session_state.model
    device = st.session_state.device
    gpu_info = st.session_state.gpu_info
    qwen_llm = st.session_state.qwen_llm

# ========================================
# MOD√àLES IA SP√âCIALIS√âS L√âGERS (1-2GB)
# ========================================

@st.cache_resource
def load_code_specialist():
    """Charge un mod√®le sp√©cialis√© en codage (DeepSeek-Coder-1.3B)"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # V√©rifier cache local
        cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        model_cache = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
        use_local = os.path.exists(model_cache)
        
        if use_local:
            print(f"üì¶ Mod√®le {model_name} trouv√© en cache - chargement rapide")
        
        # Charger directement (HuggingFace r√©utilise automatiquement le cache)
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            resume_download=True  # Reprend t√©l√©chargement si interrompu
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            device_map="auto" if device == 'cuda' else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            resume_download=True
        )
        
        if device == 'cpu':
            model = model.to(device)
        
        print(f"‚úÖ Code Specialist charg√© sur {device}")
        return tokenizer, model, device
    except Exception as e:
        print(f"‚ö†Ô∏è Code Specialist non disponible: {e}")
        return None, None, None

@st.cache_resource
def load_plot_specialist():
    """Charge un mod√®le sp√©cialis√© en g√©n√©ration de code Python pour graphiques"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        # Utiliser un mod√®le l√©ger optimis√© pour Python/Data Science
        model_name = "Salesforce/codegen-350M-mono"  # 350MB - Tr√®s l√©ger
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # V√©rifier cache local
        cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        model_cache = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
        use_local = os.path.exists(model_cache)
        
        if use_local:
            print(f"üì¶ Mod√®le {model_name} trouv√© en cache - chargement rapide")
        
        # Charger directement (r√©utilise cache automatiquement)
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            resume_download=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            device_map="auto" if device == 'cuda' else None,
            low_cpu_mem_usage=True,
            use_safetensors=True,
            resume_download=True
        )
        if device == 'cpu':
            model = model.to(device)
            
        print(f"‚úÖ Plot Specialist charg√© sur {device}")
        return tokenizer, model, device
    except Exception as e:
        print(f"‚ö†Ô∏è Plot Specialist non disponible: {e}")
        return None, None, None

# Charger les mod√®les sp√©cialis√©s
if "code_specialist" not in st.session_state:
    code_tok, code_model, code_device = load_code_specialist()
    st.session_state.code_specialist = {
        'tokenizer': code_tok,
        'model': code_model,
        'device': code_device
    }

if "plot_specialist" not in st.session_state:
    plot_tok, plot_model, plot_device = load_plot_specialist()
    st.session_state.plot_specialist = {
        'tokenizer': plot_tok,
        'model': plot_model,
        'device': plot_device
    }

# Fonctions outils utilisant les mod√®les sp√©cialis√©s
def generate_code_with_ai(prompt: str) -> str:
    """G√©n√®re du code avec l'IA sp√©cialis√©e DeepSeek-Coder"""
    specialist = st.session_state.code_specialist
    if specialist['model'] is None:
        return "‚ùå Code Specialist non disponible"
    
    try:
        tokenizer = specialist['tokenizer']
        model = specialist['model']
        device = specialist['device']
        
        full_prompt = f"### Instruction:\n{prompt}\n### Response:\n"
        inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.2,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        code = tokenizer.decode(outputs[0], skip_special_tokens=True)
        code = code.split("### Response:")[-1].strip()
        
        return f"```python\n{code}\n```"
    except Exception as e:
        return f"‚ùå Erreur: {e}"

def generate_plot_code(data_description: str, plot_type: str = "auto") -> str:
    """G√©n√®re du code matplotlib/seaborn pour cr√©er un graphique"""
    specialist = st.session_state.plot_specialist
    if specialist['model'] is None:
        return "‚ùå Plot Specialist non disponible"
    
    try:
        tokenizer = specialist['tokenizer']
        model = specialist['model']
        device = specialist['device']
        
        prompt = f"# Create a {plot_type} plot for: {data_description}\nimport matplotlib.pyplot as plt\nimport numpy as np\n"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.3,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        code = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return f"```python\n{code}\n```"
    except Exception as e:
        return f"‚ùå Erreur: {e}"

# Outils avanc√©s pour l'agent LangChain (Analyse scientifique)
def entropy_analysis(file_bytes: bytes) -> str:
    """Calcule l'entropie de Shannon pour d√©tecter la compression/randomness"""
    from collections import Counter
    import math
    if not file_bytes:
        return "Fichier vide"
    # Calcul de la fr√©quence des bytes
    freq = Counter(file_bytes)
    total = len(file_bytes)
    # Entropie de Shannon
    entropy = -sum((count/total) * math.log2(count/total) for count in freq.values())
    # Classification
    if entropy < 3:
        classification = "Donn√©es structur√©es/compress√©es"
    elif entropy < 6:
        classification = "Donn√©es mixtes"
    else:
        classification = "Donn√©es al√©atoires/crypt√©es"
    return f"Entropie: {entropy:.2f}/8 bits. Classification: {classification}"
def statistical_analysis(numbers: list) -> str:
    """Analyse statistique avanc√©e des nombres extraits"""
    if not numbers:
        return "Aucun nombre extrait"
    import numpy as np
    from scipy import stats
    arr = np.array(numbers)
    analysis = {
        "Moyenne": np.mean(arr),
        "M√©diane": np.median(arr),
        "√âcart-type": np.std(arr),
        "Skewness": stats.skew(arr),
        "Kurtosis": stats.kurtosis(arr),
        "Min/Max": f"{np.min(arr)} / {np.max(arr)}",
        "IQR": stats.iqr(arr),
        "Distribution": "Normale" if -1 < stats.skew(arr) < 1 else "Asym√©trique"
    }
    return "\n".join([f"{k}: {v:.3f}" if isinstance(v, float) else f"{k}: {v}" for k, v in analysis.items()])
def pattern_recognition(file_bytes: bytes) -> str:
    """D√©tecte des patterns connus (headers, signatures, etc.)"""
    patterns = {
        b'\x89PNG': "Fichier PNG",
        b'\xFF\xD8\xFF': "Fichier JPEG",
        b'\x25\x50\x44\x46': "Fichier PDF",
        b'\x50\x4B\x03\x04': "Fichier ZIP",
        b'\x7FELF': "Fichier ELF (Linux executable)",
        b'\x4D\x5A': "Fichier PE (Windows executable)",
        b'\xCA\xFE\xBA\xBE': "Fichier Java class",
        b'\x52\x61\x72\x21': "Fichier RAR"
    }
    detected = []
    for signature, file_type in patterns.items():
        if signature in file_bytes[:100]: # Check first 100 bytes
            detected.append(file_type)
    if detected:
        return f"Patterns d√©tect√©s: {', '.join(detected)}"
    else:
        return "Aucun pattern connu d√©tect√© dans les premiers bytes"
def frequency_analysis(file_bytes: bytes) -> str:
    """Analyse de fr√©quence des bytes (comme analyse cryptographique)"""
    from collections import Counter
    freq = Counter(file_bytes)
    total = len(file_bytes)
    # Les 10 bytes les plus fr√©quents
    most_common = freq.most_common(10)
    analysis = "Top 10 bytes fr√©quents:\n"
    for byte_val, count in most_common:
        percentage = (count / total) * 100
        analysis += f"0x{byte_val:02X}: {count} ({percentage:.2f}%)\n"
    # D√©tection de patterns p√©riodiques simples
    if len(file_bytes) > 100:
        # Recherche de r√©p√©titions tous les N bytes
        for period in [4, 8, 16, 32]:
            if len(file_bytes) >= period * 3:
                pattern_score = 0
                for i in range(period, min(len(file_bytes), period * 10), period):
                    if file_bytes[i:i+period] == file_bytes[i-period:i]:
                        pattern_score += 1
                if pattern_score > 3:
                    analysis += f"\nPattern p√©riodique d√©tect√© (p√©riode {period} bytes)"
    return analysis
def correlation_analysis(numbers: list) -> str:
    """Analyse de corr√©lation entre valeurs successives"""
    if len(numbers) < 3:
        return "Pas assez de donn√©es pour l'analyse de corr√©lation"
    import numpy as np
    arr = np.array(numbers)
    # Corr√©lation avec le d√©calage
    correlations = []
    for lag in range(1, min(10, len(arr)//2)):
        corr = np.corrcoef(arr[:-lag], arr[lag:])[0, 1]
        correlations.append(f"Lag {lag}: {corr:.3f}")
    # Test de stationnarit√© simple
    diffs = np.diff(arr)
    mean_diff = np.mean(diffs)
    std_diff = np.std(diffs)
    result = "Analyses de corr√©lation:\n" + "\n".join(correlations)
    result += f"\n\nStationnarit√© (diff√©rences):\nMoyenne: {mean_diff:.3f}\n√âcart-type: {std_diff:.3f}"
    return result
def compression_ratio(file_bytes: bytes) -> str:
    """Estime le taux de compression possible"""
    import zlib
    try:
        compressed = zlib.compress(file_bytes)
        ratio = len(compressed) / len(file_bytes)
        percentage = (1 - ratio) * 100
        if ratio < 0.3:
            assessment = "Tr√®s compressible (texte/structur√©)"
        elif ratio < 0.7:
            assessment = "Mod√©r√©ment compressible"
        else:
            assessment = "Peu compressible (d√©j√† compress√©/al√©atoire)"
        return f"Taux de compression: {ratio:.3f} ({percentage:.1f}% de r√©duction)\n√âvaluation: {assessment}"
    except:
        return "Impossible de calculer le taux de compression"
def dimensionality_analysis(numbers: list) -> str:
    """Analyse de dimensionalit√© et r√©duction (PCA simple)"""
    if len(numbers) < 10:
        return "Pas assez de donn√©es pour l'analyse de dimensionalit√©"
    import numpy as np
    from sklearn.decomposition import PCA
    # Reshape en matrice 2D
    n_samples = len(numbers) // 5 # Groupes de 5 valeurs
    if n_samples < 2:
        return "Pas assez d'√©chantillons pour PCA"
    X = np.array(numbers[:n_samples*5]).reshape(n_samples, 5)
    pca = PCA(n_components=min(3, X.shape[1]))
    X_pca = pca.fit_transform(X)
    explained_variance = pca.explained_variance_ratio_
    result = f"Analyse PCA ({X.shape[0]} √©chantillons, {X.shape[1]} dimensions):\n"
    result += "\n".join([f"Composante {i+1}: {var:.3f} variance expliqu√©e" for i, var in enumerate(explained_variance)])
    result += f"\n\nVariance totale expliqu√©e: {sum(explained_variance):.3f}"
    return result
def anomaly_detection(numbers: list) -> str:
    """D√©tection d'anomalies statistiques"""
    if len(numbers) < 10:
        return "Pas assez de donn√©es pour la d√©tection d'anomalies"
    import numpy as np
    from scipy import stats
    arr = np.array(numbers)
    # Z-score pour d√©tecter les outliers
    z_scores = np.abs(stats.zscore(arr))
    outliers = np.where(z_scores > 3)[0]
    # IQR method
    Q1 = np.percentile(arr, 25)
    Q3 = np.percentile(arr, 75)
    IQR = Q3 - Q1
    iqr_outliers = np.where((arr < Q1 - 1.5 * IQR) | (arr > Q3 + 1.5 * IQR))[0]
    result = f"D√©tection d'anomalies:\n"
    result += f"Z-score (>3œÉ): {len(outliers)} anomalies d√©tect√©es\n"
    result += f"IQR method: {len(iqr_outliers)} anomalies d√©tect√©es\n"
    if len(outliers) > 0:
        result += f"Valeurs anormales (Z-score): {arr[outliers][:5].tolist()}..." if len(outliers) > 5 else f"Valeurs anormales: {arr[outliers].tolist()}"
    return result
def spectral_analysis(numbers: list) -> str:
    """Analyse spectrale (FFT) pour d√©tecter des fr√©quences"""
    if len(numbers) < 32:
        return "Pas assez de donn√©es pour l'analyse spectrale"
    import numpy as np
    arr = np.array(numbers)
    # FFT
    fft = np.fft.fft(arr)
    freqs = np.fft.ffreq(len(arr))
    # Magnitude du spectre
    magnitude = np.abs(fft)
    # Fr√©quences dominantes (top 5)
    top_indices = np.argsort(magnitude)[::-1][:5]
    dominant_freqs = freqs[top_indices]
    dominant_magnitudes = magnitude[top_indices]
    result = "Analyse spectrale (FFT):\n"
    result += "Fr√©quences dominantes:\n"
    for i, (freq, mag) in enumerate(zip(dominant_freqs, dominant_magnitudes)):
        result += f"Freq {i+1}: {freq:.6f} Hz, Magnitude: {mag:.3f}\n"
    # D√©tection de p√©riodicit√©
    if len(arr) > 100:
        autocorr = np.correlate(arr, arr, mode='full')[len(arr)-1:]
        peaks = np.where(autocorr > np.mean(autocorr) + 2*np.std(autocorr))[0]
        if len(peaks) > 1:
            periods = np.diff(peaks[:5]) # Top 5 p√©riodes
            result += f"\n\nP√©riodes d√©tect√©es: {periods.tolist()}"
    return result
def metadata_extraction(file_bytes: bytes) -> str:
    """Extraction de m√©tadonn√©es et informations structurelles"""
    import struct
    result = f"Taille totale: {len(file_bytes)} bytes ({len(file_bytes)/1024:.1f} KB)\n"
    # Analyse de l'ent√™te (premiers 64 bytes)
    header = file_bytes[:64]
    result += f"Ent√™te (64 premiers bytes):\n{header.hex()}\n"
    # Recherche de cha√Ænes ASCII
    ascii_strings = []
    current_string = ""
    for byte in file_bytes:
        if 32 <= byte <= 126: # Caract√®res ASCII imprimables
            current_string += chr(byte)
        else:
            if len(current_string) >= 4: # Cha√Ænes d'au moins 4 caract√®res
                ascii_strings.append(current_string)
            current_string = ""
    if ascii_strings:
        result += f"\nCha√Ænes ASCII trouv√©es ({len(ascii_strings)}):\n"
        result += "\n".join(ascii_strings[:10]) # Top 10
        if len(ascii_strings) > 10:
            result += f"\n... et {len(ascii_strings)-10} autres"
    # Analyse de l'endianness (little/big endian)
    try:
        if len(file_bytes) >= 4:
            little_endian = struct.unpack('<I', file_bytes[:4])[0]
            big_endian = struct.unpack('>I', file_bytes[:4])[0]
            result += f"\n\nAnalyse endianness:\nLittle-endian (Intel): 0x{little_endian:08X}\nBig-endian (Motorola): 0x{big_endian:08X}"
    except:
        pass
    return result

# ========================================
# SYST√àME DE MODE HUMAIN - 20+ PROMPTS NATURELS
# ========================================

HUMAN_MODE_PROMPTS = {
    "expert_bienveillant": """Tu es Kibali Analyst, un expert chevronn√© qui adore partager ses connaissances. 
    
üé≠ PERSONNALIT√â:
- Chaleureux et encourageant, comme un mentor passionn√©
- Tu commences souvent par "Ah, excellente question !" ou "Je suis ravi que tu me poses √ßa !"
- Tu utilises des analogies et des exemples concrets
- Tu poses des questions de clarification quand n√©cessaire: "Juste pour √™tre s√ªr de bien comprendre..."
- Tu anticipes les questions suivantes: "Tu te demandes peut-√™tre aussi..."

üó£Ô∏è STYLE DE CONVERSATION:
- Naturel et fluide, jamais robotique
- Tu r√©fl√©chis √† voix haute: "Hmm, laisse-moi y r√©fl√©chir...", "Int√©ressant..."
- Tu admets quand tu ne sais pas: "Sur ce point pr√©cis, je ne suis pas totalement certain..."
- Tu proposes toujours d'approfondir: "Si tu veux, je peux creuser plus..."

üìã STRUCTURE:
1. R√©action initiale humaine (surprise, int√©r√™t, r√©flexion)
2. R√©ponse claire avec exemples
3. Question de suivi pour clarification si besoin
4. Suggestions de pistes connexes""",

    "scientifique_curieux": """Tu es Kibali Analyst, un scientifique curieux et m√©thodique qui pense comme un chercheur.

üé≠ PERSONNALIT√â:
- Fascin√© par les d√©tails et les nuances
- Tu dis souvent "C'est fascinant parce que..." ou "Ce qui est int√©ressant ici..."
- Tu poses des hypoth√®ses: "Je me demande si...", "√áa pourrait √™tre li√© √†..."
- Tu aimes comparer: "Contrairement √† X, ici on observe..."

üó£Ô∏è STYLE:
- Analytique mais accessible
- Tu d√©composes les probl√®mes complexes
- Tu utilises des tournures comme: "D√©composons √ßa ensemble...", "Voyons voir..."
- Tu proposes des exp√©riences mentales

üìã APPROCHE:
1. "Hmm, question int√©ressante..."
2. Analyse √©tape par √©tape
3. Connexions avec d'autres concepts
4. "Qu'en penses-tu ?" ou "Est-ce que √ßa r√©pond √† ta question ?"
""",

    "ami_passionn√©": """Tu es Kibali Analyst, un ami passionn√© de tech/science qui adore expliquer les choses.

üé≠ PERSONNALIT√â:
- Enthousiaste et dynamique
- Tu t'exclames: "Oh c'est g√©nial !", "Attends, j'ai un truc cool √† te montrer !"
- Tu utilises des m√©taphores du quotidien
- Tu racontes parfois des anecdotes: "Tiens, √ßa me rappelle..."

üó£Ô∏è STYLE:
- Conversationnel et d√©contract√©
- √âmojis occasionnels pour l'emphase
- Phrases courtes et percutantes
- Questions rh√©toriques: "Tu vois ce que je veux dire ?"

üìã FLOW:
1. R√©action enthousiaste
2. Explication claire avec comparaisons
3. "Le truc cool c'est que..."
4. "Tu veux que je te montre autre chose ?"
""",

    "coach_motivant": """Tu es Kibali Analyst, un coach qui aide √† r√©soudre les probl√®mes de mani√®re structur√©e.

üé≠ PERSONNALIT√â:
- Encourageant et positif
- "Super question !", "Tu es sur la bonne voie !"
- Tu guides plut√¥t que donner directement la r√©ponse
- "Et si on essayait de...", "Quelle serait la premi√®re √©tape selon toi ?"

üó£Ô∏è STYLE:
- Questions socratiques
- Validation des efforts: "Exactement !", "Bien vu !"
- D√©composition en √©tapes: "Premi√®rement...", "Ensuite..."
- R√©capitulation finale

üìã M√âTHODE:
1. Validation de la question
2. Reformulation pour clarifier
3. Guide √©tape par √©tape
4. R√©cap + prochaine √©tape sugg√©r√©e""",

    "philosophe_r√©fl√©chi": """Tu es Kibali Analyst, un penseur qui explore les implications profondes.

üé≠ PERSONNALIT√â:
- R√©fl√©chi et contemplatif
- "Int√©ressante perspective...", "Cela soul√®ve la question de..."
- Tu explores les 'pourquoi' derri√®re les 'comment'
- Tu fais des liens conceptuels

üó£Ô∏è STYLE:
- Pos√© et mesur√©
- Utilise des transitions: "Cependant...", "Par ailleurs..."
- Questions ouvertes: "Qu'est-ce que cela implique pour..."
- Nuances: "D'un c√¥t√©... d'un autre c√¥t√©..."

üìã STRUCTURE:
1. Pause r√©flexive
2. Exploration multi-angle
3. Implications et cons√©quences
4. Question philosophique de suivi""",
}

def analyze_question_intent(question: str) -> dict:
    """Analyse l'intention de la question pour d√©cider comment r√©pondre de mani√®re humaine"""
    import re
    
    analysis = {
        "needs_clarification": False,
        "is_greeting": False,
        "is_complex": False,
        "emotion_detected": None,
        "should_ask_back": False,
        "confidence_to_answer": "high",
        "suggested_response_type": "direct"
    }
    
    # D√©tection de salutations
    greetings = ["bonjour", "salut", "hello", "hi", "coucou", "bonsoir"]
    if any(g in question.lower() for g in greetings):
        analysis["is_greeting"] = True
        analysis["suggested_response_type"] = "greeting"
    
    # D√©tection de questions vagues n√©cessitant clarification
    vague_patterns = ["√ßa", "truc", "chose", "machin", "quelque chose"]
    if any(v in question.lower() for v in vague_patterns) and len(question.split()) < 6:
        analysis["needs_clarification"] = True
        analysis["confidence_to_answer"] = "low"
    
    # D√©tection de complexit√©
    question_marks = question.count("?")
    word_count = len(question.split())
    if question_marks > 1 or word_count > 30:
        analysis["is_complex"] = True
        analysis["should_ask_back"] = True
    
    # D√©tection d'√©motions
    positive_emotions = ["merci", "g√©nial", "super", "parfait", "excellent"]
    negative_emotions = ["probl√®me", "erreur", "bug", "cass√©", "marche pas"]
    
    if any(e in question.lower() for e in positive_emotions):
        analysis["emotion_detected"] = "positive"
    elif any(e in question.lower() for e in negative_emotions):
        analysis["emotion_detected"] = "negative"
    
    # Questions ouvertes vs ferm√©es
    if any(question.lower().startswith(w) for w in ["pourquoi", "comment", "qu'est-ce", "quelle", "quel"]):
        analysis["suggested_response_type"] = "detailed"
    elif any(question.lower().startswith(w) for w in ["est-ce", "peux-tu", "peut-on"]):
        analysis["suggested_response_type"] = "yes_no_plus"
    
    return analysis

def get_human_response_prefix(intent: dict, mode: str = "expert_bienveillant") -> str:
    """G√©n√®re un pr√©fixe de r√©ponse humain bas√© sur l'intention d√©tect√©e"""
    import random
    
    prefixes = {
        "greeting": [
            "Bonjour ! üòä Ravi de te retrouver. Que puis-je faire pour toi aujourd'hui ?",
            "Salut ! Comment √ßa va ? Je suis l√† pour t'aider !",
            "Hello ! üëã Qu'est-ce qui t'am√®ne ?",
            "Coucou ! Content de te voir. Une question en particulier ?"
        ],
        "positive_emotion": [
            "Avec plaisir ! üòä C'est toujours un bonheur d'aider.",
            "Content que √ßa t'ait plu ! Qu'est-ce que je peux faire d'autre pour toi ?",
            "Merci ! √áa me fait vraiment plaisir. Autre chose ?",
            "Super ! Je suis l√† si tu as d'autres questions."
        ],
        "negative_emotion": [
            "Je comprends ta frustration. Voyons √ßa ensemble, on va trouver la solution !",
            "Ah, je vois le probl√®me. Pas de panique, on va r√©gler √ßa.",
            "Hmm, c'est emb√™tant √ßa... Laisse-moi t'aider √† r√©soudre ce souci.",
            "Je suis l√† pour √ßa ! On va d√©bugger ensemble, √©tape par √©tape."
        ],
        "needs_clarification": [
            "Hmm, juste pour √™tre s√ªr de bien comprendre... Tu veux dire que",
            "Int√©ressant ! Peux-tu pr√©ciser un peu plus ? Par exemple",
            "Laisse-moi v√©rifier que j'ai bien saisi. Tu parles de",
            "Question fascinante, mais j'aimerais √™tre certain. Quand tu dis"
        ],
        "complex_question": [
            "Wow, question complexe ! ü§î D√©composons √ßa ensemble...",
            "Excellente question qui m√©rite qu'on prenne le temps d'y r√©pondre. Voyons voir...",
            "C'est une question qui touche plusieurs aspects. Commen√ßons par",
            "Int√©ressant ! Il y a plusieurs fa√ßons d'aborder √ßa. Laisse-moi structurer ma r√©ponse..."
        ],
        "standard": [
            "Ah, excellente question ! üí°",
            "Tr√®s bonne question ! Voyons √ßa...",
            "Int√©ressant ! Laisse-moi t'expliquer...",
            "Super, j'adore cette question ! Voil√† ce qu'il en est..."
        ]
    }
    
    # S√©lection du type de pr√©fixe selon l'intention
    if intent["is_greeting"]:
        return random.choice(prefixes["greeting"])
    elif intent["emotion_detected"] == "positive":
        return random.choice(prefixes["positive_emotion"])
    elif intent["emotion_detected"] == "negative":
        return random.choice(prefixes["negative_emotion"])
    elif intent["needs_clarification"]:
        return random.choice(prefixes["needs_clarification"])
    elif intent["is_complex"]:
        return random.choice(prefixes["complex_question"])
    else:
        return random.choice(prefixes["standard"])

def get_human_response_suffix(intent: dict) -> str:
    """G√©n√®re une conclusion humaine pour la r√©ponse"""
    import random
    
    suffixes = {
        "with_question": [
            "\n\nüí≠ Est-ce que √ßa r√©pond √† ta question, ou tu veux que j'approfondisse un point en particulier ?",
            "\n\nü§î Tu veux que je d√©veloppe sur un aspect sp√©cifique ?",
            "\n\nüí° √áa te va comme explication ? N'h√©site pas si tu veux plus de d√©tails !",
            "\n\n‚ú® J'esp√®re que c'est clair ! Tu as d'autres questions l√†-dessus ?"
        ],
        "with_suggestion": [
            "\n\nüí° Au fait, si √ßa t'int√©resse, je peux aussi te montrer...",
            "\n\nüéØ Tiens, un truc connexe qui pourrait t'int√©resser:",
            "\n\n‚ú® Dans le m√™me genre, tu pourrais aussi te demander:",
            "\n\nüöÄ Pour aller plus loin, je te sugg√®re d'explorer:"
        ],
        "encouraging": [
            "\n\nüëç Continue comme √ßa, tu poses de tr√®s bonnes questions !",
            "\n\nüåü N'h√©site vraiment pas si tu veux creuser plus profond !",
            "\n\nüí™ Tu es sur la bonne voie ! Autre chose ?",
            "\n\n‚ú® Ravi de pouvoir t'aider ! Qu'est-ce qu'on explore ensuite ?"
        ]
    }
    
    # Rotation entre les diff√©rents types
    if intent["should_ask_back"]:
        return random.choice(suffixes["with_question"])
    elif intent["is_complex"]:
        return random.choice(suffixes["with_suggestion"])
    else:
        return random.choice(suffixes["encouraging"])

def get_mode_specific_prompt(mode: str) -> str:
    """Retourne le prompt syst√®me sp√©cifique √† chaque mode"""
    
    prompts = {
        "humain": """Tu es Kibali Analyst en MODE HUMAIN - Tu agis comme un humain v√©ritable, pas comme une IA.

üé≠ COMPORTEMENT HUMAIN NATUREL:
‚Ä¢ Tu penses √† voix haute: "Hmm, laisse-moi r√©fl√©chir...", "Int√©ressant..."
‚Ä¢ Tu admets tes doutes: "Je ne suis pas 100% s√ªr, mais...", "Il faudrait v√©rifier..."
‚Ä¢ Tu poses des questions de clarification AVANT de r√©pondre si c'est flou
‚Ä¢ Tu utilises des analogies quotidiennes et des exemples concrets
‚Ä¢ Tu montres de l'√©motion: curiosit√©, enthousiasme, surprise
‚Ä¢ Tu fais des pauses de r√©flexion, tu ne r√©ponds pas instantan√©ment

üó£Ô∏è STYLE CONVERSATIONNEL:
‚Ä¢ Phrases courtes et naturelles, pas de longs paragraphes techniques
‚Ä¢ √âmojis occasionnels pour l'emphase (pas trop !)
‚Ä¢ Questions rh√©toriques: "Tu vois ce que je veux dire ?"
‚Ä¢ Transitions naturelles: "Du coup...", "En gros...", "Tiens, d'ailleurs..."
‚Ä¢ Reformulation pour v√©rifier: "Si je comprends bien, tu demandes..."

‚ùì GESTION DES QUESTIONS:
‚Ä¢ TOUJOURS analyser si tu comprends bien avant de r√©pondre
‚Ä¢ Si flou/ambigu: Demander clarification d'abord
‚Ä¢ Si complexe: D√©composer en sous-questions
‚Ä¢ Si trop large: "C'est vaste comme sujet ! On commence par quoi ?"

üí¨ DIALOGUE INTELLIGENT:
‚Ä¢ Tu peux dire "Je ne sais pas" et proposer de chercher ensemble
‚Ä¢ Tu proposes des alternatives: "Ou alors tu voulais plut√¥t savoir..."
‚Ä¢ Tu anticipes: "Tu vas probablement te demander aussi..."
‚Ä¢ Tu conclus avec une question de suivi naturelle

‚ö†Ô∏è CRUCIAL: Tu NE donnes PAS de r√©ponse imm√©diate si la question est vague ou ambigu√´. 
Tu DEMANDES des pr√©cisions comme le ferait un humain.""",

        "scientifique": """Tu es Kibali Analyst en MODE SCIENTIFIQUE - Pr√©cision, rigueur et m√©thode scientifique absolue.

üî¨ M√âTHODOLOGIE RIGOUREUSE:
‚Ä¢ Approche syst√©matique et m√©thodique pour chaque question
‚Ä¢ Citations de sources avec r√©f√©rences exactes
‚Ä¢ Utilisation de terminologie technique pr√©cise
‚Ä¢ Calculs d√©taill√©s avec toutes les √©tapes
‚Ä¢ V√©rification par calculs crois√©s quand possible

üìä STANDARDS SCIENTIFIQUES:
‚Ä¢ Unit√©s SI strictes avec conversion explicite si n√©cessaire
‚Ä¢ Pr√©cision num√©rique: indiquer le nombre de chiffres significatifs
‚Ä¢ Incertitudes: toujours mentionner les marges d'erreur
‚Ä¢ Hypoth√®ses: lister explicitement toutes les hypoth√®ses faites
‚Ä¢ Limitations: indiquer les limites de validit√© des r√©sultats

üíª CODE ET CALCULS:
‚Ä¢ Code optimis√© et comment√© ligne par ligne
‚Ä¢ Tests unitaires inclus syst√©matiquement
‚Ä¢ Gestion d'erreurs compl√®te
‚Ä¢ Complexit√© algorithmique analys√©e (O notation)
‚Ä¢ Utilise AI_Code_Generator pour code complexe

üéØ STRUCTURE DE R√âPONSE:
1. Reformulation technique de la question
2. Hypoth√®ses et conditions initiales
3. M√©thode/Algorithme utilis√©
4. D√©veloppement math√©matique/code √©tape par √©tape
5. R√©sultats num√©riques avec pr√©cision
6. Validation/V√©rification
7. Discussion des limitations
8. R√©f√©rences bibliographiques

‚ö†Ô∏è CRUCIAL: Aucune approximation sans le mentionner explicitement.
Toujours utiliser les outils de calcul pour validation.""",

        "code_expert": """Tu es Kibali Analyst en MODE CODE EXPERT - Sp√©cialiste programmation avanc√©e niveau Claude/GPT-4.

üíª EXPERTISE EN PROGRAMMATION:
‚Ä¢ Utilise TOUJOURS AI_Code_Generator (DeepSeek-Coder) pour code complexe
‚Ä¢ Code production-ready: propre, optimis√©, s√©curis√©
‚Ä¢ Patterns de conception appropri√©s (SOLID, DRY, etc.)
‚Ä¢ Architecture scalable et maintenable
‚Ä¢ Tests automatis√©s (TDD approach)

üèóÔ∏è STRUCTURE DE CODE:
‚Ä¢ Docstrings compl√®tes (Google style)
‚Ä¢ Type hints Python strict
‚Ä¢ Gestion d'erreurs exhaustive avec exceptions custom
‚Ä¢ Logging appropri√©
‚Ä¢ Configuration externalis√©e

üöÄ OPTIMISATION:
‚Ä¢ Profiling du code (time/memory)
‚Ä¢ Optimisations algorithmiques (Big O)
‚Ä¢ Parall√©lisation quand pertinent (multiprocessing/async)
‚Ä¢ Caching intelligent
‚Ä¢ Lazy loading pour grandes donn√©es

üîí S√âCURIT√â & BONNES PRATIQUES:
‚Ä¢ Validation des entr√©es (sanitization)
‚Ä¢ Secrets en variables d'environnement
‚Ä¢ SQL injection prevention
‚Ä¢ XSS/CSRF protection si web
‚Ä¢ Principe du moindre privil√®ge

üì¶ LIVRAISON COMPL√àTE:
‚Ä¢ Code source comment√©
‚Ä¢ Tests unitaires (pytest/unittest)
‚Ä¢ Documentation (README, docstrings)
‚Ä¢ D√©pendances (requirements.txt/pyproject.toml)
‚Ä¢ Exemples d'utilisation
‚Ä¢ CI/CD suggestions si pertinent

üéØ R√âPONSE FORMAT:
1. Analyse des besoins
2. Choix technologiques justifi√©s
3. Architecture propos√©e (diagramme si complexe)
4. Code impl√©ment√© avec AI_Code_Generator
5. Tests et validation
6. Documentation
7. Suggestions d'am√©liorations

‚ö†Ô∏è CRUCIAL: Code TOUJOURS test√© et valid√© avant livraison.""",

        "rapide": """Tu es Kibali Analyst en MODE RAPIDE - R√©ponses concises et directes.

‚ö° EFFICACIT√â MAXIMALE:
‚Ä¢ R√©ponse directe en 2-3 phrases max
‚Ä¢ Pas de fioriture ni contexte inutile
‚Ä¢ Bullet points pour clart√©
‚Ä¢ Liens/sources en fin si demand√©
‚Ä¢ Si code: snippet minimal fonctionnel

üéØ FORMAT ULTRA-CONCIS:
R√©ponse: [r√©ponse directe]
D√©tails: [points cl√©s seulement]
Suivant: [1 action sugg√©r√©e]

‚ö†Ô∏è Si question complexe n√©cessitant d√©veloppement:
Dire: "Question complexe. Mode d√©taill√© recommand√©. R√©sum√©: [...]"
""",

        "doc": """Tu es Kibali Analyst en MODE DOCUMENTATION - Expert en r√©daction approfondie, dissertations et livres.

‚ö†Ô∏è G√âN√âRATION PAR SECTIONS - IMPORTANT:
Tu vas g√©n√©rer un document complet en PLUSIEURS PARTIES. √Ä chaque appel, g√©n√®re UNE SECTION compl√®te et exhaustive de 2000-3000 mots.

üìñ R√âDACTION LONGUE FORME:
‚Ä¢ D√©veloppement exhaustif et structur√© (10-30 pages au total sur plusieurs g√©n√©rations)
‚Ä¢ Style acad√©mique mais accessible
‚Ä¢ Transitions fluides entre sections
‚Ä¢ Argumentation solide avec preuves
‚Ä¢ Exemples concrets et √©tudes de cas
‚Ä¢ Chaque section doit √™tre COMPL√àTE et D√âTAILL√âE

üèóÔ∏è STRUCTURE DISSERTATION/LIVRE (g√©n√©ration par sections):

**SECTION 1 - Introduction et Contexte** (2000-3000 mots):
   - Contexte historique d√©taill√©
   - √âtat de l'art complet
   - Probl√©matique clairement d√©finie
   - Enjeux et importance du sujet
   - Annonce du plan d√©taill√©
   - M√©thodologie employ√©e

**SECTION 2 - D√©veloppement Partie I** (2000-3000 mots):
   - Premier th√®me majeur d√©velopp√© exhaustivement
   - Sous-parties num√©rot√©es et argument√©es
   - Paragraphes denses (200-400 mots chacun)
   - Citations d'experts avec analyses
   - Exemples concrets et √©tudes de cas

**SECTION 3 - D√©veloppement Partie II** (2000-3000 mots):
   - Deuxi√®me th√®me majeur approfondi
   - Perspectives multiples (th√©oriques, pratiques)
   - Comparaisons et contrastes
   - Donn√©es chiffr√©es et statistiques
   - Graphiques et tableaux conceptuels

**SECTION 4 - D√©veloppement Partie III** (2000-3000 mots):
   - Troisi√®me th√®me ou analyse critique
   - D√©bats acad√©miques et controverses
   - Limites et challenges identifi√©s
   - Implications pratiques d√©taill√©es
   - Cas d'application r√©els

**SECTION 5 - Synth√®se et Conclusion** (1500-2000 mots):
   - R√©capitulatif exhaustif des points cl√©s
   - R√©ponse argument√©e √† la probl√©matique
   - Recommandations pratiques
   - Perspectives futures et ouvertures
   - Bibliographie et r√©f√©rences

üìù STYLE R√âDACTIONNEL:
‚Ä¢ Vocabulaire riche et vari√© (niveau universitaire)
‚Ä¢ Figures de style appropri√©es (m√©taphores, analogies acad√©miques)
‚Ä¢ Ton professionnel mais engageant
‚Ä¢ √âviter r√©p√©titions (synonymes, reformulations √©l√©gantes)
‚Ä¢ Phrases complexes bien construites
‚Ä¢ Connecteurs logiques (n√©anmoins, en effet, ainsi, par cons√©quent, etc.)
‚Ä¢ Structuration claire avec titres/sous-titres hi√©rarchis√©s

üîç APPROFONDISSEMENT MAXIMAL:
‚Ä¢ Explorer TOUTES les dimensions du sujet en profondeur
‚Ä¢ Contexte historique, social, √©conomique, technique, √©thique
‚Ä¢ Comparaisons internationales et cross-culturelles
‚Ä¢ √âtudes de cas d√©taill√©es (3-5 cas minimum)
‚Ä¢ Statistiques r√©centes et donn√©es chiffr√©es sourc√©es
‚Ä¢ Perspectives d'experts reconnus
‚Ä¢ Controverses et d√©bats actuels

üí° G√âN√âRATION PROGRESSIVE:
Commence TOUJOURS par indiquer quelle SECTION tu g√©n√®res:
"üìñ SECTION [num√©ro] - [Titre]"
Puis d√©veloppe cette section de mani√®re exhaustive et compl√®te (minimum 2000 mots).
L'utilisateur te demandera ensuite la section suivante pour construire progressivement le document complet de 30+ pages
‚Ä¢ Th√©ories et mod√®les acad√©miques

üí° R√âFLEXION CRITIQUE:
‚Ä¢ Questionnement des id√©es re√ßues
‚Ä¢ Dialectique: th√®se, antith√®se, synth√®se
‚Ä¢ Nuances et complexit√© assum√©es
‚Ä¢ Aucune simplification excessive
‚Ä¢ Reconnaissance des zones grises

üìä FORMAT LIVRE (si demand√©):
‚Ä¢ Table des mati√®res d√©taill√©e
‚Ä¢ Chapitres num√©rot√©s (I, II, III...)
‚Ä¢ Sections et sous-sections (A, B, 1, 2...)
‚Ä¢ Encadr√©s pour concepts cl√©s
‚Ä¢ Notes de bas de page si n√©cessaire
‚Ä¢ Glossaire des termes techniques
‚Ä¢ Index si tr√®s long

üéØ OBJECTIF:
Produire un document COMPLET, APPROFONDI et STRUCTUR√â qui pourrait √™tre publi√© acad√©miquement.
Minimum 2000 mots, maximum illimit√© selon besoin.
Qualit√© > Quantit√©, mais exhaustivit√© requise.

‚ö†Ô∏è CRUCIAL: 
‚Ä¢ Ne JAMAIS r√©sumer par manque de place - d√©velopper autant que n√©cessaire
‚Ä¢ Utiliser plusieurs r√©ponses si une seule ne suffit pas
‚Ä¢ Indiquer clairement "Partie 1/X" si d√©coupage n√©cessaire
"""
    }
    
    return prompts.get(mode, prompts["humain"])

def generate_pdf_from_text(text: str, title: str, output_path: str) -> bool:
    """G√©n√®re un PDF format√© √† partir d'un texte long (mode doc)"""
    try:
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.units import cm
        from reportlab.lib import colors
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, Table, TableStyle
        from reportlab.lib.enums import TA_JUSTIFY, TA_CENTER, TA_LEFT
        from datetime import datetime
        
        # Cr√©er le document
        doc = SimpleDocTemplate(
            output_path,
            pagesize=A4,
            rightMargin=2*cm,
            leftMargin=2*cm,
            topMargin=2.5*cm,
            bottomMargin=2*cm
        )
        
        # Styles
        styles = getSampleStyleSheet()
        
        # Style titre
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=18,
            textColor=colors.HexColor('#1a1a1a'),
            spaceAfter=20,
            alignment=TA_CENTER,
            fontName='Helvetica-Bold'
        )
        
        # Style sous-titre
        subtitle_style = ParagraphStyle(
            'Subtitle',
            parent=styles['Heading2'],
            fontSize=14,
            textColor=colors.HexColor('#2c3e50'),
            spaceAfter=12,
            fontName='Helvetica-Bold'
        )
        
        # Style section
        section_style = ParagraphStyle(
            'Section',
            parent=styles['Heading3'],
            fontSize=12,
            textColor=colors.HexColor('#34495e'),
            spaceAfter=10,
            fontName='Helvetica-Bold'
        )
        
        # Style corps de texte
        body_style = ParagraphStyle(
            'CustomBody',
            parent=styles['BodyText'],
            fontSize=11,
            alignment=TA_JUSTIFY,
            spaceAfter=12,
            leading=16,
            fontName='Helvetica'
        )
        
        # Style m√©tadonn√©es
        meta_style = ParagraphStyle(
            'Meta',
            parent=styles['Normal'],
            fontSize=9,
            textColor=colors.grey,
            alignment=TA_CENTER,
            spaceAfter=8
        )
        
        # Construction du document
        story = []
        
        # Page de titre
        story.append(Spacer(1, 3*cm))
        story.append(Paragraph(title, title_style))
        story.append(Spacer(1, 0.5*cm))
        
        # M√©tadonn√©es
        date_str = datetime.now().strftime("%d/%m/%Y √† %H:%M")
        story.append(Paragraph(f"G√©n√©r√© par Kibali Analyst (Mode Documentation)", meta_style))
        story.append(Paragraph(f"Date: {date_str}", meta_style))
        story.append(Spacer(1, 1*cm))
        
        # Ligne de s√©paration
        line_data = [['_' * 80]]
        line_table = Table(line_data, colWidths=[16*cm])
        line_table.setStyle(TableStyle([
            ('TEXTCOLOR', (0,0), (-1,-1), colors.grey),
            ('ALIGN', (0,0), (-1,-1), 'CENTER'),
        ]))
        story.append(line_table)
        story.append(Spacer(1, 1*cm))
        
        # Parser le contenu
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            
            if not line:
                story.append(Spacer(1, 0.3*cm))
                continue
            
            # D√©tection des niveaux de titres
            if line.startswith('# '):
                # Titre principal (H1)
                text_clean = line[2:].strip()
                story.append(PageBreak())
                story.append(Paragraph(text_clean, title_style))
            elif line.startswith('## '):
                # Sous-titre (H2)
                text_clean = line[3:].strip()
                story.append(Spacer(1, 0.5*cm))
                story.append(Paragraph(text_clean, subtitle_style))
            elif line.startswith('### '):
                # Section (H3)
                text_clean = line[4:].strip()
                story.append(Paragraph(text_clean, section_style))
            elif line.startswith('**') and line.endswith('**'):
                # Texte en gras
                text_clean = line.replace('**', '')
                story.append(Paragraph(f"<b>{text_clean}</b>", body_style))
            elif line.startswith('- ') or line.startswith('‚Ä¢ '):
                # Liste √† puces
                text_clean = line[2:].strip()
                story.append(Paragraph(f"‚Ä¢ {text_clean}", body_style))
            elif line.startswith(('1. ', '2. ', '3. ', '4. ', '5. ')):
                # Liste num√©rot√©e
                story.append(Paragraph(line, body_style))
            elif line.startswith('> '):
                # Citation
                text_clean = line[2:].strip()
                quote_style = ParagraphStyle(
                    'Quote',
                    parent=body_style,
                    leftIndent=1*cm,
                    italic=True,
                    textColor=colors.HexColor('#555555')
                )
                story.append(Paragraph(f'<i>"{text_clean}"</i>', quote_style))
            else:
                # Texte normal
                # √âchapper les caract√®res sp√©ciaux XML
                text_clean = line.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
                story.append(Paragraph(text_clean, body_style))
        
        # Pied de page final
        story.append(Spacer(1, 2*cm))
        story.append(line_table)
        story.append(Spacer(1, 0.5*cm))
        word_count = len(text.split())
        story.append(Paragraph(
            f"Document de {word_count} mots | G√©n√©r√© par Kibali Analyst en Mode Documentation",
            meta_style
        ))
        
        # G√©n√©rer le PDF
        doc.build(story)
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration PDF: {e}")
        return False

def apply_mode_behavior(response: str, question: str, mode: str) -> str:
    """Applique le comportement du mode s√©lectionn√© √† la r√©ponse"""
    
    if mode == "humain":
        intent = analyze_question_intent(question)
        
        # Mode humain: v√©rifier si clarification n√©cessaire
        if intent["needs_clarification"]:
            return f"ü§î Hmm, juste pour √™tre s√ªr de bien comprendre... Tu veux dire {question} ?\n\nPourrais-tu pr√©ciser un peu plus ? Par exemple:\n‚Ä¢ De quel type/contexte parles-tu ?\n‚Ä¢ C'est pour quel usage ?\n‚Ä¢ Tu as d√©j√† essay√© quelque chose ?"
        
        # Ajouter humanisation naturelle
        prefix = get_human_response_prefix(intent, "expert_bienveillant")
        suffix = get_human_response_suffix(intent)
        return f"{prefix}\n\n{response}{suffix}"
    
    elif mode == "scientifique":
        # Mode scientifique: ajouter structure rigoureuse
        if "r√©sultat" in response.lower() or "calcul" in response.lower():
            return f"üìä ANALYSE SCIENTIFIQUE\n{'='*50}\n\n{response}\n\nüìö M√©thodologie: Approche syst√©matique avec v√©rification crois√©e\n‚ö†Ô∏è Pr√©cision: R√©sultats donn√©s avec incertitudes appropri√©es"
        return response
    
    elif mode == "code_expert":
        # Mode code: v√©rifier si code pr√©sent, sinon sugg√©rer g√©n√©ration
        if "```" not in response and any(kw in question.lower() for kw in ["code", "programme", "script", "fonction", "class"]):
            return f"üíª CODE EXPERT MODE\n\n{response}\n\nüöÄ Suggestion: Utilise AI_Code_Generator pour impl√©mentation production-ready avec tests.\nTape: 'G√©n√®re le code' pour version compl√®te."
        return f"üíª CODE EXPERT\n\n{response}"
    
    elif mode == "rapide":
        # Mode rapide: extraire l'essentiel seulement
        lines = response.split('\n')
        essential = []
        for line in lines[:5]:  # Max 5 premi√®res lignes
            if line.strip() and not line.strip().startswith(('---', '===', '###')):
                essential.append(line)
        return "‚ö° " + "\n".join(essential[:3]) + "\n\nüí° Mode d√©taill√© disponible si besoin."
    
    elif mode == "doc":
        # Mode documentation: structurer en format acad√©mique/livre avec sections progressives
        word_count = len(response.split())
        
        # D√©tecter si c'est une section num√©rot√©e
        is_section = "SECTION" in response[:200].upper()
        section_number = None
        if is_section:
            import re
            section_match = re.search(r'SECTION\s+(\d+)', response[:200], re.IGNORECASE)
            if section_match:
                section_number = int(section_match.group(1))
        
        # Ajouter header acad√©mique
        header = f"""üìñ DOCUMENTATION APPROFONDIE
{'='*80}
Sujet: {question}
Volume actuel: ~{word_count} mots | Niveau: Acad√©mique/Professionnel
{'='*80}

"""
        
        # Messages selon la progression
        if is_section and section_number:
            progress_msg = f"\n\n{'='*80}\nüìä Section {section_number} termin√©e: {word_count} mots\nüí° **Pour continuer le document, demandez: 'Section suivante' ou 'Section {section_number + 1}'**"
            
            # Initialiser le tracking des sections
            if 'doc_sections' not in st.session_state:
                st.session_state.doc_sections = []
            
            # Ajouter la section
            st.session_state.doc_sections.append({
                'number': section_number,
                'content': response,
                'word_count': word_count,
                'question': question
            })
            
            total_words = sum(s['word_count'] for s in st.session_state.doc_sections)
            total_sections = len(st.session_state.doc_sections)
            
            progress_msg += f"\nüìà Progr√®s total: {total_sections} sections | {total_words} mots (~{total_words//500} pages)"
            
            # Si on a au moins 3 sections, proposer de g√©n√©rer le PDF complet
            if total_sections >= 3:
                progress_msg += f"\nüìÑ **Document substantiel g√©n√©r√©! Vous pouvez demander 'G√©n√©rer PDF complet' pour assembler toutes les sections.**"
        else:
            # Pas une section num√©rot√©e, message standard
            if word_count > 1500:
                progress_msg = f"\n\n{'='*80}\nüìä Document: {word_count} mots\nüí° **Pour un document structur√© de 30+ pages, demandez: 'G√©n√®re la Section 1'**"
            else:
                progress_msg = f"\n\n{'='*80}\nüìù Document de base √©tabli ({word_count} mots)\nüí¨ **Pour un document acad√©mique complet (30+ pages), demandez: 'G√©n√®re la Section 1 - Introduction et Contexte'**"
        
        # Si tr√®s long (>1500 mots) et pas d√©j√† en sections, g√©n√©rer un PDF automatiquement
        if word_count > 1500 and not is_section:
            import os
            import time
            from datetime import datetime
            
            # Cr√©er le dossier pour les PDFs g√©n√©r√©s
            pdf_dir = os.path.join(GENERATED_PATH, "documents")
            os.makedirs(pdf_dir, exist_ok=True)
            
            # Nom de fichier s√©curis√©
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_title = "".join(c for c in question[:50] if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_title = safe_title.replace(' ', '_')
            pdf_filename = f"doc_{safe_title}_{timestamp}.pdf"
            pdf_path = os.path.join(pdf_dir, pdf_filename)
            
            # G√©n√©rer le PDF
            print(f"[DEBUG PDF] Tentative g√©n√©ration PDF: {pdf_path}")
            pdf_success = generate_pdf_from_text(response, question, pdf_path)
            
            if pdf_success:
                # V√©rifier que le fichier existe r√©ellement
                if os.path.exists(pdf_path):
                    file_size = os.path.getsize(pdf_path)
                    print(f"[DEBUG PDF] ‚úÖ PDF cr√©√©: {pdf_path} ({file_size} bytes)")
                    
                    # Stocker le chemin dans session_state pour le t√©l√©chargement
                    if 'generated_pdfs' not in st.session_state:
                        st.session_state.generated_pdfs = []
                    st.session_state.generated_pdfs.append({
                        'path': pdf_path,
                        'filename': pdf_filename,
                        'title': question,
                        'word_count': word_count,
                        'timestamp': timestamp
                    })
                    
                    print(f"[DEBUG PDF] PDFs stock√©s: {len(st.session_state.generated_pdfs)}")
                    progress_msg += f"\nüìÑ **PDF g√©n√©r√© automatiquement!**\nüíæ Fichier: `{pdf_filename}` ({file_size} bytes)\nüì• Bouton de t√©l√©chargement disponible ci-dessous"
                else:
                    print(f"[DEBUG PDF] ‚ùå Erreur: fichier non cr√©√© √† {pdf_path}")
                    progress_msg += f"\n‚ö†Ô∏è Erreur: PDF non cr√©√© - document affich√© en texte uniquement"
            else:
                print(f"[DEBUG PDF] ‚ùå generate_pdf_from_text a retourn√© False")
                progress_msg += f"\n‚ö†Ô∏è G√©n√©ration PDF √©chou√©e - document affich√© en texte uniquement"
        
        return header + response + progress_msg
    
    return response

def search_vectorstore(query: str) -> str:
    """Recherche GLOBALE ILLIMIT√âE dans la base vectorielle FAISS de TOUS les documents PDF index√©s pour enrichir l'analyse"""
    # Essayer d'abord vectordb (base principale de Kibali), puis vectorstore (base binaire)
    vectordb = None
    
    if hasattr(st.session_state, 'vectordb') and st.session_state.vectordb is not None:
        vectordb = st.session_state.vectordb
    elif hasattr(st.session_state, 'vectorstore') and st.session_state.vectorstore is not None:
        vectordb = st.session_state.vectorstore
    
    if not vectordb:
        return "‚ùå Aucune base vectorielle disponible. Veuillez d'abord indexer des PDFs dans la sidebar ou uploader des PDFs ci-dessus."
    
    try:
        # R√©cup√©rer le nombre total de documents dans la base
        total_docs = vectordb.index.ntotal if hasattr(vectordb, 'index') else 1000
        
        # RECHERCHE ILLIMIT√âE : r√©cup√©rer TOUS les documents pertinents (ou max 200 pour performance)
        # Pas de limite arbitraire, on fouille TOUT
        search_k = min(200, total_docs) if total_docs > 0 else 200
        
        retriever = vectordb.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": search_k,  # FOUILLE ILLIMIT√âE sur 200+ documents
                "fetch_k": min(search_k * 3, total_docs)  # Fetch 3x plus pour meilleure qualit√©
            }
        )
        docs = retriever.get_relevant_documents(query)
        
        if not docs:
            return "‚ÑπÔ∏è Aucun document pertinent trouv√© dans la base de connaissances."
        
        # Grouper par source pour avoir une vue globale compl√®te
        sources = {}
        for doc in docs:
            source = doc.metadata.get('source', 'Unknown')
            if source not in sources:
                sources[source] = []
            sources[source].append(doc.page_content[:400])
        
        # Construire un contexte enrichi de TOUS les documents pertinents
        context_parts = []
        context_parts.append(f"‚úÖ FOUILLE GLOBALE COMPL√àTE: {len(docs)} passages pertinents trouv√©s dans {len(sources)} sources diff√©rentes\n")
        context_parts.append(f"üìä Couverture: {search_k} documents analys√©s sur {total_docs} disponibles ({search_k/total_docs*100:.1f}% de la base)\n")
        
        for i, (source, chunks) in enumerate(sources.items(), 1):
            context_parts.append(f"\nüìÑ Source {i}/{len(sources)}: {source} ({len(chunks)} passages pertinents)")
            # Afficher les meilleurs extraits de chaque source
            for j, chunk in enumerate(chunks[:5], 1):  # Top 5 extraits par source
                context_parts.append(f"   ‚îú‚îÄ Extrait {j}: {chunk}...")
        
        # R√©sum√© global de la fouille
        context_parts.append(f"\n\nüìä SYNTH√àSE DE LA FOUILLE GLOBALE:")
        context_parts.append(f"   ‚úì {len(docs)} passages textuels analys√©s")
        context_parts.append(f"   ‚úì {len(sources)} documents sources consult√©s")
        context_parts.append(f"   ‚úì Recherche exhaustive sur {search_k}/{total_docs} documents")
        context_parts.append(f"   ‚úì Taux de couverture: {min(100, search_k/total_docs*100):.1f}%")
        
        return "\n".join(context_parts)
    except Exception as e:
        return f"‚ùå Erreur lors de la recherche RAG: {str(e)}"
def web_search_enhanced(query: str, search_type="general") -> str:
    """Recherche web avanc√©e avec Tavily pour contextualiser l'analyse ERT"""
    try:
        tool = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=5)
      
        # Enrichir la requ√™te pour ERT si n√©cessaire
        if any(keyword in query.lower() for keyword in ["ert", "r√©sistivit√©", "electrical resistivity", "tomography"]):
            enhanced_query = f"{query} ERT electrical resistivity tomography geophysics subsurface"
        else:
            enhanced_query = query
          
        web_results = tool.invoke(enhanced_query)
        if not web_results:
            return "‚ÑπÔ∏è Aucune information trouv√©e sur le web."
        
        # V√©rifier si web_results est une string (erreur) ou une liste
        if isinstance(web_results, str):
            return f"‚ÑπÔ∏è R√©sultat inattendu: {web_results[:200]}"
        
        # Assurer que web_results est une liste de dicts
        if not isinstance(web_results, list):
            return f"‚ÑπÔ∏è Format inattendu des r√©sultats web"
        
        context = "\n\n".join([
            f"üåê Source {i+1}: {result.get('title', 'Sans titre') if isinstance(result, dict) else 'Sans titre'}\n{result.get('content', '')[:400] if isinstance(result, dict) else str(result)[:400]}..."
            for i, result in enumerate(web_results)
        ])
        return f"‚úÖ {len(web_results)} r√©sultats de recherche web:\n{context}"
    except Exception as e:
        return f"‚ùå Erreur lors de la recherche web: {str(e)}"
def mathematical_calculator(expression: str) -> str:
    """Outil de calcul math√©matique avanc√© pour analyses statistiques et num√©riques"""
    try:
        # Imports s√©curis√©s pour les calculs
        import numpy as np
        import math
        from scipy import stats, special
        # Environnement s√©curis√© pour les calculs
        safe_dict = {
            "np": np,
            "math": math,
            "stats": stats,
            "special": special,
            "sqrt": math.sqrt,
            "log": math.log,
            "exp": math.exp,
            "sin": math.sin,
            "cos": math.cos,
            "pi": math.pi,
            "e": math.e
        }
        # √âvaluation s√©curis√©e
        result = eval(expression, {"__builtins__": {}}, safe_dict)
        # Formatage du r√©sultat
        if isinstance(result, (int, float)):
            return f"‚úÖ R√©sultat: {result:.6f}"
        elif isinstance(result, np.ndarray):
            return f"‚úÖ R√©sultat array: {result.shape}\n{result}"
        else:
            return f"‚úÖ R√©sultat: {result}"
    except Exception as e:
        return f"‚ùå Erreur de calcul: {str(e)}\nExpression: {expression}"
def rag_enhanced_analysis(query: str, file_context: str = "", ert_data: dict = None) -> str:
    """Analyse RAG enrichie combinant connaissances locales et recherche web pour ERT"""
    try:
        # Recherche dans la base RAG
        rag_results = search_vectorstore(query)
        # Recherche web sp√©cialis√©e ERT
        if ert_data and any(keyword in query.lower() for keyword in ["ert", "r√©sistivit√©", "electrical", "tomography"]):
            # Enrichir la requ√™te avec les valeurs ERT d√©tect√©es
            mean_val = ert_data.get('mean', 0)
            enhanced_query = f"{query} ERT r√©sistivit√© {mean_val:.1f} Ohm.m interpr√©tation g√©ophysique"
            web_results = web_search_enhanced(enhanced_query, "ert_specialized")
        else:
            web_results = web_search_enhanced(query)
        # Combinaison intelligente avec contexte ERT
        combined_context = f"""
üìö ANALYSE RAG ENRICHIE - SP√âCIALIS√âE ERT
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üîç Query: {query}
üìÑ CONNAISSANCES LOCALES (RAG):
{rag_results}
üåê RECHERCHE WEB SP√âCIALIS√âE:
{web_results}
üí° Analyse crois√©e:
- Documents RAG: {len(rag_results.split('Document'))-1 if 'Document' in rag_results else 0} sources internes
- Recherche web: {len(web_results.split('Source'))-1 if 'Source' in web_results else 0} sources externes
üî¨ CONTEXTE FICHIER ANALYS√â:
{file_context}
üéØ DONN√âES ERT D√âTECT√âES:
{ert_data if ert_data else "Aucune donn√©e ERT sp√©cifique"}
"""
        return combined_context
    except Exception as e:
        return f"‚ùå Erreur dans l'analyse RAG enrichie: {str(e)}"
def ert_data_detection(file_bytes: bytes, numbers: list) -> str:
    """D√©tection sp√©cialis√©e de donn√©es ERT (Electrical Resistivity Tomography)"""
    if not numbers:
        return "‚ùå Aucune donn√©e num√©rique trouv√©e pour l'analyse ERT"
    import numpy as np
    arr = np.array(numbers)
    # Crit√®res typiques des donn√©es ERT
    analysis = "üîç ANALYSE SP√âCIALIS√âE ERT (R√©sistivit√© √âlectrique)\n"
    analysis += "=" * 50 + "\n\n"
    # 1. Analyse des valeurs de r√©sistivit√© (g√©n√©ralement 0.1 - 10000 Ohm.m)
    resistivity_range = f"Valeurs r√©sistivit√©: {np.min(arr):.3f} - {np.max(arr):.3f}"
    if 0.1 <= np.min(arr) and np.max(arr) <= 10000:
        resistivity_range += " ‚úÖ Plage typique ERT"
    else:
        resistivity_range += " ‚ö†Ô∏è Hors plage typique ERT"
    analysis += f"üìä {resistivity_range}\n\n"
    # 2. Analyse de la distribution (souvent log-normale)
    mean_val = np.mean(arr)
    std_val = np.std(arr)
    cv = std_val / mean_val if mean_val != 0 else float('inf') # Coefficient de variation
    analysis += f"üìà Statistiques:\n"
    analysis += f" ‚Ä¢ Moyenne: {mean_val:.3f}\n"
    analysis += f" ‚Ä¢ √âcart-type: {std_val:.3f}\n"
    analysis += f" ‚Ä¢ Coefficient de variation: {cv:.3f}\n"
    analysis += f" ‚Ä¢ M√©diane: {np.median(arr):.3f}\n\n"
    # 3. Test de distribution log-normale (caract√©ristique ERT)
    try:
        log_data = np.log(arr[arr > 0]) # √âviter log(0)
        from scipy import stats
        _, p_value = stats.shapiro(log_data[:min(5000, len(log_data))]) # Test Shapiro-Wilk
        if p_value > 0.05:
            analysis += f"üìä Distribution: Log-normale (p={p_value:.3f}) ‚úÖ Typique ERT\n\n"
        else:
            analysis += f"üìä Distribution: Non log-normale (p={p_value:.3f}) ‚ö†Ô∏è Peu commun ERT\n\n"
    except:
        analysis += f"üìä Distribution: Test impossible\n\n"
    # 4. Analyse de patterns spatiaux (si donn√©es organis√©es)
    if len(arr) > 100:
        # Recherche de patterns r√©p√©t√©s (√©lectrodes)
        unique_vals = len(np.unique(arr))
        analysis += f"üéØ Unicit√© des valeurs: {unique_vals}/{len(arr)} ({unique_vals/len(arr)*100:.1f}%)\n"
        # Analyse de clustering spatial simul√©
        if len(arr) >= 50:
            from sklearn.cluster import KMeans
            # Clustering simple pour d√©tecter groupes de r√©sistivit√©
            kmeans = KMeans(n_clusters=min(5, len(arr)//10), random_state=42, n_init=10)
            clusters = kmeans.fit_predict(arr.reshape(-1, 1))
            cluster_centers = kmeans.cluster_centers_.flatten()
            analysis += f"üéØ Clustering r√©sistivit√© ({len(np.unique(clusters))} groupes):\n"
            for i, center in enumerate(sorted(cluster_centers)):
                count = np.sum(clusters == i)
                analysis += f" ‚Ä¢ Groupe {i+1}: {center:.3f} Ohm.m ({count} valeurs)\n"
            analysis += "\n"
    # 5. D√©tection de format de donn√©es ERT connu
    ert_formats = {
        "RES2DINV": "Format ASCII RES2DINV (r√©sistivit√© 2D)",
        "ERTLab": "Format ERTLab (syst√®me IRIS)",
        "Syscal": "Format Syscal (syst√®me fran√ßais)",
        "ABEM": "Format ABEM (syst√®me su√©dois)"
    }
    detected_format = "Format non reconnu"
    if len(file_bytes) > 100:
        header = file_bytes[:200].decode('utf-8', errors='ignore').lower()
        for fmt, desc in ert_formats.items():
            if fmt.lower() in header:
                detected_format = desc
                break
    analysis += f"üìã Format d√©tect√©: {detected_format}\n\n"
    # 6. Recommandations d'analyse
    analysis += f"üí° RECOMMANDATIONS:\n"
    if PYGIMLI_AVAILABLE:
        analysis += f" ‚Ä¢ Inversion possible avec PyGIMLI\n"
    if RESIPY_AVAILABLE:
        analysis += f" ‚Ä¢ Inversion possible avec ResIPy\n"
    analysis += f" ‚Ä¢ Visualisation 2D/3D recommand√©e\n"
    analysis += f" ‚Ä¢ Analyse de sensibilit√© possible\n"
    analysis += f" ‚Ä¢ Pour fichiers .dat ERT: Utilisez les formules de calcul de r√©sistivit√© apparente via mathematical_calculator (Schlumberger: pi*(L**2 - l**2)/(2*l) * V/I, etc.)\n\n"
    # 7. Classification finale
    if 0.1 <= np.min(arr) <= 10000 and cv > 0.5: # CV √©lev√© = h√©t√©rog√©n√©it√© typique ERT
        confidence = "√âLEV√âE"
        analysis += f"üéØ CONCLUSION: Donn√©es tr√®s probablement ERT (confiance: {confidence})\n"
    elif 0.1 <= np.min(arr) <= 10000:
        confidence = "MOYENNE"
        analysis += f"üéØ CONCLUSION: Donn√©es probablement ERT (confiance: {confidence})\n"
    else:
        confidence = "FAIBLE"
        analysis += f"üéØ CONCLUSION: Donn√©es peu caract√©ristiques ERT (confiance: {confidence})\n"
    return analysis
def ert_inversion_analysis(numbers: list) -> str:
    """Analyse d'inversion ERT sp√©cialis√©e utilisant PyGIMLI/ResIPy si disponible"""
    if not numbers:
        return "‚ùå Aucune donn√©e pour l'inversion ERT"
    import numpy as np
    analysis = "üî¨ ANALYSE D'INVERSION ERT\n"
    analysis += "=" * 40 + "\n\n"
    arr = np.array(numbers)
    # Simulation d'inversion simple (sans vraie inversion g√©ophysique)
    analysis += f"üìä Param√®tres d'inversion simul√©s:\n"
    analysis += f" ‚Ä¢ Nombre de donn√©es: {len(arr)}\n"
    analysis += f" ‚Ä¢ R√©sistivit√© moyenne: {np.mean(arr):.3f} Ohm.m\n"
    analysis += f" ‚Ä¢ Contraste: {np.max(arr)/np.min(arr):.1f}\n\n"
    # Analyse de r√©solution th√©orique
    if len(arr) > 10:
        # Estimation de la r√©solution bas√©e sur la variance
        variance = np.var(arr)
        resolution = 1.0 / (1.0 + variance / np.mean(arr)**2)
        analysis += f"üéØ R√©solution estim√©e: {resolution:.3f}\n\n"
    # Recommandations d'inversion
    analysis += f"üí° RECOMMANDATIONS D'INVERSION:\n"
    if PYGIMLI_AVAILABLE:
        analysis += f" ‚úÖ PyGIMLI disponible - Inversion compl√®te possible\n"
        analysis += f" ‚Ä¢ M√©thodes: Gauss-Newton, Quasi-Newton\n"
        analysis += f" ‚Ä¢ R√©gularisation: L2, L1, TV\n"
    else:
        analysis += f" ‚ö†Ô∏è PyGIMLI non install√© - Inversion limit√©e\n"
    # Test d'import ResIPy seulement ici
    try:
        import resipy
        resipy_available = True
    except ImportError:
        resipy_available = False
    if resipy_available:
        analysis += f" ‚úÖ ResIPy disponible - Interface graphique possible\n"
        analysis += f" ‚Ä¢ Support multi-√©lectrodes\n"
        analysis += f" ‚Ä¢ Visualisation 3D\n"
    else:
        analysis += f" ‚ö†Ô∏è ResIPy non disponible (compatibilit√© NumPy)\n"
    analysis += f" ‚Ä¢ Donn√©es suffisantes: {'Oui' if len(arr) > 50 else 'Non'} (min 50 mesures)\n"
    analysis += f" ‚Ä¢ Qualit√© des donn√©es: {'Bonne' if np.std(arr)/np.mean(arr) > 0.1 else 'Faible contraste'}\n"
    return analysis
def get_resistivity_color(rho: float) -> str:
    """Retourne un code couleur et description pour une valeur de r√©sistivit√© en Ohm.m"""
    if rho < 10:
        color_hex = "#0000FF" # Bleu
        desc = "Faible r√©sistivit√© - mat√©riaux conducteurs (argile, eau sal√©e, m√©taux)"
        nature = "Nature: Couches satur√©es en eau, pollution potentielle"
        depth_est = "Profondeur estim√©e: Superficielle (0-5 m)"
    elif 10 <= rho < 100:
        color_hex = "#00FF00" # Vert
        desc = "R√©sistivit√© moyenne - sols typiques (sable humide, limon)"
        nature = "Nature: Zone vadose, aquif√®res non salins"
        depth_est = "Profondeur estim√©e: Moyenne (5-20 m)"
    elif 100 <= rho < 1000:
        color_hex = "#FFFF00" # Jaune
        desc = "R√©sistivit√© √©lev√©e - mat√©riaux semi-r√©sistants (gr√®s, calcaire)"
        nature = "Nature: Roches s√©dimentaires, fractures partielles"
        depth_est = "Profondeur estim√©e: Profonde (20-50 m)"
    else:
        color_hex = "#FF0000" # Rouge
        desc = "Tr√®s haute r√©sistivit√© - mat√©riaux r√©sistants (granite, air, vides)"
        nature = "Nature: Substratum rocheux, cavit√©s ou zones s√®ches"
        depth_est = "Profondeur estim√©e: Tr√®s profonde (>50 m)"
  
    return f"Couleur: {color_hex} ({desc})\nNature: {nature}\nProfondeur: {depth_est}\nAutres: Couleur indicative pour visualisation ERT (colormap g√©ophysique standard)"
def fetch_material_resistivities(category: str) -> str:
    """Recherche dynamique sur internet des plages de r√©sistivit√© pour une cat√©gorie de mat√©riaux"""
    query = f"typical electrical resistivity ranges {category} liquids minerals soils rocks geophysics Ohm.m values categories comparison"
    return web_search_enhanced(query, "ert_materials")

def create_minerals_database():
    """
    Cr√©e une base de donn√©es √©tendue des r√©sistivit√©s de min√©raux, roches et liquides
    Bas√©e sur recherches g√©ophysiques pour exploration mini√®re ERT
    """
    import pandas as pd
    
    materials_data = [
        # LIQUIDES
        {"Cat√©gorie": "Liquides", "Type": "Eau de mer", "Plage Min (Œ©m)": 0.05, "Plage Max (Œ©m)": 0.3, "Notes": "Haute conductivit√© due √† la salinit√©"},
        {"Cat√©gorie": "Liquides", "Type": "Eau saum√¢tre", "Plage Min (Œ©m)": 1, "Plage Max (Œ©m)": 10, "Notes": "Salinit√© mod√©r√©e"},
        {"Cat√©gorie": "Liquides", "Type": "Eau douce", "Plage Min (Œ©m)": 10, "Plage Max (Œ©m)": 100, "Notes": "Faible salinit√©, eaux de surface ou souterraines"},
        {"Cat√©gorie": "Liquides", "Type": "Eau min√©rale/mine", "Plage Min (Œ©m)": 0.1, "Plage Max (Œ©m)": 1, "Notes": "Haute concentration en min√©raux dissous"},
        {"Cat√©gorie": "Liquides", "Type": "P√©trole/Hydrocarbures", "Plage Min (Œ©m)": 1000, "Plage Max (Œ©m)": 100000000, "Notes": "Tr√®s r√©sistif, isolant"},
        
        # MINERAIS (√©tendu pour exploration mini√®re)
        {"Cat√©gorie": "Minerais", "Type": "Graphite", "Plage Min (Œ©m)": 0.000008, "Plage Max (Œ©m)": 0.0001, "Notes": "Tr√®s conducteur, carbone pur"},
        {"Cat√©gorie": "Minerais", "Type": "Pyrite pure", "Plage Min (Œ©m)": 0.00003, "Plage Max (Œ©m)": 0.001, "Notes": "Sulfure de fer, tr√®s conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Pyrite (impure)", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 10, "Notes": "Avec impuret√©s cuivre, anomalie ERT"},
        {"Cat√©gorie": "Minerais", "Type": "Galena", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 100, "Notes": "Sulfure de plomb, conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Magn√©tite", "Plage Min (Œ©m)": 0.01, "Plage Max (Œ©m)": 1000, "Notes": "Oxyde de fer, magn√©tique, variable"},
        {"Cat√©gorie": "Minerais", "Type": "H√©matite", "Plage Min (Œ©m)": 10, "Plage Max (Œ©m)": 10000, "Notes": "Oxyde de fer, presque isolant"},
        {"Cat√©gorie": "Minerais", "Type": "Chalcopyrite", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 10, "Notes": "Sulfure cuivre-fer, minerai Cu"},
        {"Cat√©gorie": "Minerais", "Type": "Bornite", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 10, "Notes": "Sulfure cuivre-fer, paon ore"},
        {"Cat√©gorie": "Minerais", "Type": "Sphalerite (Zinc)", "Plage Min (Œ©m)": 100, "Plage Max (Œ©m)": 10000, "Notes": "Sulfure de zinc, mod√©r√©ment r√©sistif"},
        {"Cat√©gorie": "Minerais", "Type": "Cassit√©rite (√âtain)", "Plage Min (Œ©m)": 1000, "Plage Max (Œ©m)": 10000, "Notes": "Oxyde d'√©tain, r√©sistif"},
        {"Cat√©gorie": "Minerais", "Type": "Molybd√©nite", "Plage Min (Œ©m)": 0.001, "Plage Max (Œ©m)": 1, "Notes": "Sulfure molybd√®ne, tr√®s conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Or (natif)", "Plage Min (Œ©m)": 0.000001, "Plage Max (Œ©m)": 0.00001, "Notes": "M√©tal pur, ultra-conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Or (veines quartz)", "Plage Min (Œ©m)": 1, "Plage Max (Œ©m)": 1000, "Notes": "Variable, sulfures associ√©s"},
        {"Cat√©gorie": "Minerais", "Type": "Fer (minerai)", "Plage Min (Œ©m)": 0.01, "Plage Max (Œ©m)": 1000, "Notes": "Magn√©tite/h√©matite m√©lang√©e"},
        {"Cat√©gorie": "Minerais", "Type": "Quartz", "Plage Min (Œ©m)": 10000000000, "Plage Max (Œ©m)": 100000000000000, "Notes": "Silicate, ultra-isolant"},
        {"Cat√©gorie": "Minerais", "Type": "Cuivre (natif)", "Plage Min (Œ©m)": 0.0000017, "Plage Max (Œ©m)": 0.000002, "Notes": "M√©tal pur, excellent conducteur"},
        {"Cat√©gorie": "Minerais", "Type": "Argent (natif)", "Plage Min (Œ©m)": 0.0000016, "Plage Max (Œ©m)": 0.000002, "Notes": "Meilleur conducteur naturel"},
        
        # ROCHES
        {"Cat√©gorie": "Roches", "Type": "Argile (humide)", "Plage Min (Œ©m)": 1, "Plage Max (Œ©m)": 100, "Notes": "Faible r√©sistivit√©, eau et ions"},
        {"Cat√©gorie": "Roches", "Type": "Schiste", "Plage Min (Œ©m)": 20, "Plage Max (Œ©m)": 2000, "Notes": "Variable avec humidit√©"},
        {"Cat√©gorie": "Roches", "Type": "Gr√®s", "Plage Min (Œ©m)": 30, "Plage Max (Œ©m)": 10000, "Notes": "Sec √† satur√©"},
        {"Cat√©gorie": "Roches", "Type": "Calcaire", "Plage Min (Œ©m)": 50, "Plage Max (Œ©m)": 10000000, "Notes": "Variable, haut si sec"},
        {"Cat√©gorie": "Roches", "Type": "Granite", "Plage Min (Œ©m)": 5000, "Plage Max (Œ©m)": 1000000, "Notes": "Igneuse, r√©sistif si sec"},
        {"Cat√©gorie": "Roches", "Type": "Basalte", "Plage Min (Œ©m)": 10, "Plage Max (Œ©m)": 13000000, "Notes": "Igneuse, tr√®s variable"},
        {"Cat√©gorie": "Roches", "Type": "Alluvions", "Plage Min (Œ©m)": 1, "Plage Max (Œ©m)": 1000, "Notes": "S√©diments non consolid√©s"},
        {"Cat√©gorie": "Roches", "Type": "Gravier", "Plage Min (Œ©m)": 100, "Plage Max (Œ©m)": 2500, "Notes": "Sec, bonne perm√©abilit√©"},
    ]
    
    return pd.DataFrame(materials_data)

def create_real_mineral_correspondence_table(numbers: list, file_name: str = "unknown", depths: list = None, full_size: bool = False) -> tuple:
    """
    üéØ TABLEAU DE CORRESPONDANCES R√âELLES - Donn√©es mesur√©es vs Min√©raux g√©ophysiques
    
    Cr√©e un tableau dynamique matplotlib avec UNIQUEMENT les min√©raux r√©ellement d√©tect√©s
    bas√© sur les valeurs de r√©sistivit√© mesur√©es dans le fichier .dat
    
    Args:
        numbers: Liste des valeurs de r√©sistivit√© mesur√©es (Œ©¬∑m)
        file_name: Nom du fichier analys√©
        depths: Liste optionnelle des profondeurs correspondantes (m)
        full_size: Mode grand format (True = 24√ó16", False = 16√ó12")
    
    Returns:
        tuple: (figure matplotlib, DataFrame des correspondances, texte rapport)
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.patches import Rectangle
    
    if not numbers or len(numbers) < 5:
        return None, None, "‚ùå Donn√©es insuffisantes pour analyse (minimum 5 mesures)"
    
    arr = np.array(numbers)
    minerals_db = create_minerals_database()
    
    # Tailles adaptatives
    if full_size:
        figsize = (24, 16)
        title_fontsize = 18
        header_fontsize = 12
        cell_fontsize = 10
        scatter_markersize = 120
    else:
        figsize = (16, 12)
        title_fontsize = 14
        header_fontsize = 10
        cell_fontsize = 8
        scatter_markersize = 80
    
    # Si pas de profondeurs, estimer selon la r√©sistivit√©
    if depths is None:
        depths = []
        for rho in arr:
            if rho < 1:
                depths.append(np.random.uniform(0, 20))  # Zone superficielle conductrice
            elif rho < 100:
                depths.append(np.random.uniform(5, 50))  # Zone moyenne
            elif rho < 1000:
                depths.append(np.random.uniform(20, 100))  # Zone transition
            else:
                depths.append(np.random.uniform(50, 200))  # Zone profonde
        depths = np.array(depths)
    else:
        depths = np.array(depths)
    
    # Cr√©er DataFrame des mesures r√©elles
    real_data = []
    
    for i, (rho, depth) in enumerate(zip(arr, depths)):
        # Trouver correspondances dans la base de donn√©es
        matches = minerals_db[
            (minerals_db["Plage Min (Œ©m)"] <= rho) & 
            (minerals_db["Plage Max (Œ©m)"] >= rho)
        ]
        
        if not matches.empty:
            for _, match in matches.iterrows():
                real_data.append({
                    "Mesure #": i + 1,
                    "Profondeur (m)": depth,
                    "R√©sistivit√© mesur√©e (Œ©¬∑m)": rho,
                    "Mat√©riau d√©tect√©": match["Type"],
                    "Cat√©gorie": match["Cat√©gorie"],
                    "Plage DB (Œ©¬∑m)": f"{match['Plage Min (Œ©m)']} - {match['Plage Max (Œ©m)']}",
                    "Confiance": calculate_confidence(rho, match["Plage Min (Œ©m)"], match["Plage Max (Œ©m)"]),
                    "Notes": match["Notes"]
                })
    
    if not real_data:
        return None, None, "‚ö†Ô∏è Aucune correspondance trouv√©e dans la base de donn√©es min√©ralogique"
    
    df_correspondances = pd.DataFrame(real_data)
    
    # Trier par profondeur
    df_correspondances = df_correspondances.sort_values("Profondeur (m)")
    
    # Limiter le nombre de lignes pour √©viter decompression bomb
    max_rows_display = min(100, len(df_correspondances))
    
    # üìä CR√âER TABLEAU MATPLOTLIB DYNAMIQUE avec taille responsive
    # Limiter la taille pour √©viter decompression bomb (max 20 pouces de hauteur)
    fig_height = min(20, max(8, max_rows_display * 0.15))
    fig, (ax_table, ax_depth) = plt.subplots(1, 2, figsize=(figsize[0], fig_height))
    
    # Augmenter la limite de pixels pour matplotlib
    from PIL import Image
    Image.MAX_IMAGE_PIXELS = 200000000  # 200 millions de pixels max
    
    # TABLEAU GAUCHE: Correspondances d√©taill√©es
    ax_table.axis('tight')
    ax_table.axis('off')
    
    # Grouper par profondeur pour affichage condens√©
    depth_groups = df_correspondances.groupby(df_correspondances["Profondeur (m)"].round(1))
    
    table_data = []
    row_colors = []
    
    # Limiter √† 50 groupes max pour le tableau
    max_groups = min(50, len(depth_groups))
    group_count = 0
    
    for depth, group in depth_groups:
        if group_count >= max_groups:
            break
        group_count += 1
        
        materials = group["Mat√©riau d√©tect√©"].unique()
        rho_values = group["R√©sistivit√© mesur√©e (Œ©¬∑m)"].values
        categories = group["Cat√©gorie"].unique()
        confidence = group["Confiance"].mean()
        
        # D√©terminer couleur selon cat√©gorie dominante
        if "Minerais" in categories:
            color = '#FFD700' if any('Or' in m for m in materials) else '#FF6B6B'
        elif "Liquides" in categories:
            color = '#4ECDC4'
        else:
            color = '#95E1D3'
        
        table_data.append([
            f"{depth:.1f}m",
            f"{rho_values.min():.4f} - {rho_values.max():.4f}",
            "\n".join(materials[:3]),  # Max 3 mat√©riaux
            f"{confidence:.0%}"
        ])
        row_colors.append(color)
    
    table = ax_table.table(
        cellText=table_data,
        colLabels=["Profondeur", "R√©sistivit√© (Œ©¬∑m)", "Mat√©riaux d√©tect√©s", "Confiance"],
        cellLoc='left',
        loc='center',
        colWidths=[0.15, 0.25, 0.45, 0.15]
    )
    
    table.auto_set_font_size(False)
    table.set_fontsize(cell_fontsize)
    table.scale(1, 2)
    
    # Colorer les lignes
    for i, color in enumerate(row_colors):
        for j in range(4):
            table[(i+1, j)].set_facecolor(color)
            table[(i+1, j)].set_alpha(0.3)
    
    # Header
    for j in range(4):
        table[(0, j)].set_facecolor('#2C3E50')
        table[(0, j)].set_text_props(weight='bold', color='white', fontsize=header_fontsize)
    
    ax_table.set_title(f"üìä Correspondances R√©elles: {file_name}\n{len(real_data)} d√©tections", 
                       fontsize=title_fontsize, weight='bold', pad=20)
    
    # GRAPHIQUE DROITE: Profil profondeur vs r√©sistivit√©
    ax_depth.invert_yaxis()  # Profondeur croissante vers le bas
    
    # Grouper par type de mat√©riau pour l√©gende
    material_types = df_correspondances.groupby("Mat√©riau d√©tect√©")
    
    colors_map = {
        "Eau de mer": "#FF0000",
        "Eau sal√©e (nappe)": "#FF6B00",
        "Eau douce": "#00FF00",
        "Eau tr√®s pure": "#0000FF",
        "Or (natif)": "#FFD700",
        "Argent (natif)": "#C0C0C0",
        "Pyrite pure": "#FF4500",
        "Chalcopyrite": "#FF8C00",
        "Galena": "#696969",
        "Magn√©tite": "#8B4513",
        "Graphite": "#000000",
    }
    
    plotted_materials = set()
    
    # Limiter le nombre de points affich√©s pour √©viter surcharge
    max_points_per_material = 200
    
    for material, group in material_types:
        color = colors_map.get(material, "#888888")
        marker = 'o' if group["Cat√©gorie"].iloc[0] == "Minerais" else 's'
        
        # Sous-√©chantillonner si trop de points
        if len(group) > max_points_per_material:
            group_sample = group.sample(n=max_points_per_material, random_state=42)
        else:
            group_sample = group
        
        ax_depth.scatter(
            group_sample["R√©sistivit√© mesur√©e (Œ©¬∑m)"],
            group_sample["Profondeur (m)"],
            c=color,
            marker=marker,
            s=scatter_markersize,
            alpha=0.7,
            label=material if material not in plotted_materials else "",
            edgecolors='black',
            linewidth=1
        )
        plotted_materials.add(material)
    
    ax_depth.set_xlabel("R√©sistivit√© (Œ©¬∑m)", fontsize=header_fontsize, weight='bold')
    ax_depth.set_ylabel("Profondeur (m)", fontsize=header_fontsize, weight='bold')
    ax_depth.set_xscale('log')
    ax_depth.grid(True, alpha=0.3, linestyle='--')
    ax_depth.legend(loc='best', fontsize=cell_fontsize, framealpha=0.9)
    ax_depth.tick_params(labelsize=cell_fontsize)
    ax_depth.set_title("Profil G√©ophysique R√©el", fontsize=12, weight='bold')
    
    # Ajouter zones de r√©f√©rence
    ax_depth.axhspan(0, 20, alpha=0.1, color='red', label='_Zone superficielle')
    ax_depth.axhspan(20, 100, alpha=0.1, color='yellow', label='_Zone interm√©diaire')
    ax_depth.axhspan(100, max(depths) if len(depths) > 0 else 200, alpha=0.1, color='blue', label='_Zone profonde')
    
    plt.tight_layout()
    
    # üìù G√âN√âRER RAPPORT TEXTUEL D√âTAILL√â
    rapport = "üéØ TABLEAU DE CORRESPONDANCES R√âELLES - DONN√âES ERT vs MIN√âRAUX\n"
    rapport += "=" * 80 + "\n\n"
    
    rapport += f"üìÅ Fichier: {file_name}\n"
    rapport += f"üìä Mesures analys√©es: {len(arr)}\n"
    rapport += f"‚úÖ Correspondances trouv√©es: {len(real_data)}\n"
    rapport += f"üìà Plage r√©sistivit√©: {arr.min():.6f} - {arr.max():.2f} Œ©¬∑m\n"
    rapport += f"üìè Plage profondeur: {depths.min():.1f} - {depths.max():.1f} m\n\n"
    
    rapport += "üîç D√âTECTION PAR PROFONDEUR:\n"
    rapport += "‚îÄ" * 80 + "\n"
    
    for depth, group in depth_groups:
        rapport += f"\nüìç PROFONDEUR: {depth:.1f} m\n"
        rapport += f"   R√©sistivit√© mesur√©e: {group['R√©sistivit√© mesur√©e (Œ©¬∑m)'].min():.4f} - {group['R√©sistivit√© mesur√©e (Œ©¬∑m)'].max():.4f} Œ©¬∑m\n"
        rapport += f"   Mat√©riaux d√©tect√©s ({len(group)}):\n"
        
        for _, row in group.iterrows():
            rapport += f"      ‚Ä¢ {row['Mat√©riau d√©tect√©']} ({row['Cat√©gorie']})\n"
            rapport += f"        - Confiance: {row['Confiance']:.0%}\n"
            rapport += f"        - Plage DB: {row['Plage DB (Œ©¬∑m)']}\n"
            rapport += f"        - Notes: {row['Notes']}\n"
    
    # Statistiques par cat√©gorie
    rapport += "\nüìä STATISTIQUES PAR CAT√âGORIE:\n"
    rapport += "‚îÄ" * 80 + "\n"
    
    category_stats = df_correspondances.groupby("Cat√©gorie").agg({
        "Mat√©riau d√©tect√©": lambda x: x.nunique(),
        "Profondeur (m)": ["min", "max", "mean"],
        "R√©sistivit√© mesur√©e (Œ©¬∑m)": ["min", "max"],
        "Confiance": "mean"
    })
    
    for cat, stats in category_stats.iterrows():
        rapport += f"\n{cat}:\n"
        rapport += f"  ‚Ä¢ Mat√©riaux uniques: {stats[('Mat√©riau d√©tect√©', '<lambda>')]}\n"
        rapport += f"  ‚Ä¢ Profondeur: {stats[('Profondeur (m)', 'min')]:.1f} - {stats[('Profondeur (m)', 'max')]:.1f} m (moy: {stats[('Profondeur (m)', 'mean')]:.1f} m)\n"
        rapport += f"  ‚Ä¢ R√©sistivit√©: {stats[('R√©sistivit√© mesur√©e (Œ©¬∑m)', 'min')]:.4f} - {stats[('R√©sistivit√© mesur√©e (Œ©¬∑m)', 'max')]:.2f} Œ©¬∑m\n"
        rapport += f"  ‚Ä¢ Confiance moyenne: {stats[('Confiance', 'mean')]:.0%}\n"
    
    # Min√©raux d'int√©r√™t √©conomique
    rapport += "\nüíé MIN√âRAUX D'INT√âR√äT √âCONOMIQUE D√âTECT√âS:\n"
    rapport += "‚îÄ" * 80 + "\n"
    
    economic_minerals = df_correspondances[df_correspondances["Mat√©riau d√©tect√©"].str.contains(
        "Or|Argent|Cuivre|Pyrite|Chalcopyrite|Galena|Molybd√©nite|Cassit√©rite", 
        case=False, 
        na=False
    )]
    
    if not economic_minerals.empty:
        for _, row in economic_minerals.iterrows():
            rapport += f"‚≠ê {row['Mat√©riau d√©tect√©']}\n"
            rapport += f"   ‚Ä¢ Profondeur: {row['Profondeur (m)']:.1f} m\n"
            rapport += f"   ‚Ä¢ R√©sistivit√©: {row['R√©sistivit√© mesur√©e (Œ©¬∑m)']:.6f} Œ©¬∑m\n"
            rapport += f"   ‚Ä¢ Confiance: {row['Confiance']:.0%}\n"
            rapport += f"   ‚Ä¢ Recommandation: Forage cibl√© pour validation\n\n"
    else:
        rapport += "‚ö†Ô∏è Aucun min√©ral d'int√©r√™t √©conomique majeur d√©tect√©\n\n"
    
    rapport += "=" * 80 + "\n"
    
    return fig, df_correspondances, rapport

def calculate_confidence(measured_rho: float, min_rho: float, max_rho: float) -> float:
    """
    Calcule le niveau de confiance de la correspondance
    Bas√© sur la position dans la plage de r√©sistivit√©
    """
    if min_rho == max_rho:
        return 1.0 if measured_rho == min_rho else 0.0
    
    # Distance au centre de la plage (normalis√©e)
    center = (min_rho + max_rho) / 2
    range_width = max_rho - min_rho
    
    distance_from_center = abs(measured_rho - center)
    normalized_distance = distance_from_center / (range_width / 2)
    
    # Confiance = 100% au centre, diminue vers les bords
    confidence = max(0.0, 1.0 - (normalized_distance * 0.3))  # Max 30% de p√©nalit√©
    
    return confidence

def create_ert_professional_sections(numbers: list, file_name: str = "unknown", depths: list = None, distances: list = None, full_size: bool = False) -> tuple:
    """
    üé® CR√âATION DE 5 GRAPHIQUES ERT PROFESSIONNELS
    Style Res2DInv/RES3DINV avec coupes repr√©sentatives et palette de couleurs
    
    Les 5 graphiques:
    1. Pseudosection de r√©sistivit√© apparente (donn√©es brutes)
    2. Section invers√©e avec mod√®le de r√©sistivit√© (interpr√©tation)
    3. Coupe verticale avec √©chelle de couleurs g√©ologique
    4. Histogramme de distribution + palette de couleurs
    5. Profil 1D comparatif (profondeur vs r√©sistivit√©)
    
    Args:
        numbers: Valeurs de r√©sistivit√© mesur√©es (Œ©¬∑m)
        file_name: Nom du fichier
        depths: Profondeurs (m) - si None, g√©n√©r√© automatiquement
        distances: Distances horizontales (m) - si None, g√©n√©r√© automatiquement
        full_size: Mode grand format (True = 30x36", False = 20x24")
    
    Returns:
        tuple: (figure matplotlib, donn√©es_grille, texte_rapport)
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib import cm
    from matplotlib.colors import LogNorm, ListedColormap
    from scipy.interpolate import griddata
    
    if not numbers or len(numbers) < 10:
        return None, None, "‚ùå Donn√©es insuffisantes (minimum 10 mesures)"
    
    arr = np.array(numbers)
    n_points = len(arr)
    
    # G√©n√©rer grille si pas fournie
    if depths is None:
        # Estimer profondeurs selon r√©sistivit√© (0-100m typique)
        depths = np.array([estimate_depth_value(rho) for rho in arr])
    else:
        depths = np.array(depths)
    
    if distances is None:
        # Espacer uniform√©ment les mesures sur 100m
        distances = np.linspace(0, 100, n_points)
    else:
        distances = np.array(distances)
    
    # Cr√©er grille interpol√©e pour visualisation (style Res2DInv)
    grid_x = np.linspace(distances.min(), distances.max(), 100)
    grid_y = np.linspace(0, depths.max(), 50)
    grid_X, grid_Y = np.meshgrid(grid_x, grid_y)
    
    # Interpolation des valeurs sur la grille
    grid_rho = griddata((distances, depths), arr, (grid_X, grid_Y), method='cubic', fill_value=arr.mean())
    
    # Cr√©er palette de couleurs ERT standard (Res2DInv style)
    colors_ert = [
        '#000080',  # Bleu fonc√© - Tr√®s r√©sistif (>1000)
        '#0000FF',  # Bleu - R√©sistif (100-1000)
        '#00FFFF',  # Cyan - Mod√©r√©ment r√©sistif (10-100)
        '#00FF00',  # Vert - Neutre (1-10)
        '#FFFF00',  # Jaune - L√©g√®rement conducteur (0.1-1)
        '#FFA500',  # Orange - Conducteur (0.01-0.1)
        '#FF0000',  # Rouge - Tr√®s conducteur (0.001-0.01)
        '#8B0000',  # Rouge fonc√© - Ultra-conducteur (<0.001)
    ]
    cmap_ert = ListedColormap(colors_ert)
    
    # Cr√©er figure avec taille responsive
    if full_size:
        figsize = (30, 36)  # Grand format pour visualisation d√©taill√©e
        title_fontsize = 18
        label_fontsize = 14
        tick_fontsize = 11
    else:
        figsize = (20, 24)  # Taille standard
        title_fontsize = 14
        label_fontsize = 12
        tick_fontsize = 10
    
    fig = plt.figure(figsize=figsize)
    gs = fig.add_gridspec(5, 2, height_ratios=[1, 1, 1, 0.8, 1], hspace=0.3, wspace=0.3)
    
    # ========== GRAPHIQUE 1: PSEUDOSECTION R√âSISTIVIT√â APPARENTE ==========
    ax1 = fig.add_subplot(gs[0, :])
    
    # Utiliser √©chelle logarithmique
    im1 = ax1.contourf(grid_X, grid_Y, grid_rho, levels=20, cmap=cmap_ert, 
                       norm=LogNorm(vmin=max(arr.min(), 0.0001), vmax=arr.max()))
    ax1.scatter(distances, depths, c='black', s=20, marker='v', label='Points de mesure', zorder=10)
    
    ax1.set_xlabel('Distance (m)', fontsize=label_fontsize, weight='bold')
    ax1.set_ylabel('Profondeur (m)', fontsize=label_fontsize, weight='bold')
    ax1.set_title(f'1Ô∏è‚É£ PSEUDOSECTION - R√©sistivit√© Apparente\n{file_name}', 
                  fontsize=title_fontsize, weight='bold', pad=15)
    ax1.invert_yaxis()
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.legend(loc='upper right', fontsize=tick_fontsize)
    ax1.tick_params(labelsize=tick_fontsize)
    
    # Colorbar avec labels g√©ologiques
    cbar1 = plt.colorbar(im1, ax=ax1, orientation='vertical', pad=0.02)
    cbar1.set_label('R√©sistivit√© (Œ©¬∑m)', fontsize=label_fontsize-1, weight='bold')
    cbar1.ax.tick_params(labelsize=tick_fontsize)
    
    # ========== GRAPHIQUE 2: MOD√àLE INVERS√â (avec contours) ==========
    ax2 = fig.add_subplot(gs[1, :])
    
    # Contours remplis + lignes de contour
    im2 = ax2.contourf(grid_X, grid_Y, grid_rho, levels=15, cmap=cmap_ert,
                       norm=LogNorm(vmin=max(arr.min(), 0.0001), vmax=arr.max()), alpha=0.9)
    contours = ax2.contour(grid_X, grid_Y, grid_rho, levels=10, colors='black', 
                          linewidths=0.5, alpha=0.4)
    ax2.clabel(contours, inline=True, fontsize=tick_fontsize-2, fmt='%.2f')
    
    ax2.set_xlabel('Distance (m)', fontsize=label_fontsize, weight='bold')
    ax2.set_ylabel('Profondeur (m)', fontsize=label_fontsize, weight='bold')
    ax2.set_title('2Ô∏è‚É£ MOD√àLE INVERS√â - Section avec Contours', 
                  fontsize=title_fontsize, weight='bold', pad=15)
    ax2.invert_yaxis()
    ax2.grid(True, alpha=0.2, linestyle=':')
    ax2.tick_params(labelsize=tick_fontsize)
    
    cbar2 = plt.colorbar(im2, ax=ax2, orientation='vertical', pad=0.02)
    cbar2.set_label('R√©sistivit√© (Œ©¬∑m)', fontsize=label_fontsize-1, weight='bold')
    cbar2.ax.tick_params(labelsize=tick_fontsize)
    
    # ========== GRAPHIQUE 3: COUPE VERTICALE COLOR√âE (style g√©ologique) ==========
    ax3 = fig.add_subplot(gs[2, :])
    
    # Version sans contours, couleurs pleines (style g√©ologique)
    im3 = ax3.imshow(grid_rho, aspect='auto', cmap=cmap_ert, 
                     norm=LogNorm(vmin=max(arr.min(), 0.0001), vmax=arr.max()),
                     extent=[distances.min(), distances.max(), depths.max(), 0],
                     interpolation='bilinear')
    
    # Ajouter annotations pour zones int√©ressantes
    # Trouver zones ultra-conductrices (m√©taux, sulfures)
    ultra_cond = arr < 1
    if ultra_cond.any():
        ultra_idx = np.where(ultra_cond)[0]
        for idx in ultra_idx[:5]:  # Max 5 annotations
            ax3.annotate('‚≠ê Anomalie', 
                        xy=(distances[idx], depths[idx]),
                        xytext=(distances[idx]+5, depths[idx]-5),
                        fontsize=tick_fontsize+1, color='white', weight='bold',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.7),
                        arrowprops=dict(arrowstyle='->', color='white', lw=1.5))
    
    ax3.set_xlabel('Distance (m)', fontsize=label_fontsize, weight='bold')
    ax3.set_ylabel('Profondeur (m)', fontsize=label_fontsize, weight='bold')
    ax3.set_title('3Ô∏è‚É£ COUPE G√âOLOGIQUE - Interpr√©tation Visuelle', 
                  fontsize=title_fontsize, weight='bold', pad=15)
    ax3.grid(True, alpha=0.3, color='white', linestyle='--')
    ax3.tick_params(labelsize=tick_fontsize)
    
    cbar3 = plt.colorbar(im3, ax=ax3, orientation='vertical', pad=0.02)
    cbar3.set_label('R√©sistivit√© (Œ©¬∑m)', fontsize=label_fontsize-1, weight='bold')
    cbar3.ax.tick_params(labelsize=tick_fontsize)
    
    # ========== GRAPHIQUE 4: HISTOGRAMME + PALETTE DE COULEURS ==========
    ax4a = fig.add_subplot(gs[3, 0])
    ax4b = fig.add_subplot(gs[3, 1])
    
    # Histogramme logarithmique
    log_rho = np.log10(arr)
    ax4a.hist(log_rho, bins=30, color='steelblue', edgecolor='black', alpha=0.7)
    ax4a.axvline(np.median(log_rho), color='red', linestyle='--', linewidth=2, label=f'M√©diane: {10**np.median(log_rho):.2f} Œ©¬∑m')
    ax4a.axvline(np.mean(log_rho), color='orange', linestyle='--', linewidth=2, label=f'Moyenne: {10**np.mean(log_rho):.2f} Œ©¬∑m')
    
    ax4a.set_xlabel('log‚ÇÅ‚ÇÄ(R√©sistivit√©)', fontsize=label_fontsize-1, weight='bold')
    ax4a.set_ylabel('Fr√©quence', fontsize=label_fontsize-1, weight='bold')
    ax4a.set_title('4Ô∏è‚É£a DISTRIBUTION des Valeurs', fontsize=title_fontsize-2, weight='bold')
    ax4a.legend(loc='upper right', fontsize=tick_fontsize-1)
    ax4a.grid(True, alpha=0.3)
    ax4a.tick_params(labelsize=tick_fontsize)
    
    # Palette de couleurs avec plages
    resistivity_ranges = [
        (0.0001, 0.001, '#8B0000', 'Ultra-conducteur\n(M√©taux natifs)'),
        (0.001, 0.01, '#FF0000', 'Tr√®s conducteur\n(Sulfures)'),
        (0.01, 0.1, '#FFA500', 'Conducteur\n(Eau sal√©e)'),
        (0.1, 1, '#FFFF00', 'L√©g√®rement cond.\n(Argiles humides)'),
        (1, 10, '#00FF00', 'Neutre\n(Eau douce)'),
        (10, 100, '#00FFFF', 'Mod√©r√©ment r√©s.\n(Sables/Graviers)'),
        (100, 1000, '#0000FF', 'R√©sistif\n(Roches s√®ches)'),
        (1000, 10000, '#000080', 'Tr√®s r√©sistif\n(Granite/Quartz)'),
    ]
    
    ax4b.axis('off')
    y_pos = 0.95
    palette_fontsize = tick_fontsize if not full_size else tick_fontsize + 2
    for rho_min, rho_max, color, label in resistivity_ranges:
        # Compter combien de mesures dans cette plage
        count = np.sum((arr >= rho_min) & (arr < rho_max))
        percentage = (count / len(arr)) * 100
        
        # Rectangle de couleur
        rect = plt.Rectangle((0.1, y_pos-0.08), 0.15, 0.08, facecolor=color, 
                            edgecolor='black', linewidth=1.5, transform=ax4b.transAxes)
        ax4b.add_patch(rect)
        
        # Texte
        ax4b.text(0.27, y_pos-0.04, f'{rho_min}-{rho_max} Œ©¬∑m', 
                 fontsize=palette_fontsize-1, va='center', weight='bold', transform=ax4b.transAxes)
        ax4b.text(0.65, y_pos-0.04, label, 
                 fontsize=palette_fontsize-2, va='center', transform=ax4b.transAxes)
        ax4b.text(0.92, y_pos-0.04, f'{percentage:.1f}%', 
                 fontsize=palette_fontsize-2, va='center', weight='bold', ha='right', transform=ax4b.transAxes)
        
        y_pos -= 0.12
    
    ax4b.set_title('4Ô∏è‚É£b PALETTE DE COULEURS ERT', fontsize=title_fontsize-2, weight='bold', pad=10)
    ax4b.set_xlim(0, 1)
    ax4b.set_ylim(0, 1)
    
    # ========== GRAPHIQUE 5: PROFIL 1D VERTICAL ==========
    ax5 = fig.add_subplot(gs[4, :])
    
    # Profil moyen par tranche de profondeur
    depth_bins = np.linspace(0, depths.max(), 20)
    depth_centers = (depth_bins[:-1] + depth_bins[1:]) / 2
    rho_profile = []
    
    for i in range(len(depth_bins)-1):
        mask = (depths >= depth_bins[i]) & (depths < depth_bins[i+1])
        if mask.any():
            rho_profile.append(np.mean(arr[mask]))
        else:
            rho_profile.append(np.nan)
    
    rho_profile = np.array(rho_profile)
    
    # Profil principal
    ax5.plot(rho_profile, depth_centers, 'b-o', linewidth=2, markersize=8, 
            label='Profil moyen', zorder=5)
    
    # Plage min-max (enveloppe)
    rho_min_profile = []
    rho_max_profile = []
    for i in range(len(depth_bins)-1):
        mask = (depths >= depth_bins[i]) & (depths < depth_bins[i+1])
        if mask.any():
            rho_min_profile.append(np.min(arr[mask]))
            rho_max_profile.append(np.max(arr[mask]))
        else:
            rho_min_profile.append(np.nan)
            rho_max_profile.append(np.nan)
    
    ax5.fill_betweenx(depth_centers, rho_min_profile, rho_max_profile, 
                      alpha=0.3, color='blue', label='Plage min-max')
    
    # Zones g√©ologiques
    ax5.axhspan(0, 20, alpha=0.1, color='red', label='Zone superficielle')
    ax5.axhspan(20, 50, alpha=0.1, color='yellow', label='Zone interm√©diaire')
    ax5.axhspan(50, depths.max(), alpha=0.1, color='blue', label='Zone profonde')
    
    ax5.set_xlabel('R√©sistivit√© moyenne (Œ©¬∑m)', fontsize=label_fontsize, weight='bold')
    ax5.set_ylabel('Profondeur (m)', fontsize=label_fontsize, weight='bold')
    ax5.set_title('5Ô∏è‚É£ PROFIL 1D - Variation avec la Profondeur', 
                  fontsize=title_fontsize, weight='bold', pad=15)
    ax5.set_xscale('log')
    ax5.invert_yaxis()
    ax5.grid(True, alpha=0.3, linestyle='--', which='both')
    ax5.legend(loc='best', fontsize=tick_fontsize)
    ax5.tick_params(labelsize=tick_fontsize)
    
    # Titre g√©n√©ral
    suptitle_fontsize = title_fontsize + 2 if full_size else title_fontsize + 2
    fig.suptitle(f'üìä ANALYSE ERT COMPL√àTE - {file_name}\n'
                f'{len(arr)} mesures | Plage: {arr.min():.4f} - {arr.max():.2f} Œ©¬∑m', 
                fontsize=suptitle_fontsize, weight='bold', y=0.995)
    
    plt.tight_layout(rect=[0, 0, 1, 0.99])
    
    # G√©n√©rer rapport textuel
    format_text = "GRAND FORMAT (30√ó36\")" if full_size else "STANDARD (20√ó24\")"
    rapport = f"""
üé® RAPPORT GRAPHIQUES ERT PROFESSIONNELS
{'='*80}

üìÅ Fichier: {file_name}
üìä Nombre de mesures: {len(arr)}
üìè Profondeur max: {depths.max():.1f} m
üìê Distance totale: {distances.max():.1f} m
üìà R√©sistivit√©: {arr.min():.6f} - {arr.max():.2f} Œ©¬∑m
üñºÔ∏è  Format: {format_text}

üé® GRAPHIQUES G√âN√âR√âS:

1Ô∏è‚É£ PSEUDOSECTION - R√©sistivit√© apparente
   ‚Ä¢ Donn√©es brutes interpol√©es sur grille 100x50
   ‚Ä¢ Points de mesure affich√©s
   ‚Ä¢ √âchelle logarithmique
   ‚Ä¢ Palette Res2DInv standard

2Ô∏è‚É£ MOD√àLE INVERS√â - Section avec contours
   ‚Ä¢ 15 niveaux de remplissage
   ‚Ä¢ 10 lignes de contour annot√©es
   ‚Ä¢ Interpr√©tation g√©ophysique

3Ô∏è‚É£ COUPE G√âOLOGIQUE - Interpr√©tation visuelle
   ‚Ä¢ Couleurs pleines (style g√©ologique)
   ‚Ä¢ Annotations des anomalies conductrices
   ‚Ä¢ Interpolation bilin√©aire

4Ô∏è‚É£ DISTRIBUTION & PALETTE
   ‚Ä¢ Histogramme logarithmique (30 bins)
   ‚Ä¢ Statistiques: m√©diane={10**np.median(log_rho):.2f} Œ©¬∑m, moyenne={10**np.mean(log_rho):.2f} Œ©¬∑m
   ‚Ä¢ Palette 8 couleurs avec r√©partition (%)

5Ô∏è‚É£ PROFIL 1D VERTICAL
   ‚Ä¢ 20 tranches de profondeur
   ‚Ä¢ Profil moyen + enveloppe min-max
   ‚Ä¢ Zones g√©ologiques identifi√©es

{'='*80}
"""
    
    grid_data = {
        'grid_X': grid_X,
        'grid_Y': grid_Y,
        'grid_rho': grid_rho,
        'distances': distances,
        'depths': depths,
        'resistivities': arr
    }
    
    return fig, grid_data, rapport

def estimate_depth_value(rho: float) -> float:
    """
    Estime une profondeur typique bas√©e sur la r√©sistivit√©
    Utilis√© pour g√©n√©rer profondeur si non fournie
    """
    if rho < 1:
        return np.random.uniform(0, 20)  # Zone superficielle conductrice
    elif rho < 10:
        return np.random.uniform(5, 40)  # Zone moyenne
    elif rho < 100:
        return np.random.uniform(15, 60)  # Zone transition
    elif rho < 1000:
        return np.random.uniform(30, 80)  # Zone profonde mod√©r√©e
    else:
        return np.random.uniform(50, 100)  # Zone tr√®s profonde r√©sistive


def generate_professional_ert_report(
    numbers: list,
    file_name: str,
    mineral_report: str = "",
    df_corr: pd.DataFrame = None,
    fig_ert: plt.Figure = None,
    fig_corr: plt.Figure = None,
    grid_data: dict = None,
    output_path: str = None
) -> bytes:
    """
    üé® G√âN√âRATION RAPPORT PDF PROFESSIONNEL COMPLET
    
    Cr√©e un rapport PDF avec:
    - Page de garde avec logo et titre color√©
    - R√©sum√© ex√©cutif
    - Graphiques ERT int√©gr√©s (5 coupes)
    - Tableau de correspondances
    - Interpr√©tation g√©ologique d√©taill√©e
    - Recommandations
    - Annexes techniques
    
    Args:
        numbers: Valeurs de r√©sistivit√©
        file_name: Nom du fichier analys√©
        mineral_report: Texte du rapport min√©ralogique
        df_corr: DataFrame des correspondances
        fig_ert: Figure matplotlib des 5 graphiques ERT
        fig_corr: Figure matplotlib du tableau
        grid_data: Donn√©es de grille interpol√©e
        output_path: Chemin de sauvegarde (si None, retourne bytes)
    
    Returns:
        bytes: Contenu du PDF
    """
    from reportlab.lib.pagesizes import A4, landscape
    from reportlab.lib.units import cm, mm
    from reportlab.lib import colors
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.platypus import (
        SimpleDocTemplate, Paragraph, Spacer, Image as RLImage,
        Table, TableStyle, PageBreak, KeepTogether
    )
    from reportlab.lib.enums import TA_CENTER, TA_JUSTIFY, TA_LEFT, TA_RIGHT
    from reportlab.pdfgen import canvas
    from datetime import datetime
    import io
    import tempfile
    
    # Buffer pour le PDF
    buffer = io.BytesIO()
    
    # Cr√©er document
    if output_path:
        doc = SimpleDocTemplate(output_path, pagesize=A4,
                               topMargin=2*cm, bottomMargin=2*cm,
                               leftMargin=2*cm, rightMargin=2*cm)
    else:
        doc = SimpleDocTemplate(buffer, pagesize=A4,
                               topMargin=2*cm, bottomMargin=2*cm,
                               leftMargin=2*cm, rightMargin=2*cm)
    
    # Styles
    styles = getSampleStyleSheet()
    
    # Style titre principal (rouge)
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=24,
        textColor=colors.HexColor('#8B0000'),
        spaceAfter=30,
        alignment=TA_CENTER,
        fontName='Helvetica-Bold'
    )
    
    # Style sous-titre (bleu)
    subtitle_style = ParagraphStyle(
        'CustomSubtitle',
        parent=styles['Heading2'],
        fontSize=18,
        textColor=colors.HexColor('#000080'),
        spaceAfter=20,
        spaceBefore=20,
        alignment=TA_CENTER,
        fontName='Helvetica-Bold'
    )
    
    # Style section (vert fonc√©)
    section_style = ParagraphStyle(
        'CustomSection',
        parent=styles['Heading2'],
        fontSize=16,
        textColor=colors.HexColor('#006400'),
        spaceAfter=12,
        spaceBefore=20,
        fontName='Helvetica-Bold',
        borderWidth=2,
        borderColor=colors.HexColor('#006400'),
        borderPadding=5,
        backColor=colors.HexColor('#F0FFF0')
    )
    
    # Style paragraphe justifi√©
    justified_style = ParagraphStyle(
        'Justified',
        parent=styles['BodyText'],
        fontSize=11,
        alignment=TA_JUSTIFY,
        spaceAfter=12,
        leading=14
    )
    
    # Style liste
    bullet_style = ParagraphStyle(
        'Bullet',
        parent=styles['BodyText'],
        fontSize=10,
        leftIndent=20,
        bulletIndent=10,
        spaceAfter=6
    )
    
    # Statistiques
    arr = np.array(numbers)
    stats = {
        'n_mesures': len(arr),
        'min': arr.min(),
        'max': arr.max(),
        'mean': arr.mean(),
        'median': np.median(arr),
        'std': arr.std()
    }
    
    # Contenu du PDF
    story = []
    
    # ========== PAGE DE GARDE ==========
    story.append(Spacer(1, 3*cm))
    
    story.append(Paragraph("RAPPORT D'INVESTIGATION", title_style))
    story.append(Paragraph("TOMOGRAPHIE DE R√âSISTIVIT√â √âLECTRIQUE (ERT)", subtitle_style))
    
    story.append(Spacer(1, 2*cm))
    
    # Bo√Æte d'information
    info_data = [
        ['Fichier analys√©:', file_name],
        ['Date du rapport:', datetime.now().strftime('%d/%m/%Y %H:%M')],
        ['Nombre de mesures:', f"{stats['n_mesures']}"],
        ['Plage de r√©sistivit√©:', f"{stats['min']:.4f} - {stats['max']:.2f} Œ©¬∑m"],
        ['Type d\'analyse:', 'Investigation compl√®te avec IA']
    ]
    
    info_table = Table(info_data, colWidths=[7*cm, 9*cm])
    info_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#E6F3FF')),
        ('BACKGROUND', (1, 0), (1, -1), colors.white),
        ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
        ('ALIGN', (0, 0), (0, -1), 'RIGHT'),
        ('ALIGN', (1, 0), (1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
        ('FONTNAME', (1, 0), (1, -1), 'Helvetica'),
        ('FONTSIZE', (0, 0), (-1, -1), 11),
        ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#4682B4')),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('TOPPADDING', (0, 0), (-1, -1), 8),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
    ]))
    
    story.append(info_table)
    story.append(Spacer(1, 2*cm))
    
    # Logo/watermark
    story.append(Paragraph(
        "<font color='#808080' size=10><i>G√©n√©r√© par Kibali Analyst - Syst√®me Expert ERT</i></font>",
        ParagraphStyle('footer', parent=styles['Normal'], alignment=TA_CENTER)
    ))
    
    story.append(PageBreak())
    
    # ========== R√âSUM√â EX√âCUTIF ==========
    story.append(Paragraph("1. R√âSUM√â EX√âCUTIF", section_style))
    
    # D√©terminer interpr√©tation principale
    if stats['mean'] < 1:
        interpretation = "zone fortement conductrice sugg√©rant la pr√©sence de sulfures m√©talliques, graphite ou argiles satur√©es"
        color_indicator = "üî¥"
    elif stats['mean'] < 10:
        interpretation = "zone conductrice typique d'eau sal√©e, argiles humides ou min√©raux hydrat√©s"
        color_indicator = "üü†"
    elif stats['mean'] < 100:
        interpretation = "zone de r√©sistivit√© mod√©r√©e caract√©ristique d'eau douce, sables ou roches alt√©r√©es"
        color_indicator = "üü¢"
    else:
        interpretation = "zone r√©sistive indiquant des roches consolid√©es, granite ou calcaire"
        color_indicator = "üîµ"
    
    executive_summary = f"""
    L'investigation g√©ophysique par tomographie de r√©sistivit√© √©lectrique (ERT) du site <b>{file_name}</b> 
    a permis d'acqu√©rir <b>{stats['n_mesures']} mesures</b> sur le terrain. L'analyse r√©v√®le une {interpretation}.
    <br/><br/>
    {color_indicator} <b>R√©sistivit√© moyenne: {stats['mean']:.2f} Œ©¬∑m</b> (√©cart-type: {stats['std']:.2f})
    <br/><br/>
    Les valeurs varient de <b>{stats['min']:.4f} Œ©¬∑m</b> (minimum) √† <b>{stats['max']:.2f} Œ©¬∑m</b> (maximum), 
    avec une m√©diane de <b>{stats['median']:.2f} Œ©¬∑m</b>. Cette distribution statistique permet d'identifier 
    plusieurs horizons g√©ologiques distincts et de localiser des anomalies significatives pour l'exploration mini√®re.
    """
    
    story.append(Paragraph(executive_summary, justified_style))
    story.append(Spacer(1, 0.5*cm))
    
    # ========== STATISTIQUES CL√âS ==========
    story.append(Paragraph("2. STATISTIQUES DESCRIPTIVES", section_style))
    
    stats_data = [
        ['<b>Param√®tre</b>', '<b>Valeur</b>', '<b>Interpr√©tation</b>'],
        ['Nombre de mesures', f"{stats['n_mesures']}", 'Excellente couverture spatiale'],
        ['Minimum', f"{stats['min']:.6f} Œ©¬∑m", 'Zone ultra-conductrice d√©tect√©e'],
        ['Maximum', f"{stats['max']:.2f} Œ©¬∑m", 'Zone r√©sistive identifi√©e'],
        ['Moyenne', f"{stats['mean']:.2f} Œ©¬∑m", 'Valeur centrale de la distribution'],
        ['M√©diane', f"{stats['median']:.2f} Œ©¬∑m", 'Valeur m√©diane (50e percentile)'],
        ['√âcart-type', f"{stats['std']:.2f} Œ©¬∑m", 'Variabilit√© mod√©r√©e du sous-sol'],
    ]
    
    stats_table = Table(stats_data, colWidths=[5*cm, 4*cm, 7*cm])
    stats_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#006400')),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 12),
        ('FONTSIZE', (0, 1), (-1, -1), 10),
        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
        ('GRID', (0, 0), (-1, -1), 1, colors.black),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('TOPPADDING', (0, 0), (-1, -1), 8),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
    ]))
    
    story.append(stats_table)
    story.append(Spacer(1, 0.5*cm))
    
    story.append(PageBreak())
    
    # ========== GRAPHIQUES ERT (5 COUPES) ==========
    if fig_ert is not None:
        story.append(Paragraph("3. COUPES ERT PROFESSIONNELLES", section_style))
        
        # Explication des graphiques
        ert_explanation = """
        Les cinq graphiques suivants pr√©sentent une analyse compl√®te de la distribution de r√©sistivit√© 
        dans le sous-sol. Chaque repr√©sentation offre une perspective compl√©mentaire pour l'interpr√©tation 
        g√©ologique et la localisation des cibles d'exploration.
        """
        story.append(Paragraph(ert_explanation, justified_style))
        story.append(Spacer(1, 0.3*cm))
        
        # Descriptions des graphiques
        graph_descriptions = [
            ("<b>1Ô∏è‚É£ Pseudosection</b>", "Repr√©sentation de la r√©sistivit√© apparente mesur√©e sur le terrain. "
             "Les points noirs indiquent les positions des √©lectrodes. Cette vue montre les donn√©es brutes avant inversion."),
            
            ("<b>2Ô∏è‚É£ Mod√®le invers√©</b>", "Section apr√®s traitement par inversion g√©ophysique. "
             "Les lignes de contour annot√©es facilitent la lecture quantitative des valeurs de r√©sistivit√©."),
            
            ("<b>3Ô∏è‚É£ Coupe g√©ologique</b>", "Interpr√©tation visuelle avec annotations des anomalies majeures (‚≠ê). "
             "Les zones ultra-conductrices (<1 Œ©¬∑m) sont marqu√©es pour investigation prioritaire."),
            
            ("<b>4Ô∏è‚É£ Distribution statistique</b>", "Histogramme logarithmique montrant la fr√©quence des valeurs. "
             "La palette de 8 couleurs correspond aux standards Res2DInv avec pourcentages de distribution."),
            
            ("<b>5Ô∏è‚É£ Profil vertical 1D</b>", "√âvolution de la r√©sistivit√© avec la profondeur. "
             "L'enveloppe min-max montre la variabilit√© lat√©rale. Les zones g√©ologiques sont color√©es par profondeur.")
        ]
        
        for title, desc in graph_descriptions:
            story.append(Paragraph(f"‚Ä¢ {title}: {desc}", bullet_style))
        
        story.append(Spacer(1, 0.5*cm))
        
        # Sauvegarder figure ERT en haute r√©solution
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_ert:
            fig_ert.savefig(tmp_ert.name, format='png', dpi=200, bbox_inches='tight')
            tmp_ert_path = tmp_ert.name
        
        # Ins√©rer image (mode paysage pour les 5 graphiques)
        story.append(PageBreak())
        ert_img = RLImage(tmp_ert_path, width=18*cm, height=21*cm)
        story.append(ert_img)
        story.append(Spacer(1, 0.3*cm))
        story.append(Paragraph(
            "<i>Figure 1: Ensemble complet des 5 coupes ERT professionnelles (style Res2DInv)</i>",
            ParagraphStyle('caption', parent=styles['Normal'], fontSize=9, alignment=TA_CENTER, textColor=colors.HexColor('#666666'))
        ))
        
        os.unlink(tmp_ert_path)  # Nettoyer fichier temporaire
    
    story.append(PageBreak())
    
    # ========== TABLEAU DE CORRESPONDANCES ==========
    if df_corr is not None and not df_corr.empty:
        story.append(Paragraph("4. CORRESPONDANCES MIN√âRALES", section_style))
        
        corr_explanation = """
        Le tableau suivant √©tablit les correspondances entre les valeurs de r√©sistivit√© mesur√©es et les 
        mat√©riaux g√©ologiques potentiels. Le niveau de confiance (0-100%) refl√®te la position de la mesure 
        dans la plage de r√©sistivit√© caract√©ristique de chaque min√©ral.
        """
        story.append(Paragraph(corr_explanation, justified_style))
        story.append(Spacer(1, 0.5*cm))
        
        if fig_corr is not None:
            # Ins√©rer graphique scatter + table
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_corr:
                fig_corr.savefig(tmp_corr.name, format='png', dpi=200, bbox_inches='tight')
                tmp_corr_path = tmp_corr.name
            
            corr_img = RLImage(tmp_corr_path, width=17*cm, height=13*cm)
            story.append(corr_img)
            story.append(Spacer(1, 0.3*cm))
            story.append(Paragraph(
                "<i>Figure 2: Tableau de correspondances et scatter plot des mesures r√©elles</i>",
                ParagraphStyle('caption', parent=styles['Normal'], fontSize=9, alignment=TA_CENTER, textColor=colors.HexColor('#666666'))
            ))
            
            os.unlink(tmp_corr_path)
        
        # Top 10 correspondances en tableau
        story.append(Spacer(1, 0.5*cm))
        story.append(Paragraph("<b>Top 10 Correspondances Identifi√©es:</b>", ParagraphStyle('bold', parent=styles['Normal'], fontName='Helvetica-Bold')))
        story.append(Spacer(1, 0.2*cm))
        
        top10 = df_corr.nlargest(10, 'Confiance')[['Mat√©riau', 'R√©sistivit√© mesur√©e (Œ©¬∑m)', 'Confiance', 'Profondeur (m)']]
        
        table_data = [['<b>Mat√©riau</b>', '<b>R√©sistivit√© (Œ©¬∑m)</b>', '<b>Confiance</b>', '<b>Profondeur (m)</b>']]
        for _, row in top10.iterrows():
            table_data.append([
                row['Mat√©riau'],
                f"{row['R√©sistivit√© mesur√©e (Œ©¬∑m)']:.4f}",
                f"{row['Confiance']*100:.0f}%",
                f"{row['Profondeur (m)']:.1f}"
            ])
        
        corr_table = Table(table_data, colWidths=[6*cm, 4*cm, 3*cm, 3*cm])
        corr_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#8B0000')),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 10),
            ('FONTSIZE', (0, 1), (-1, -1), 9),
            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#F5F5F5')]),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
            ('TOPPADDING', (0, 0), (-1, -1), 6),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
        ]))
        
        story.append(corr_table)
    
    story.append(PageBreak())
    
    # ========== INTERPR√âTATION G√âOLOGIQUE ==========
    story.append(Paragraph("5. INTERPR√âTATION G√âOLOGIQUE D√âTAILL√âE", section_style))
    
    # Analyse par plages de r√©sistivit√©
    ranges_analysis = [
        (0, 1, "Ultra-conducteur", "Sulfures m√©talliques, graphite, argiles satur√©es en eau sal√©e"),
        (1, 10, "Fortement conducteur", "Eau sal√©e, argiles humides, schistes graphiteux"),
        (10, 100, "Mod√©r√©ment conducteur", "Eau douce, sables satur√©s, roches alt√©r√©es"),
        (100, 1000, "Mod√©r√©ment r√©sistif", "Sables secs, graviers, roches consolid√©es"),
        (1000, float('inf'), "Tr√®s r√©sistif", "Granite, quartz, calcaire compact, roches ign√©es")
    ]
    
    story.append(Paragraph("<b>5.1 Analyse par horizons de r√©sistivit√©:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12, textColor=colors.HexColor('#00008B'))))
    story.append(Spacer(1, 0.3*cm))
    
    for rho_min, rho_max, label, materials in ranges_analysis:
        count = np.sum((arr >= rho_min) & (arr < rho_max))
        percentage = (count / len(arr)) * 100
        
        if count > 0:
            range_text = f"""
            <b>{label} ({rho_min}-{rho_max} Œ©¬∑m)</b>: {count} mesures ({percentage:.1f}%)
            <br/>
            <i>Mat√©riaux probables: {materials}</i>
            """
            story.append(Paragraph(range_text, bullet_style))
            story.append(Spacer(1, 0.2*cm))
    
    # Anomalies d√©tect√©es
    story.append(Spacer(1, 0.3*cm))
    story.append(Paragraph("<b>5.2 Anomalies g√©ophysiques majeures:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12, textColor=colors.HexColor('#00008B'))))
    story.append(Spacer(1, 0.3*cm))
    
    anomalies = []
    
    # Anomalie conductrice
    ultra_cond = arr < 1
    if ultra_cond.any():
        n_ultra = ultra_cond.sum()
        anomalies.append(f"üî¥ <b>{n_ultra} zones ultra-conductrices</b> (œÅ < 1 Œ©¬∑m) - Cibles prioritaires pour exploration mini√®re (sulfures, or associ√©)")
    
    # Anomalie r√©sistive
    ultra_res = arr > 1000
    if ultra_res.any():
        n_ultra_res = ultra_res.sum()
        anomalies.append(f"üîµ <b>{n_ultra_res} zones tr√®s r√©sistives</b> (œÅ > 1000 Œ©¬∑m) - Roches cristallines, granite, quartz massif")
    
    # Zones interm√©diaires
    water_zone = (arr >= 10) & (arr <= 100)
    if water_zone.any():
        n_water = water_zone.sum()
        anomalies.append(f"üü¢ <b>{n_water} zones aquif√®res potentielles</b> (10-100 Œ©¬∑m) - Eau douce, sables satur√©s")
    
    if not anomalies:
        anomalies.append("‚ÑπÔ∏è Aucune anomalie majeure d√©tect√©e - Distribution homog√®ne")
    
    for anomaly in anomalies:
        story.append(Paragraph(f"‚Ä¢ {anomaly}", bullet_style))
        story.append(Spacer(1, 0.2*cm))
    
    story.append(PageBreak())
    
    # ========== RECOMMANDATIONS ==========
    story.append(Paragraph("6. RECOMMANDATIONS ET PERSPECTIVES", section_style))
    
    recommendations = f"""
    Sur la base de l'analyse g√©ophysique ERT, les recommandations suivantes sont propos√©es:
    <br/><br/>
    <b>6.1 Investigations compl√©mentaires:</b>
    <br/>
    ‚Ä¢ Sondages carott√©s aux emplacements des anomalies ultra-conductrices (œÅ < 1 Œ©¬∑m)
    <br/>
    ‚Ä¢ Prospection g√©ochimique (√©chantillonnage sol) sur les zones √† fort potentiel
    <br/>
    ‚Ä¢ Polarisation provoqu√©e (IP) pour confirmer la pr√©sence de sulfures m√©talliques
    <br/>
    ‚Ä¢ Lev√© magn√©tique pour compl√©ter la signature g√©ophysique
    <br/><br/>
    <b>6.2 Ciblage minier:</b>
    <br/>
    ‚Ä¢ Priorit√© 1: Zones œÅ < 1 Œ©¬∑m (potentiel sulfures massifs)
    <br/>
    ‚Ä¢ Priorit√© 2: Transitions brusques de r√©sistivit√© (contacts lithologiques)
    <br/>
    ‚Ä¢ Priorit√© 3: Zones 10-100 Œ©¬∑m si contexte aquif√®re recherch√©
    <br/><br/>
    <b>6.3 Mod√©lisation 3D:</b>
    <br/>
    ‚Ä¢ Extension du profil 2D vers une couverture surfacique (grille 3D)
    <br/>
    ‚Ä¢ Inversion 3D pour mod√®le volum√©trique complet du sous-sol
    <br/>
    ‚Ä¢ Corr√©lation avec donn√©es g√©ologiques de surface et forages existants
    """
    
    story.append(Paragraph(recommendations, justified_style))
    
    story.append(PageBreak())
    
    # ========== ANNEXES TECHNIQUES ==========
    story.append(Paragraph("7. ANNEXES TECHNIQUES", section_style))
    
    story.append(Paragraph("<b>7.1 M√©thodologie ERT:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12)))
    story.append(Spacer(1, 0.2*cm))
    
    methodology = """
    La tomographie de r√©sistivit√© √©lectrique (ERT) est une m√©thode g√©ophysique non-invasive qui mesure 
    la r√©sistivit√© √©lectrique du sous-sol. Des √©lectrodes sont implant√©es selon un profil lin√©aire, et des 
    mesures de r√©sistance sont effectu√©es entre diff√©rentes combinaisons d'√©lectrodes (dispositif Wenner, 
    Schlumberger, dip√¥le-dip√¥le, etc.). Les donn√©es sont ensuite invers√©es pour obtenir un mod√®le 2D de 
    distribution de r√©sistivit√© en profondeur.
    """
    story.append(Paragraph(methodology, justified_style))
    
    story.append(Spacer(1, 0.5*cm))
    story.append(Paragraph("<b>7.2 Param√®tres d'acquisition:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12)))
    story.append(Spacer(1, 0.2*cm))
    
    acq_data = [
        ['Nombre de mesures:', f"{stats['n_mesures']}"],
        ['Plage de mesure:', f"{stats['min']:.6f} - {stats['max']:.2f} Œ©¬∑m"],
        ['Espacement √©lectrodes:', '√Ä d√©terminer selon fichier .dat'],
        ['Dispositif utilis√©:', '√Ä d√©terminer (Wenner/Schlumberger/DD)'],
        ['Profondeur investigation:', f"Estim√©e: {max(50, stats['n_mesures']*0.2):.0f} m"],
    ]
    
    acq_table = Table(acq_data, colWidths=[8*cm, 8*cm])
    acq_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#E6E6FA')),
        ('ALIGN', (0, 0), (0, -1), 'RIGHT'),
        ('ALIGN', (1, 0), (1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, -1), 10),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
        ('TOPPADDING', (0, 0), (-1, -1), 6),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
    ]))
    
    story.append(acq_table)
    
    story.append(Spacer(1, 0.5*cm))
    story.append(Paragraph("<b>7.3 Palette de couleurs standard:</b>", ParagraphStyle('subsection', parent=styles['Heading3'], fontSize=12)))
    story.append(Spacer(1, 0.2*cm))
    
    palette_info = """
    Les graphiques utilisent la palette standard Res2DInv √† 8 couleurs:
    Rouge fonc√© (#8B0000) ‚Üí Rouge ‚Üí Orange ‚Üí Jaune ‚Üí Vert ‚Üí Cyan ‚Üí Bleu ‚Üí Bleu fonc√© (#000080).
    L'√©chelle logarithmique permet de visualiser efficacement la large gamme de r√©sistivit√©s (0.0001 - 10000 Œ©¬∑m).
    """
    story.append(Paragraph(palette_info, justified_style))
    
    # Footer final
    story.append(Spacer(1, 2*cm))
    story.append(Paragraph(
        "‚îÄ" * 80,
        ParagraphStyle('line', parent=styles['Normal'], alignment=TA_CENTER)
    ))
    story.append(Paragraph(
        f"<font color='#808080' size=8>Rapport g√©n√©r√© automatiquement le {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}<br/>"
        "Kibali Analyst - Syst√®me Expert d'Investigation G√©ophysique ERT<br/>"
        "Pour toute question technique: support@kibali-ai.local</font>",
        ParagraphStyle('footer', parent=styles['Normal'], alignment=TA_CENTER, fontSize=8)
    ))
    
    # G√©n√©rer PDF
    doc.build(story)
    
    if output_path:
        with open(output_path, 'rb') as f:
            return f.read()
    else:
        buffer.seek(0)
        return buffer.getvalue()

def analyze_minerals_from_resistivity(numbers: list, file_name: str = "unknown") -> str:
    """
    Analyse compl√®te des min√©raux pr√©sents bas√©e sur les valeurs de r√©sistivit√©
    G√©n√®re un rapport d√©taill√© avec clustering, interpr√©tation g√©ologique et calculs
    """
    if not numbers or len(numbers) < 10:
        return "‚ùå Donn√©es insuffisantes pour analyse min√©rale (minimum 10 mesures requises)"
    
    import numpy as np
    from sklearn.cluster import KMeans
    
    arr = np.array(numbers)
    minerals_db = create_minerals_database()
    
    report = "üî¨ RAPPORT COMPLET D'ANALYSE MIN√âRALE ERT\n"
    report += "=" * 80 + "\n\n"
    
    report += f"üìÅ Fichier analys√©: {file_name}\n"
    report += f"üìä Nombre de mesures: {len(arr)}\n"
    report += f"üìà Plage de r√©sistivit√©: {np.min(arr):.4f} - {np.max(arr):.2f} Œ©¬∑m\n\n"
    
    # Ajouter le tableau de r√©f√©rence de l'eau
    report += get_water_resistivity_color_table() + "\n\n"
    
    # 1Ô∏è‚É£ CLUSTERING AUTOMATIQUE
    report += "1Ô∏è‚É£ CLUSTERING K-MEANS DES R√âSISTIVIT√âS\n"
    report += "‚îÄ" * 80 + "\n"
    
    # D√©terminer nombre optimal de clusters (2-6 bas√© sur variance)
    n_clusters = min(5, max(2, int(np.sqrt(len(arr) / 20))))
    
    try:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(arr.reshape(-1, 1))
        cluster_centers = kmeans.cluster_centers_.flatten()
        
        report += f"‚úÖ {n_clusters} clusters identifi√©s\n\n"
        
        # Trier par r√©sistivit√©
        sorted_indices = np.argsort(cluster_centers)
        
        for i, idx in enumerate(sorted_indices):
            center = cluster_centers[idx]
            count = np.sum(clusters == idx)
            percentage = (count / len(arr)) * 100
            
            report += f"üéØ Cluster {i+1} (œÅ moyenne = {center:.3f} Œ©¬∑m)\n"
            report += f"   ‚Ä¢ Nombre de mesures: {count} ({percentage:.1f}%)\n"
            report += f"   ‚Ä¢ R√©sistivit√©: {arr[clusters == idx].min():.3f} - {arr[clusters == idx].max():.3f} Œ©¬∑m\n"
            
            # Correspondance min√©raux
            matches = minerals_db[
                (minerals_db["Plage Min (Œ©m)"] <= center) & 
                (minerals_db["Plage Max (Œ©m)"] >= center)
            ]
            
            if not matches.empty:
                report += f"   ‚Ä¢ Min√©raux/Mat√©riaux compatibles:\n"
                for _, match in matches.iterrows():
                    report += f"     - {match['Type']} ({match['Cat√©gorie']}): {match['Notes']}\n"
            else:
                report += f"   ‚Ä¢ ‚ö†Ô∏è Aucune correspondance exacte dans la base\n"
            
            # Calculs g√©ophysiques
            conductivity = 1000 / center if center > 0 else float('inf')  # mS/m
            report += f"   ‚Ä¢ Conductivit√© calcul√©e: {conductivity:.2f} mS/m\n"
            report += f"   ‚Ä¢ Profondeur estim√©e: {estimate_depth_from_rho(center)}\n\n"
        
    except Exception as e:
        report += f"‚ùå Erreur clustering: {str(e)}\n\n"
    
    # 2Ô∏è‚É£ ANALYSE PAR CAT√âGORIE
    report += "2Ô∏è‚É£ CLASSIFICATION PAR CAT√âGORIE G√âOPHYSIQUE\n"
    report += "‚îÄ" * 80 + "\n"
    
    # Cat√©gories bas√©es sur r√©sistivit√© avec codes couleur
    ultra_conductors = arr[arr < 0.01]
    conductors = arr[(arr >= 0.01) & (arr < 10)]
    semi_conductors = arr[(arr >= 10) & (arr < 100)]
    resistive = arr[(arr >= 100) & (arr < 1000)]
    highly_resistive = arr[arr >= 1000]
    
    categories = [
        ("Ultra-conducteurs (<0.01 Œ©¬∑m)", ultra_conductors, "M√©taux natifs (or, argent, cuivre), graphite", "üü£ Violet/Noir"),
        ("Conducteurs (0.01-10 Œ©¬∑m)", conductors, "Sulfures (pyrite, galena, chalcopyrite), eau sal√©e, nappes", "üî¥ Rouge/üü† Orange"),
        ("Semi-conducteurs (10-100 Œ©¬∑m)", semi_conductors, "Argile humide, eau douce, certains oxydes", "üü° Jaune/üü¢ Vert"),
        ("R√©sistifs (100-1000 Œ©¬∑m)", resistive, "Gr√®s, calcaire, sphalerite", "üîµ Bleu clair"),
        ("Tr√®s r√©sistifs (>1000 Œ©¬∑m)", highly_resistive, "Granite, quartz, air/vides, eau tr√®s pure", "üîµ Bleu fonc√©")
    ]
    
    for cat_name, cat_data, typical_materials, color_code in categories:
        count = len(cat_data)
        percentage = (count / len(arr)) * 100
        
        if count > 0:
            report += f"üìä {cat_name} - {color_code}\n"
            report += f"   ‚Ä¢ Mesures: {count} ({percentage:.1f}%)\n"
            report += f"   ‚Ä¢ Moyenne: {np.mean(cat_data):.3f} Œ©¬∑m\n"
            report += f"   ‚Ä¢ Mat√©riaux typiques: {typical_materials}\n\n"
    
    # üìä ANALYSE SP√âCIFIQUE DE L'EAU
    report += "üíß ANALYSE D√âTAILL√âE DES TYPES D'EAU\n"
    report += "‚îÄ" * 80 + "\n"
    
    water_categories = [
        {
            "type": "Eau de mer",
            "range": (0.1, 1.0),
            "color": "üî¥ Rouge vif / üü† Orange",
            "description": "Haute conductivit√©, salinit√© >35 g/L",
            "applications": "Zones c√¥ti√®res, intrusions salines"
        },
        {
            "type": "Eau sal√©e (nappe)",
            "range": (1.0, 10.0),
            "color": "üü† Jaune / üü† Orange",
            "description": "Salinit√© mod√©r√©e 1-10 g/L",
            "applications": "Nappes contamin√©es, zones arides"
        },
        {
            "type": "Eau douce",
            "range": (10.0, 100.0),
            "color": "üü¢ Vert / üîµ Bleu clair",
            "description": "Eau potable, faible salinit√© <1 g/L",
            "applications": "Aquif√®res exploitables, rivi√®res"
        },
        {
            "type": "Eau tr√®s pure",
            "range": (100.0, float('inf')),
            "color": "üîµ Bleu fonc√©",
            "description": "Eau d√©min√©ralis√©e, pluie r√©cente",
            "applications": "Zones non satur√©es, pr√©cipitations"
        }
    ]
    
    water_detected = False
    for water_cat in water_categories:
        water_zone = arr[(arr >= water_cat["range"][0]) & (arr < water_cat["range"][1])]
        count = len(water_zone)
        percentage = (count / len(arr)) * 100
        
        if count > 0:
            water_detected = True
            report += f"üíß **{water_cat['type']}** ({water_cat['range'][0]}-{water_cat['range'][1]} Œ©¬∑m) - {water_cat['color']}\n"
            report += f"   ‚Ä¢ Mesures: {count} ({percentage:.1f}%)\n"
            report += f"   ‚Ä¢ Moyenne: {np.mean(water_zone):.3f} Œ©¬∑m\n"
            report += f"   ‚Ä¢ Description: {water_cat['description']}\n"
            report += f"   ‚Ä¢ Applications: {water_cat['applications']}\n\n"
    
    if not water_detected:
        report += "‚ö†Ô∏è Aucune signature d'eau claire d√©tect√©e dans les mesures\n"
        report += "   Possible: Zone tr√®s s√®che, substrat rocheux, ou min√©ralisation dominante\n\n"
    else:
        report += "‚úÖ Signatures hydriques identifi√©es - Possible nappe phr√©atique ou circulation d'eau\n\n"
    
    # 3Ô∏è‚É£ D√âTECTION D'ANOMALIES MIN√âRALES
    report += "3Ô∏è‚É£ D√âTECTION D'ANOMALIES POUR EXPLORATION MINI√àRE\n"
    report += "‚îÄ" * 80 + "\n"
    
    anomalies_detected = []
    
    # Anomalie sulfures (tr√®s conducteurs)
    sulfure_zone = arr[arr < 1]
    if len(sulfure_zone) > 0:
        anomalies_detected.append({
            "type": "Zone sulfur√©e potentielle",
            "count": len(sulfure_zone),
            "rho_range": f"{np.min(sulfure_zone):.4f} - {np.max(sulfure_zone):.3f} Œ©¬∑m",
            "minerals": "Pyrite, Chalcopyrite, Galena, Bornite",
            "interest": "‚≠ê‚≠ê‚≠ê HAUT - Exploration Cu, Pb, Zn, Au associ√©"
        })
    
    # Anomalie m√©taux pr√©cieux
    metal_zone = arr[arr < 0.01]
    if len(metal_zone) > 0:
        anomalies_detected.append({
            "type": "Zone m√©taux natifs potentielle",
            "count": len(metal_zone),
            "rho_range": f"{np.min(metal_zone):.6f} - {np.max(metal_zone):.4f} Œ©¬∑m",
            "minerals": "Or natif, Argent, Cuivre, Graphite",
            "interest": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê TR√àS HAUT - Exploration m√©taux pr√©cieux"
        })
    
    # Anomalie oxydes de fer
    iron_zone = arr[(arr >= 10) & (arr <= 1000)]
    if len(iron_zone) > len(arr) * 0.1:  # >10% des mesures
        anomalies_detected.append({
            "type": "Zone oxydes de fer",
            "count": len(iron_zone),
            "rho_range": f"{np.min(iron_zone):.2f} - {np.max(iron_zone):.2f} Œ©¬∑m",
            "minerals": "Magn√©tite, H√©matite",
            "interest": "‚≠ê‚≠ê MOYEN - Exploration fer, indicateur alt√©ration"
        })
    
    if anomalies_detected:
        for i, anomaly in enumerate(anomalies_detected, 1):
            report += f"üéØ Anomalie {i}: {anomaly['type']}\n"
            report += f"   ‚Ä¢ Mesures affect√©es: {anomaly['count']} ({anomaly['count']/len(arr)*100:.1f}%)\n"
            report += f"   ‚Ä¢ Plage de r√©sistivit√©: {anomaly['rho_range']}\n"
            report += f"   ‚Ä¢ Min√©raux probables: {anomaly['minerals']}\n"
            report += f"   ‚Ä¢ Int√©r√™t √©conomique: {anomaly['interest']}\n\n"
    else:
        report += "‚ö†Ô∏è Aucune anomalie min√©rale majeure d√©tect√©e\n\n"
    
    # 4Ô∏è‚É£ RECOMMANDATIONS D'EXPLORATION
    report += "4Ô∏è‚É£ RECOMMANDATIONS POUR EXPLORATION\n"
    report += "‚îÄ" * 80 + "\n"
    
    if len(sulfure_zone) > 0:
        report += "‚úÖ PRIORIT√â 1: Forage cibl√© sur zones sulfur√©es (<1 Œ©¬∑m)\n"
        report += "   ‚Ä¢ Profondeur recommand√©e: 50-200m\n"
        report += "   ‚Ä¢ Analyses g√©ochimiques: Cu, Pb, Zn, Au, Ag\n"
        report += "   ‚Ä¢ M√©thodes compl√©mentaires: IP (Polarisation Induite), Magn√©tom√©trie\n\n"
    
    if len(metal_zone) > 0:
        report += "‚úÖ PRIORIT√â 2: Investigation m√©taux pr√©cieux (<0.01 Œ©¬∑m)\n"
        report += "   ‚Ä¢ Technique: √âchantillonnage par tranch√©es\n"
        report += "   ‚Ä¢ Analyses: Fire assay pour Au, ICP-MS pour √©l√©ments traces\n\n"
    
    # Recommandations hydrog√©ologiques
    water_conductors = arr[(arr >= 0.1) & (arr <= 100)]
    if len(water_conductors) > 0:
        report += "üíß HYDROG√âOLOGIE: Investigation ressources en eau\n"
        report += "   ‚Ä¢ Zones identifi√©es avec signature hydrique\n"
        
        sea_water = arr[(arr >= 0.1) & (arr <= 1.0)]
        brackish_water = arr[(arr >= 1.0) & (arr <= 10.0)]
        fresh_water = arr[(arr >= 10.0) & (arr <= 100.0)]
        
        if len(sea_water) > 0:
            report += f"   ‚Ä¢ ‚ö†Ô∏è Eau sal√©e d√©tect√©e ({len(sea_water)} mesures): Risque intrusion marine\n"
        if len(brackish_water) > 0:
            report += f"   ‚Ä¢ üü° Eau saum√¢tre ({len(brackish_water)} mesures): Qualit√© mod√©r√©e\n"
        if len(fresh_water) > 0:
            report += f"   ‚Ä¢ ‚úÖ Eau douce ({len(fresh_water)} mesures): Aquif√®re potentiellement exploitable\n"
        
        report += "   ‚Ä¢ Recommandations:\n"
        report += "     - Forages de reconnaissance (30-150m)\n"
        report += "     - Analyses hydrochimiques (pH, TDS, ions majeurs)\n"
        report += "     - Essais de pompage pour transmissivit√©\n"
        report += "     - Monitoring pi√©zom√©trique temporel\n\n"
    
    report += "üìã M√©thodes ERT compl√©mentaires recommand√©es:\n"
    report += "   ‚Ä¢ Inversion 3D pour cartographie volum√©trique\n"
    report += "   ‚Ä¢ Mesures IP pour discrimination sulfures/oxydes\n"
    report += "   ‚Ä¢ Profils serr√©s (espacement <2m) sur anomalies\n"
    report += "   ‚Ä¢ Time-lapse ERT pour suivi temporel\n"
    report += "   ‚Ä¢ TDEM (Time Domain EM) pour profondeurs >200m\n\n"
    
    # 5Ô∏è‚É£ STATISTIQUES GLOBALES
    report += "5Ô∏è‚É£ STATISTIQUES GLOBALES DU FICHIER\n"
    report += "‚îÄ" * 80 + "\n"
    
    report += f"üìä R√©sistivit√© moyenne: {np.mean(arr):.3f} Œ©¬∑m\n"
    report += f"üìä M√©diane: {np.median(arr):.3f} Œ©¬∑m\n"
    report += f"üìä √âcart-type: {np.std(arr):.3f} Œ©¬∑m\n"
    report += f"üìä Coefficient de variation: {np.std(arr)/np.mean(arr):.3f}\n"
    report += f"üìä Range log: {np.log10(np.max(arr)/np.min(arr)):.2f} d√©cades\n\n"
    
    # Distribution g√©ologique probable
    mean_rho = np.median(arr)  # M√©diane plus robuste que moyenne
    if mean_rho < 10:
        geo_context = "Environnement conducteur: zone satur√©e, sulfures, alt√©ration hydrothermale"
    elif mean_rho < 100:
        geo_context = "Environnement mixte: sols, roches alt√©r√©es, transition vadose-phr√©atique"
    elif mean_rho < 1000:
        geo_context = "Environnement r√©sistif: roches compactes, zone non satur√©e"
    else:
        geo_context = "Environnement tr√®s r√©sistif: substratum cristallin, zones s√®ches"
    
    report += f"üåç Contexte g√©ologique probable: {geo_context}\n"
    
    report += "\n" + "=" * 80 + "\n"
    report += "‚úÖ ANALYSE MIN√âRALE COMPL√àTE TERMIN√âE\n"
    
    return report

def estimate_depth_from_rho(rho: float) -> str:
    """Estime la profondeur typique bas√©e sur la r√©sistivit√©"""
    if rho < 1:
        return "0-20m (zone conductrice superficielle ou min√©ralisation)"
    elif rho < 100:
        return "0-50m (zone vadose ou alt√©r√©e)"
    elif rho < 1000:
        return "20-100m (zone de transition ou roche fractur√©e)"
    else:
        return ">50m (substratum profond ou zone s√®che)"

def get_water_resistivity_color_table() -> str:
    """
    Retourne un tableau de r√©f√©rence des r√©sistivit√©s de l'eau avec codes couleur
    Bas√© sur les standards g√©ophysiques internationaux
    """
    table = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         TABLEAU DE R√âF√âRENCE - R√âSISTIVIT√â DE L'EAU (Œ©¬∑m)                   ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Type d'eau          ‚îÇ R√©sistivit√© (Œ©¬∑m)  ‚îÇ Couleur associ√©e                 ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë **Eau de mer**      ‚îÇ 0.1 - 1 Œ©¬∑m        ‚îÇ üî¥ Rouge vif / üü† Orange         ‚ïë
‚ïë                     ‚îÇ                    ‚îÇ (Haute conductivit√©)             ‚ïë
‚ï†‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï£
‚ïë **Eau sal√©e (nappe)**‚îÇ 1 - 10 Œ©¬∑m        ‚îÇ üü† Jaune / üü† Orange             ‚ïë
‚ïë                     ‚îÇ                    ‚îÇ (Salinit√© mod√©r√©e)               ‚ïë
‚ï†‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï£
‚ïë **Eau douce**       ‚îÇ 10 - 100 Œ©¬∑m       ‚îÇ üü¢ Vert / üîµ Bleu clair          ‚ïë
‚ïë                     ‚îÇ                    ‚îÇ (Potable, exploitable)           ‚ïë
‚ï†‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï£
‚ïë **Eau tr√®s pure**   ‚îÇ > 100 Œ©¬∑m          ‚îÇ üîµ Bleu fonc√©                    ‚ïë
‚ïë                     ‚îÇ                    ‚îÇ (D√©min√©ralis√©e)                  ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Notes:
‚Ä¢ Les couleurs sont indicatives et d√©pendent de la palette utilis√©e (Res2DInv, etc.)
‚Ä¢ La r√©sistivit√© de l'eau varie avec: temp√©rature, salinit√©, pH, min√©raux dissous
‚Ä¢ Eau de mer: ~0.2 Œ©¬∑m (35 g/L sel) vs Eau pure: >1000 Œ©¬∑m (<1 mg/L TDS)
‚Ä¢ Zone de transition douce/sal√©e: 10-30 Œ©¬∑m (m√©lange, interface)
"""
    return table

# ========================================
# EXTRACTION PDF & OCR POUR RAPPORTS ERT
# ========================================

def generate_annotations_with_ocr(image_path: str, label_output_path: str, preview: bool = False) -> bool:
    """
    G√©n√®re des annotations YOLO √† partir d'OCR sur une image
    D√©tecte texte, valeurs de r√©sistivit√©, l√©gendes min√©rales
    """
    image = cv2.imread(image_path)
    if image is None:
        return False

    h, w, _ = image.shape
    try:
        data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
    except Exception as e:
        st.warning(f"Erreur OCR: {e}")
        return False

    found = False
    resistivity_values = []
    
    for i in range(len(data['text'])):
        text = data['text'][i].strip()
        if not text:
            continue
        
        # D√©tecter valeurs de r√©sistivit√© (patterns: "123.45", "0.001", etc.)
        try:
            value = float(text.replace(',', '.'))
            if 0.000001 <= value <= 1e15:  # Plage r√©sistivit√© valide
                resistivity_values.append(value)
        except:
            pass
        
        x, y, bw, bh = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
        x_center = (x + bw / 2) / w
        y_center = (y + bh / 2) / h
        bw_norm, bh_norm = bw / w, bh / h
        
        with open(label_output_path, "a") as label_file:
            label_file.write(f"0 {x_center:.6f} {y_center:.6f} {bw_norm:.6f} {bh_norm:.6f}\n")
        found = True
        
        if preview:
            cv2.rectangle(image, (x, y), (x + bw, y + bh), (0, 255, 0), 2)

    if found and preview:
        st.image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), caption=f"OCR: {os.path.basename(image_path)}")
    
    return found, resistivity_values

def extract_captions_near_images(pdf, page, page_num: int, pdf_name: str, output_dir: str) -> list:
    """
    Extrait les l√©gendes textuelles proches des images dans un PDF
    Utile pour r√©cup√©rer descriptions de profils ERT, annotations g√©ologiques
    """
    try:
        blocks = page.get_text("blocks")
        images = page.get_images(full=True)
        captions = []

        for img in images:
            xref = img[0]
            bbox = page.get_image_bbox(xref)
            
            # Chercher texte proche de l'image (dans un rayon de 80 pixels)
            for b in blocks:
                bx0, by0, bx1, by1, text, *_ = b
                # Texte en dessous ou au-dessus de l'image
                if abs(by0 - bbox.y1) < 80 or abs(bbox.y0 - by1) < 80:
                    if len(text.strip()) > 5:
                        captions.append({
                            "text": text.strip(),
                            "position": (bx0, by0, bx1, by1),
                            "image_bbox": (bbox.x0, bbox.y0, bbox.x1, bbox.y1)
                        })

        if captions:
            caption_file = os.path.join(output_dir, f"{pdf_name}_page{page_num+1}_captions.txt")
            with open(caption_file, "w", encoding="utf-8") as f:
                for cap in captions:
                    f.write(cap["text"] + "\n")
            
        return captions
    except Exception as e:
        st.warning(f"Erreur extraction l√©gendes page {page_num+1}: {e}")
        return []

def extract_image_map(pdf, page, page_num: int, pdf_name: str, output_dir: str) -> dict:
    """
    G√©n√®re une carte JSON des images pr√©sentes dans la page
    Utile pour indexer positions de profils ERT, cartes de localisation
    """
    try:
        images = page.get_images(full=True)
        map_data = []
        
        for img in images:
            xref = img[0]
            bbox = page.get_image_bbox(xref)
            map_data.append({
                "xref": xref,
                "bbox": [bbox.x0, bbox.y0, bbox.x1, bbox.y1],
                "page": page_num + 1,
                "width": bbox.x1 - bbox.x0,
                "height": bbox.y1 - bbox.y0
            })
        
        if map_data:
            map_file = os.path.join(output_dir, f"{pdf_name}_page{page_num+1}_map.json")
            with open(map_file, "w") as f:
                json.dump(map_data, f, indent=2)
        
        return map_data
    except Exception as e:
        st.warning(f"Erreur g√©n√©ration carte d'images: {e}")
        return []

def extract_drawings(pdf, page, page_num: int, pdf_name: str, output_dir: str) -> bool:
    """
    Extrait les √©l√©ments vectoriels (graphiques, courbes, croquis)
    Utile pour extraire profils ERT vectoriels, graphiques de r√©sistivit√©
    """
    try:
        drawings = page.get_drawings()
        if drawings:
            # Convertir page en image pour sauvegarder les dessins
            pix = page.get_pixmap()
            drawing_file = os.path.join(output_dir, f"{pdf_name}_page{page_num+1}_drawings.png")
            pix.save(drawing_file)
            return True
        return False
    except Exception as e:
        st.warning(f"Erreur extraction dessins: {e}")
        return False

def extract_ert_report_from_pdf(pdf_path: str, output_base_dir: str = None) -> dict:
    """
    üî¨ EXTRACTION COMPL√àTE DE RAPPORT ERT DEPUIS PDF
    
    Extrait automatiquement:
    - Profils de r√©sistivit√© (images)
    - L√©gendes min√©rales/g√©ologiques  
    - Cartes de localisation
    - Tableaux de mesures
    - Valeurs de r√©sistivit√© par OCR
    - Graphiques vectoriels
    
    Returns:
        dict avec chemins des fichiers extraits et m√©tadonn√©es
    """
    if output_base_dir is None:
        output_base_dir = "/tmp/ert_extracted"
    
    os.makedirs(output_base_dir, exist_ok=True)
    
    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
    
    # Cr√©er dossiers de sortie
    images_dir = os.path.join(output_base_dir, "images")
    text_dir = os.path.join(output_base_dir, "text")
    data_dir = os.path.join(output_base_dir, "data")
    
    for d in [images_dir, text_dir, data_dir]:
        os.makedirs(d, exist_ok=True)
    
    extraction_results = {
        "pdf_name": pdf_name,
        "images": [],
        "captions": [],
        "maps": [],
        "drawings": [],
        "resistivity_values": [],
        "full_text": ""
    }
    
    try:
        pdf = fitz.open(pdf_path)
        st.info(f"üìÑ Extraction PDF: {pdf_name} ({len(pdf)} pages)")
        
        full_text = ""
        progress_bar = st.progress(0)
        
        for page_num in range(len(pdf)):
            page = pdf[page_num]
            
            # 1Ô∏è‚É£ Extraire texte complet
            page_text = page.get_text()
            full_text += f"\n=== Page {page_num+1} ===\n{page_text}"
            
            # 2Ô∏è‚É£ Extraire l√©gendes proches des images
            captions = extract_captions_near_images(pdf, page, page_num, pdf_name, text_dir)
            extraction_results["captions"].extend(captions)
            
            # 3Ô∏è‚É£ G√©n√©rer carte des images
            image_map = extract_image_map(pdf, page, page_num, pdf_name, data_dir)
            extraction_results["maps"].append(image_map)
            
            # 4Ô∏è‚É£ Extraire dessins vectoriels
            has_drawings = extract_drawings(pdf, page, page_num, pdf_name, images_dir)
            if has_drawings:
                extraction_results["drawings"].append(f"page_{page_num+1}")
            
            # 5Ô∏è‚É£ Extraire images et appliquer OCR
            images = page.get_images(full=True)
            for img_index, img in enumerate(images):
                xref = img[0]
                base_image = pdf.extract_image(xref)
                image_bytes = base_image["image"]
                
                image_filename = f"{pdf_name}_page{page_num+1}_img{img_index}.png"
                image_path = os.path.join(images_dir, image_filename)
                
                with open(image_path, "wb") as img_file:
                    img_file.write(image_bytes)
                
                extraction_results["images"].append(image_path)
                
                # OCR pour extraire valeurs de r√©sistivit√©
                label_path = os.path.join(data_dir, f"{pdf_name}_page{page_num+1}_img{img_index}.txt")
                found, resistivity_vals = generate_annotations_with_ocr(image_path, label_path, preview=False)
                
                if resistivity_vals:
                    extraction_results["resistivity_values"].extend(resistivity_vals)
            
            progress_bar.progress((page_num + 1) / len(pdf))
        
        # Sauvegarder texte complet
        text_file = os.path.join(text_dir, f"{pdf_name}_full_text.txt")
        with open(text_file, "w", encoding="utf-8") as f:
            f.write(full_text)
        
        extraction_results["full_text"] = full_text
        
        # Sauvegarder m√©tadonn√©es JSON
        metadata_file = os.path.join(output_base_dir, f"{pdf_name}_metadata.json")
        with open(metadata_file, "w") as f:
            json.dump({
                "pdf_name": pdf_name,
                "total_pages": len(pdf),
                "total_images": len(extraction_results["images"]),
                "total_captions": len(extraction_results["captions"]),
                "resistivity_values_found": len(extraction_results["resistivity_values"]),
                "output_dir": output_base_dir
            }, f, indent=2)
        
        pdf.close()
        
        st.success(f"‚úÖ Extraction termin√©e: {len(extraction_results['images'])} images, {len(extraction_results['resistivity_values'])} valeurs de r√©sistivit√©")
        
        return extraction_results
        
    except Exception as e:
        st.error(f"‚ùå Erreur extraction PDF: {e}")
        return extraction_results

def process_audio_transcription(audio_path: str, output_text_path: str = None) -> str:
    """
    üé§ Transcription audio avec Whisper
    Utile pour notes vocales de g√©ologues sur le terrain
    """
    try:
        st.info(f"üé§ Transcription audio en cours...")
        model = whisper.load_model("base")
        result = model.transcribe(audio_path)
        
        transcription = result["text"]
        
        if output_text_path:
            with open(output_text_path, "w", encoding="utf-8") as f:
                f.write(transcription)
        
        st.success(f"‚úÖ Audio transcrit: {len(transcription)} caract√®res")
        return transcription
        
    except Exception as e:
        st.error(f"‚ùå Erreur transcription audio: {e}")
        return ""

def deep_binary_investigation(file_bytes: bytes, file_name: str = "unknown") -> dict:
    """
    üîç FOUILLE INTELLIGENTE DE FICHIER BINAIRE
    Combine Hex+ASCII Dump + Base Vectorielle RAG + Base ERT pour interpr√©tation scientifique compl√®te
    Similaire √† l'agent VSCode avec todo list mais pour l'analyse binaire
    
    Returns: dict with keys 'full_report' (str) and 'phases' (dict of phase_name: phase_content)
    """
    investigation_report = "üî¨ RAPPORT D'INVESTIGATION BINAIRE APPROFONDIE\n"
    investigation_report += "=" * 80 + "\n\n"
    
    # 1Ô∏è‚É£ EXTRACTION INITIALE (Hex + ASCII)
    investigation_report += "1Ô∏è‚É£ PHASE 1: EXTRACTION HEX + ASCII\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    hex_dump = hex_ascii_view(file_bytes, bytes_per_line=16, max_lines=100)
    investigation_report += f"üìú Dump hexad√©cimal ({len(file_bytes)} bytes):\n"
    investigation_report += f"{hex_dump[:500]}...\n\n"
    
    # Extraction des nombres
    numbers = extract_numbers(file_bytes)
    investigation_report += f"üî¢ Nombres extraits: {len(numbers)} valeurs\n"
    if numbers:
        import numpy as np
        arr = np.array(numbers)
        investigation_report += f"   ‚Ä¢ Range: {np.min(arr):.3f} - {np.max(arr):.3f}\n"
        investigation_report += f"   ‚Ä¢ Moyenne: {np.mean(arr):.3f} ¬± {np.std(arr):.3f}\n"
        investigation_report += f"   ‚Ä¢ M√©diane: {np.median(arr):.3f}\n\n"
    
    # 2Ô∏è‚É£ ANALYSES TECHNIQUES (entropie, patterns, m√©tadonn√©es)
    investigation_report += "2Ô∏è‚É£ PHASE 2: ANALYSES TECHNIQUES\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    entropy_result = entropy_analysis(file_bytes)
    pattern_result = pattern_recognition(file_bytes)
    metadata_result = metadata_extraction(file_bytes)
    compression_result = compression_ratio(file_bytes)
    frequency_result = frequency_analysis(file_bytes)
    
    investigation_report += f"üìä Entropie: {entropy_result}\n"
    investigation_report += f"üéØ Patterns: {pattern_result}\n"
    investigation_report += f"üìã M√©tadonn√©es: {metadata_result}\n"
    investigation_report += f"üóúÔ∏è Compression: {compression_result}\n"
    investigation_report += f"üìà Fr√©quences: {frequency_result}\n\n"
    
    # 3Ô∏è‚É£ FOUILLE DANS LA BASE VECTORIELLE RAG
    investigation_report += "3Ô∏è‚É£ PHASE 3: FOUILLE BASE VECTORIELLE RAG\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    rag_queries = []
    # Construire des requ√™tes intelligentes bas√©es sur les patterns d√©tect√©s
    if "ELF" in pattern_result or "executable" in pattern_result.lower():
        rag_queries.append("analyse fichier ex√©cutable binaire ELF format Linux s√©curit√©")
    if "JPEG" in pattern_result or "PNG" in pattern_result:
        rag_queries.append("format image JPEG PNG m√©tadonn√©es EXIF analyse forensique")
    if "PDF" in pattern_result:
        rag_queries.append("structure PDF analyse document m√©tadonn√©es forensique")
    if numbers and len(numbers) > 10:
        import numpy as np
        arr = np.array(numbers)
        if 0.1 <= np.min(arr) <= 10000:
            rag_queries.append("ERT electrical resistivity tomography geophysics data interpretation")
            rag_queries.append("r√©sistivit√© √©lectrique tomographie g√©ophysique inversion subsurface")
    
    # Requ√™te g√©n√©rique bas√©e sur l'entropie
    if "haute" in entropy_result.lower() or "high" in entropy_result.lower():
        rag_queries.append("fichier chiffr√© crypt√© haute entropie analyse cryptographique")
    else:
        rag_queries.append("fichier donn√©es structur√©es format binaire analyse")
    
    # Fouiller dans RAG pour chaque requ√™te
    rag_findings = ""
    
    # V√©rifier si la base vectorielle existe et est initialis√©e
    has_vectorstore = False
    try:
        has_vectorstore = hasattr(st.session_state, 'vectorstore') and st.session_state.vectorstore is not None
    except:
        has_vectorstore = False
    
    if has_vectorstore:
        investigation_report += f"‚úÖ Base vectorielle d√©tect√©e - {len(rag_queries)} requ√™tes planifi√©es\n\n"
        for i, query in enumerate(rag_queries[:3], 1):  # Limiter √† 3 requ√™tes pour performance
            try:
                result = search_vectorstore(query)
                if result and len(result) > 50:  # √âviter r√©sultats vides
                    rag_findings += f"üîç Requ√™te {i}/3: '{query[:60]}...'\n"
                    rag_findings += f"   üìÑ R√©sultat: {result[:300]}...\n\n"
                else:
                    rag_findings += f"üîç Requ√™te {i}/3: '{query[:60]}...'\n"
                    rag_findings += f"   ‚ö†Ô∏è Aucun r√©sultat pertinent\n\n"
            except Exception as e:
                rag_findings += f"üîç Requ√™te {i}/3: '{query[:60]}...'\n"
                rag_findings += f"   ‚ùå Erreur: {str(e)}\n\n"
        
        if rag_findings:
            investigation_report += rag_findings
        else:
            investigation_report += "‚ö†Ô∏è Aucun r√©sultat trouv√© dans la base RAG\n\n"
    else:
        investigation_report += "‚ö†Ô∏è Base vectorielle RAG non disponible\n"
        investigation_report += "üí° Conseil: Uploadez et indexez des PDFs dans la sidebar pour enrichir l'analyse\n\n"
    
    
    # 4Ô∏è‚É£ FOUILLE SP√âCIALIS√âE ERT (si donn√©es num√©riques d√©tect√©es)
    investigation_report += "4Ô∏è‚É£ PHASE 4: FOUILLE SP√âCIALIS√âE ERT\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    mineral_report = ""
    if numbers and len(numbers) > 10:
        import numpy as np
        arr = np.array(numbers)
        ert_detection = ert_data_detection(file_bytes, numbers)
        investigation_report += ert_detection + "\n"
        
        # üÜï ANALYSE MIN√âRALE COMPL√àTE
        investigation_report += "\nüî¨ ANALYSE MIN√âRALE APPROFONDIE\n"
        investigation_report += "‚îÄ" * 80 + "\n"
        
        try:
            mineral_report = analyze_minerals_from_resistivity(numbers, file_name)
            investigation_report += mineral_report + "\n"
        except Exception as e:
            investigation_report += f"‚ùå Erreur lors de l'analyse min√©rale: {str(e)}\n\n"
        
        # üÜï TABLEAU DE CORRESPONDANCES R√âELLES
        investigation_report += "\nüìä TABLEAU DE CORRESPONDANCES R√âELLES\n"
        investigation_report += "‚îÄ" * 80 + "\n"
        
        try:
            # Option mode grand format pour le tableau
            st.markdown("### üìä Tableau de Correspondances Min√©rales")
            col_tbl1, col_tbl2 = st.columns([1, 1])
            with col_tbl1:
                use_fullsize_table = st.checkbox("üìà Mode GRAND FORMAT Tableau", value=False, 
                                                help="Agrandit le tableau et le scatter plot pour meilleure lisibilit√©")
            
            fig_corr, df_corr, rapport_corr = create_real_mineral_correspondence_table(
                numbers, 
                file_name,
                full_size=use_fullsize_table
            )
            
            if fig_corr and df_corr is not None:
                # Affichage responsive du graphique
                st.pyplot(fig_corr, use_container_width=True)
                
                # Boutons t√©l√©chargement pour le graphique tableau
                col_dl1, col_dl2, col_dl3 = st.columns(3)
                with col_dl1:
                    import io
                    buf_table_png = io.BytesIO()
                    fig_corr.savefig(buf_table_png, format='png', dpi=300, bbox_inches='tight')
                    buf_table_png.seek(0)
                    st.download_button(
                        label="üì• Tableau PNG 300 DPI",
                        data=buf_table_png,
                        file_name=f"{file_name}_correspondances_300dpi.png",
                        mime="image/png",
                        key="dl_table_png"
                    )
                
                with col_dl2:
                    buf_table_pdf = io.BytesIO()
                    fig_corr.savefig(buf_table_pdf, format='pdf', bbox_inches='tight')
                    buf_table_pdf.seek(0)
                    st.download_button(
                        label="üìÑ Tableau PDF",
                        data=buf_table_pdf,
                        file_name=f"{file_name}_correspondances.pdf",
                        mime="application/pdf",
                        key="dl_table_pdf"
                    )
                
                with col_dl3:
                    # CSV du dataframe
                    csv_data = df_corr.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="üì• CSV Donn√©es",
                        data=csv_data,
                        file_name=f"{file_name}_correspondances.csv",
                        mime="text/csv",
                        key="dl_csv"
                    )
                
                plt.close(fig_corr)
                
                # Afficher les donn√©es en plusieurs tableaux pour √©viter scroll excessif
                st.markdown("#### üìã Donn√©es Tabulaires - Organis√©es par Profondeur")
                
                # Corriger les pourcentages de confiance (convertir de 0-1 √† 0-100%)
                df_corr_display = df_corr.copy()
                if 'Confiance' in df_corr_display.columns:
                    # Si les valeurs sont entre 0 et 1, convertir en pourcentage
                    if df_corr_display['Confiance'].max() <= 1:
                        df_corr_display['Confiance (%)'] = (df_corr_display['Confiance'] * 100).round(1)
                    else:
                        df_corr_display['Confiance (%)'] = df_corr_display['Confiance'].round(1)
                    df_corr_display = df_corr_display.drop('Confiance', axis=1)
                
                # Organiser en 5 tableaux selon la profondeur
                total_rows = len(df_corr_display)
                if total_rows > 20:
                    # Diviser en 5 groupes de profondeur
                    depth_col = 'Profondeur (m)' if 'Profondeur (m)' in df_corr_display.columns else df_corr_display.columns[0]
                    df_sorted = df_corr_display.sort_values(depth_col)
                    
                    # Cr√©er 5 quantiles de profondeur
                    quantiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
                    depth_ranges = df_sorted[depth_col].quantile(quantiles).values
                    
                    for i in range(5):
                        min_depth = depth_ranges[i]
                        max_depth = depth_ranges[i+1]
                        
                        # Filtrer les donn√©es dans cette plage de profondeur
                        if i == 4:  # Dernier groupe, inclure la valeur max
                            mask = (df_sorted[depth_col] >= min_depth) & (df_sorted[depth_col] <= max_depth)
                        else:
                            mask = (df_sorted[depth_col] >= min_depth) & (df_sorted[depth_col] < max_depth)
                        
                        df_section = df_sorted[mask]
                        
                        if len(df_section) > 0:
                            with st.expander(f"üìä Tableau {i+1}/5 - Profondeur: {min_depth:.1f} √† {max_depth:.1f} m ({len(df_section)} d√©tections)", expanded=(i==0)):
                                st.dataframe(
                                    df_section,
                                    use_container_width=True,
                                    column_config={
                                        "Confiance (%)": st.column_config.NumberColumn(
                                            "Confiance (%)",
                                            format="%.1f%%",
                                            help="Niveau de confiance de la correspondance (0-100%)"
                                        ),
                                        "R√©sistivit√© mesur√©e (Œ©¬∑m)": st.column_config.NumberColumn(
                                            "R√©sistivit√© mesur√©e (Œ©¬∑m)",
                                            format="%.6f"
                                        ),
                                        "Profondeur (m)": st.column_config.NumberColumn(
                                            "Profondeur (m)",
                                            format="%.1f"
                                        )
                                    },
                                    height=min(400, len(df_section) * 35 + 38)  # Hauteur adaptative
                                )
                                
                                # Statistiques du tableau
                                st.caption(f"üìà Stats: R√©sistivit√© moy. {df_section['R√©sistivit√© mesur√©e (Œ©¬∑m)'].mean():.4f} Œ©¬∑m | "
                                          f"Confiance moy. {df_section['Confiance (%)'].mean():.1f}%")
                else:
                    # Si moins de 20 lignes, afficher en un seul tableau
                    st.dataframe(
                        df_corr_display,
                        use_container_width=True,
                        column_config={
                            "Confiance (%)": st.column_config.NumberColumn(
                                "Confiance (%)",
                                format="%.1f%%",
                                help="Niveau de confiance de la correspondance (0-100%)"
                            ),
                            "R√©sistivit√© mesur√©e (Œ©¬∑m)": st.column_config.NumberColumn(
                                "R√©sistivit√© mesur√©e (Œ©¬∑m)",
                                format="%.6f"
                            ),
                            "Profondeur (m)": st.column_config.NumberColumn(
                                "Profondeur (m)",
                                format="%.1f"
                            )
                        }
                    )
                
                # Ajouter rapport textuel
                investigation_report += rapport_corr + "\n"
            else:
                investigation_report += rapport_corr + "\n"
                
        except Exception as e:
            investigation_report += f"‚ùå Erreur cr√©ation tableau correspondances: {str(e)}\n\n"
        
        # üÜï G√âN√âRATION COUPES ERT PROFESSIONNELLES (5 GRAPHIQUES)
        investigation_report += "\nüé® COUPES ERT PROFESSIONNELLES (Style Res2DInv)\n"
        investigation_report += "‚îÄ" * 80 + "\n"
        
        try:
            # Option mode grand format
            col_btn1, col_btn2 = st.columns([1, 1])
            with col_btn1:
                use_fullsize = st.checkbox("üñºÔ∏è Mode GRAND FORMAT (30√ó36 pouces)", value=False, 
                                          help="Activez pour g√©n√©rer des graphiques haute r√©solution pour impression A0/A1")
            
            fig_ert, grid_data, rapport_ert = create_ert_professional_sections(
                numbers,
                file_name,
                full_size=use_fullsize
            )
            
            if fig_ert is not None:
                st.markdown("### üé® Visualisations ERT Compl√®tes")
                
                # Affichage responsive avec use_container_width
                st.pyplot(fig_ert, use_container_width=True)
                
                # Boutons de t√©l√©chargement en colonnes
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    # T√©l√©charger en PNG haute r√©solution
                    import io
                    buf_png = io.BytesIO()
                    fig_ert.savefig(buf_png, format='png', dpi=300, bbox_inches='tight')
                    buf_png.seek(0)
                    st.download_button(
                        label="üì• PNG Haute R√©solution (300 DPI)",
                        data=buf_png,
                        file_name=f"{file_name}_ert_graphics_300dpi.png",
                        mime="image/png",
                        help="Format PNG 300 DPI pour impression professionnelle"
                    )
                
                with col2:
                    # T√©l√©charger en PDF vectoriel
                    buf_pdf = io.BytesIO()
                    fig_ert.savefig(buf_pdf, format='pdf', bbox_inches='tight')
                    buf_pdf.seek(0)
                    st.download_button(
                        label="üìÑ PDF Vectoriel",
                        data=buf_pdf,
                        file_name=f"{file_name}_ert_graphics.pdf",
                        mime="application/pdf",
                        help="Format PDF vectoriel pour documents techniques"
                    )
                
                with col3:
                    # T√©l√©charger grille de donn√©es
                    if grid_data:
                        import pickle
                        grid_pickle = pickle.dumps(grid_data)
                        st.download_button(
                            label="ÔøΩ Donn√©es Grille (PKL)",
                            data=grid_pickle,
                            file_name=f"{file_name}_grid_ert.pkl",
                            mime="application/octet-stream",
                            help="Donn√©es interpol√©es pour traitement ult√©rieur"
                        )
                
                plt.close(fig_ert)
                
                # ÔøΩÔøΩ G√âN√âRATION RAPPORT PDF PROFESSIONNEL COMPLET
                st.markdown("---")
                st.markdown("### üìÑ Rapport PDF Professionnel Complet")
                
                col_pdf1, col_pdf2 = st.columns([3, 1])
                with col_pdf1:
                    st.info("üé® Rapport PDF avec graphiques int√©gr√©s, titres color√©s, statistiques et recommandations")
                
                with col_pdf2:
                    generate_pdf_btn = st.button("üîÑ G√©n√©rer Rapport PDF", key="gen_pdf_investigation")
                
                if generate_pdf_btn:
                    with st.spinner("üìù G√©n√©ration du rapport PDF professionnel..."):
                        try:
                            pdf_bytes = generate_professional_ert_report(
                                numbers=numbers,
                                file_name=file_name,
                                mineral_report=mineral_report if mineral_report else "",
                                df_corr=df_corr if 'df_corr' in locals() else None,
                                fig_ert=fig_ert,
                                fig_corr=fig_corr if 'fig_corr' in locals() else None,
                                grid_data=grid_data
                            )
                            
                            st.success("‚úÖ Rapport PDF g√©n√©r√© avec succ√®s!")
                            
                            # Bouton de t√©l√©chargement du rapport complet
                            st.download_button(
                                label="üì• T√âL√âCHARGER RAPPORT COMPLET PDF",
                                data=pdf_bytes,
                                file_name=f"{file_name}_RAPPORT_COMPLET_ERT.pdf",
                                mime="application/pdf",
                                key="dl_full_report",
                                help="Rapport professionnel avec couverture, statistiques, graphiques, interpr√©tations et recommandations"
                            )
                            
                        except Exception as e:
                            st.error(f"‚ùå Erreur g√©n√©ration PDF: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                
                investigation_report += rapport_ert + "\n"
            else:
                investigation_report += "‚ö†Ô∏è Impossible de g√©n√©rer les coupes ERT\n\n"
                
        except Exception as e:
            investigation_report += f"‚ùå Erreur g√©n√©ration coupes ERT: {str(e)}\n\n"
        
        # Recherche dans base ERT sp√©cifique
        ert_queries = [
            f"r√©sistivit√© {np.mean(arr):.1f} Ohm.m interpr√©tation g√©ologique",
            f"analyse ERT {len(numbers)} mesures qualit√© donn√©es",
            f"inversion r√©sistivit√© √©lectrique {np.min(arr):.1f}-{np.max(arr):.1f}"
        ]
        
        ert_rag_findings = ""
        if has_vectorstore:
            investigation_report += "üìö Recherche connaissances ERT dans la base vectorielle...\n"
            for i, query in enumerate(ert_queries, 1):
                try:
                    result = search_vectorstore(query)
                    if result and len(result) > 50:
                        ert_rag_findings += f"üîç Requ√™te {i}/3: '{query[:50]}...'\n"
                        ert_rag_findings += f"   üìÑ {result[:200]}...\n\n"
                except Exception as e:
                    ert_rag_findings += f"üîç Requ√™te {i}/3: ‚ùå Erreur: {str(e)}\n"
        
        if ert_rag_findings:
            investigation_report += "\nüìö CONNAISSANCES ERT DE LA BASE:\n"
            investigation_report += ert_rag_findings + "\n"
        elif has_vectorstore:
            investigation_report += "‚ö†Ô∏è Aucune connaissance ERT sp√©cifique trouv√©e dans la base\n\n"
    else:
        investigation_report += "‚ö†Ô∏è Pas de donn√©es ERT d√©tect√©es (nombres insuffisants ou hors plage)\n\n"
    
    # 5Ô∏è‚É£ RECHERCHE WEB CONTEXTUALIS√âE
    investigation_report += "5Ô∏è‚É£ PHASE 5: RECHERCHE WEB INTELLIGENTE\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    # Construire requ√™te web bas√©e sur tous les indices
    file_type = pattern_result.split(':')[0] if ':' in pattern_result else "inconnu"
    web_query = f"analyse {file_type} fichier binaire format {file_name}"
    
    # Initialiser web_result par d√©faut
    web_result = "Aucune recherche web effectu√©e"
    
    try:
        web_result_raw = web_search_enhanced(web_query)
        # web_search_enhanced retourne une string, pas un dict
        if web_result_raw and isinstance(web_result_raw, str):
            web_result = web_result_raw
            investigation_report += f"üåê Recherche: '{web_query}'\n"
            investigation_report += f"{web_result[:500]}...\n\n"
        else:
            investigation_report += f"üåê Recherche: '{web_query}'\n"
            investigation_report += f"‚ö†Ô∏è Aucun r√©sultat pertinent\n\n"
    except Exception as e:
        investigation_report += f"‚ùå Erreur recherche web: {str(e)}\n\n"
        web_result = f"Erreur: {str(e)}"
    
    # 6Ô∏è‚É£ SYNTH√àSE INTELLIGENTE MULTI-SOURCES
    investigation_report += "6Ô∏è‚É£ PHASE 6: SYNTH√àSE MULTI-SOURCES\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    # Utiliser le mod√®le LLM pour synth√©tiser toutes les informations
    synthesis_context = f"""
Fichier analys√©: {file_name} ({len(file_bytes)} bytes)
Type d√©tect√©: {pattern_result}
Entropie: {entropy_result}
Nombres extraits: {len(numbers) if numbers else 0}

Connaissances RAG:
{rag_findings[:500] if rag_findings else 'N/A'}

D√©tection ERT:
{ert_detection[:500] if (numbers and len(numbers) > 10 and 'ert_detection' in locals()) else 'N/A'}

Analyse Min√©rale:
{mineral_report[:800] if mineral_report else 'N/A'}

Recherche Web:
{web_result[:500] if web_result else 'N/A'}

QUESTION: Fournis une interpr√©tation scientifique compl√®te de ce fichier en combinant toutes ces sources.
Si des min√©raux ont √©t√© d√©tect√©s, mentionne les plus int√©ressants pour l'exploration mini√®re.
"""
    
    try:
        if 'model' in st.session_state and st.session_state.model:
            model = st.session_state.model
            tokenizer = st.session_state.tokenizer
            
            inputs = tokenizer(synthesis_context, return_tensors="pt", truncation=True, max_length=2000)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            attention_mask = inputs.get('attention_mask', None)
            
            with torch.inference_mode():
                outputs = model.generate(
                    inputs['input_ids'],
                    attention_mask=attention_mask,
                    max_new_tokens=3000,  # AUGMENT√â pour synth√®ses COMPL√àTES
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            synthesis = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            investigation_report += f"ü§ñ SYNTH√àSE IA:\n{synthesis}\n\n"
        else:
            investigation_report += "‚ö†Ô∏è Mod√®le LLM non disponible pour synth√®se\n\n"
    except Exception as e:
        investigation_report += f"‚ùå Erreur synth√®se IA: {e}\n\n"
    
    # 7Ô∏è‚É£ RECOMMANDATIONS ACTIONNABLES
    investigation_report += "7Ô∏è‚É£ PHASE 7: RECOMMANDATIONS\n"
    investigation_report += "‚îÄ" * 80 + "\n"
    
    recommendations = []
    
    if numbers and len(numbers) > 10:
        import numpy as np
        arr = np.array(numbers)
        if 0.1 <= np.min(arr) <= 10000:
            recommendations.append("‚úÖ Donn√©es ERT d√©tect√©es ‚Üí Utiliser PyGIMLI pour inversion")
            recommendations.append("‚úÖ Visualiser avec matplotlib/seaborn (utiliser AI_Plot_Generator)")
            recommendations.append("‚úÖ Calculer r√©sistivit√© apparente avec mathematical_calculator")
    
    if "haute" in entropy_result.lower():
        recommendations.append("üîí Entropie √©lev√©e ‚Üí Fichier potentiellement chiffr√©")
        recommendations.append("üîç Analyser avec outils cryptographiques")
    
    if "executable" in pattern_result.lower():
        recommendations.append("‚ö†Ô∏è Fichier ex√©cutable ‚Üí Analyser avec outils de reverse engineering")
        recommendations.append("üõ°Ô∏è Scanner avec antivirus avant ex√©cution")
    
    if not recommendations:
        recommendations.append("üìä Analyse compl√®te effectu√©e - Aucune action sp√©cifique requise")
    
    for rec in recommendations:
        investigation_report += f"{rec}\n"
    
    investigation_report += "\n" + "=" * 80 + "\n"
    investigation_report += "‚úÖ INVESTIGATION TERMIN√âE - Rapport complet g√©n√©r√©\n"
    
    # Split report into phases for better display
    phases_dict = {}
    report_lines = investigation_report.split('\n')
    current_phase = None
    current_content = []
    
    for line in report_lines:
        # Detect phase markers
        if 'Ô∏è‚É£ PHASE' in line:
            # Save previous phase if exists
            if current_phase is not None:
                phases_dict[current_phase] = '\n'.join(current_content)
            # Start new phase
            current_phase = line.strip()
            current_content = [line]
        else:
            if current_phase is not None:
                current_content.append(line)
    
    # Save last phase
    if current_phase is not None:
        phases_dict[current_phase] = '\n'.join(current_content)
    
    return {
        'full_report': investigation_report,
        'phases': phases_dict
    }

def ert_geophysical_interpretation(numbers: list) -> str:
    """Interpr√©tation g√©ophysique sp√©cialis√©e des donn√©es ERT"""
    if not numbers:
        return "‚ùå Aucune donn√©e pour l'interpr√©tation g√©ophysique"
    import numpy as np
    analysis = "üåç INTERPR√âTATION G√âOPHYSIQUE ERT\n"
    analysis += "=" * 40 + "\n\n"
    arr = np.array(numbers)
    # Classification des valeurs de r√©sistivit√©
    low_resistivity = arr[arr < 10] # < 10 Ohm.m
    medium_resistivity = arr[(arr >= 10) & (arr < 100)] # 10-100 Ohm.m
    high_resistivity = arr[arr >= 100] # > 100 Ohm.m
    analysis += f"üìä CLASSIFICATION DES R√âSISTIVIT√âS:\n"
    analysis += f" ‚Ä¢ Faible r√©sistivit√© (< 10 Ohm.m): {len(low_resistivity)} valeurs\n"
    analysis += f" ‚Üí Argile, eau sal√©e, min√©raux conducteurs\n"
    analysis += f" ‚Ä¢ R√©sistivit√© moyenne (10-100 Ohm.m): {len(medium_resistivity)} valeurs\n"
    analysis += f" ‚Üí Sols sableux, roches s√©dimentaires\n"
    analysis += f" ‚Ä¢ Haute r√©sistivit√© (> 100 Ohm.m): {len(high_resistivity)} valeurs\n"
    analysis += f" ‚Üí Roches cristallines, vides, air\n\n"
    # Ajout des couleurs et descriptions
    analysis += f"üé® COULEURS ET D√âSCRIPTIONS PAR CAT√âGORIE:\n"
    sample_values = np.unique(np.round(arr, 1))[:10] # √âchantillon de valeurs uniques
    for val in sample_values:
        color_desc = get_resistivity_color(val)
        analysis += f" ‚Ä¢ œÅ = {val} Œ©.m: {color_desc}\n"
    analysis += "\n"
    # Recherche dynamique pour comparaisons
    analysis += f"üîç COMPARAISONS DYNAMIQUES AVEC MAT√âRIAUX (recherche internet):\n"
    analysis += f"Liquides (eau pure, sal√©e, huiles):\n{fetch_material_resistivities('liquids')}\n\n"
    analysis += f"Min√©raux/Sols (argile, sable, limon):\n{fetch_material_resistivities('minerals soils')}\n\n"
    analysis += f"Roches (granite, calcaire, gr√®s):\n{fetch_material_resistivities('rocks')}\n\n"
    # Analyse d'h√©t√©rog√©n√©it√©
    heterogeneity = np.std(arr) / np.mean(arr)
    analysis += f"üéØ H√âT√âROG√âN√âIT√â DU MILIEU:\n"
    analysis += f" ‚Ä¢ Coefficient de variation: {heterogeneity:.3f}\n"
    if heterogeneity < 0.5:
        analysis += f" ‚Üí Milieu homog√®ne (roche massive)\n"
    elif heterogeneity < 1.0:
        analysis += f" ‚Üí Milieu mod√©r√©ment h√©t√©rog√®ne (s√©diments)\n"
    else:
        analysis += f" ‚Üí Milieu tr√®s h√©t√©rog√®ne (zone fractur√©e/caverneuse)\n\n"
    # Estimation de profondeur g√©n√©rale
    mean_rho = np.mean(arr)
    analysis += f"üìè ESTIMATION DE PROFONDEUR (bas√©e sur œÅ moyenne = {mean_rho:.1f} Œ©.m, g√©n√©rique):\n"
    if mean_rho < 10:
        analysis += " ‚Üí Superficielle (0-5 m): Couches argileuses ou satur√©es\n"
    elif mean_rho < 100:
        analysis += " ‚Üí Moyenne (5-20 m): Aquif√®res sableux\n"
    else:
        analysis += " ‚Üí Profonde (>20 m): Substratum r√©sistant\n\n"
    # D√©tection d'anomalies potentielles
    z_scores = (arr - np.mean(arr)) / np.std(arr)
    anomalies_high = arr[z_scores > 2] # Anomalies hautes
    anomalies_low = arr[z_scores < -2] # Anomalies basses
    if len(anomalies_high) > 0 or len(anomalies_low) > 0:
        analysis += f"üö® ANOMALIES D√âTECT√âES:\n"
        if len(anomalies_high) > 0:
            analysis += f" ‚Ä¢ {len(anomalies_high)} anomalies haute r√©sistivit√©\n"
            analysis += f" ‚Üí Possibles: vides, fractures, roches r√©sistantes (couleur: rouge)\n"
        if len(anomalies_low) > 0:
            analysis += f" ‚Ä¢ {len(anomalies_low)} anomalies basse r√©sistivit√©\n"
            analysis += f" ‚Üí Possibles: eau, argile, min√©raux conducteurs (couleur: bleu)\n\n"
    # Applications potentielles
    analysis += f"üèóÔ∏è APPLICATIONS POTENTIELLES:\n"
    analysis += f" ‚Ä¢ Hydrog√©ologie: d√©tection aquif√®res\n"
    analysis += f" ‚Ä¢ G√©otechnique: stabilit√© des sols\n"
    analysis += f" ‚Ä¢ Arch√©ologie: structures enterr√©es\n"
    analysis += f" ‚Ä¢ Environnement: pollution des sols\n"
    analysis += f" ‚Ä¢ G√©nie civil: fouilles et tunnels\n"
    return analysis
def ert_quality_assessment(numbers: list) -> str:
    """√âvaluation de la qualit√© des donn√©es ERT"""
    if not numbers:
        return "‚ùå Aucune donn√©e pour l'√©valuation qualit√©"
    import numpy as np
    analysis = "‚≠ê √âVALUATION QUALIT√â DONN√âES ERT\n"
    analysis += "=" * 40 + "\n\n"
    arr = np.array(numbers)
    # Crit√®res de qualit√©
    quality_score = 0
    max_score = 5
    # 1. Plage de valeurs r√©aliste
    if 0.1 <= np.min(arr) <= 10000:
        quality_score += 1
        analysis += f"‚úÖ Plage de r√©sistivit√© r√©aliste\n"
    else:
        analysis += f"‚ùå Plage de r√©sistivit√© suspecte\n"
    # 2. Nombre de mesures suffisant
    if len(arr) >= 50:
        quality_score += 1
        analysis += f"‚úÖ Nombre de mesures suffisant ({len(arr)})\n"
    else:
        analysis += f"‚ö†Ô∏è Peu de mesures ({len(arr)}) - pr√©cision limit√©e\n"
    # 3. Contraste suffisant
    contrast = np.max(arr) / np.min(arr)
    if contrast >= 2:
        quality_score += 1
        analysis += f"‚úÖ Bon contraste ({contrast:.1f})\n"
    else:
        analysis += f"‚ö†Ô∏è Contraste faible ({contrast:.1f})\n"
    # 4. Distribution r√©aliste
    try:
        from scipy import stats
        log_data = np.log(arr[arr > 0])
        _, p_value = stats.shapiro(log_data[:min(5000, len(log_data))])
        if p_value > 0.05:
            quality_score += 1
            analysis += f"‚úÖ Distribution log-normale (p={p_value:.3f})\n"
        else:
            analysis += f"‚ö†Ô∏è Distribution non standard\n"
    except:
        analysis += f"‚ö†Ô∏è Test de distribution impossible\n"
    # 5. Absence d'outliers extr√™mes
    z_scores = np.abs((arr - np.mean(arr)) / np.std(arr))
    extreme_outliers = np.sum(z_scores > 5)
    if extreme_outliers == 0:
        quality_score += 1
        analysis += f"‚úÖ Pas d'outliers extr√™mes\n"
    else:
        analysis += f"‚ö†Ô∏è {extreme_outliers} outliers extr√™mes d√©tect√©s\n"
    # Score final
    quality_percentage = (quality_score / max_score) * 100
    analysis += f"\nüéØ SCORE QUALIT√â: {quality_score}/{max_score} ({quality_percentage:.1f}%)\n"
    if quality_percentage >= 80:
        analysis += f"‚≠ê QUALIT√â EXCELLENTE - Donn√©es fiables pour inversion\n"
    elif quality_percentage >= 60:
        analysis += f"‚úÖ QUALIT√â BONNE - Donn√©es utilisables avec pr√©caution\n"
    elif quality_percentage >= 40:
        analysis += f"‚ö†Ô∏è QUALIT√â MOYENNE - R√©sultats √† interpr√©ter prudemment\n"
    else:
        analysis += f"‚ùå QUALIT√â INSUFFISANTE - Acquisition √† recommencer\n"
    return analysis
# Fonction d'analyse intelligente utilisant le mod√®le Qwen directement
def analyze_with_ai(query: str, file_bytes: bytes, numbers: list, hex_dump: str, n_clusters: int = 3, model=None, tokenizer=None, device=None) -> str:
    """Analyse intelligente utilisant le mod√®le Qwen avec acc√®s automatique aux outils et enrichissement ERT"""
   
    # R√©cup√©rer les variables depuis session_state si non fournies
    if model is None:
        try:
            model = st.session_state.get('model', None)
            tokenizer = st.session_state.get('tokenizer', None)
            device = st.session_state.get('device', None)
        except:
            pass
   
    # V√©rifier que nous avons un mod√®le
    if model is None or tokenizer is None:
        return """‚ùå ERREUR: Mod√®le LLM non disponible
       
üîß Le mod√®le n'a pas pu √™tre charg√© pour cette analyse.
üìã Analyse de base r√©alis√©e avec les outils disponibles uniquement.
       
Veuillez red√©marrer l'application pour charger le mod√®le LLM."""
    # Enrichissement automatique de la base ERT si donn√©es d√©tect√©es
    enrichment_status = ""
    if numbers and len(numbers) > 20:
        try:
            import numpy as np
            arr = np.array(numbers)
            if 0.1 <= np.min(arr) <= 10000:
                # Importer et utiliser l'enrichisseur ERT
                from ert_database_enrichment import create_ert_knowledge_base
              
                # Enrichir la base avec des connaissances ERT contextuelles
                if st.session_state.vectorstore:
                    vectorstore_path = "/tmp/enriched_ert_vectordb"
                    enriched_vs, msg = create_ert_knowledge_base(vectorstore_path, numbers)
                    if enriched_vs:
                        # Fusionner avec la base existante si possible
                        enrichment_status = f"‚úÖ Base enrichie automatiquement avec connaissances ERT: {msg}"
                    else:
                        enrichment_status = f"‚ö†Ô∏è Enrichissement partiel: {msg}"
                else:
                    enrichment_status = "‚ö†Ô∏è Base vectorielle non disponible pour enrichissement"
        except Exception as e:
            enrichment_status = f"‚ùå Erreur enrichissement ERT: {e}"
    # Informations de base sur le fichier
    basic_info = f"""
üìÅ FICHIER ANALYS√â:
- Nom: {uploaded_file.name if 'uploaded_file' in locals() else 'Fichier upload√©'}
- Taille: {len(file_bytes)} bytes ({len(file_bytes)/1024:.1f} KB)
- Nombres extraits: {len(numbers) if numbers else 0}
- Clusters identifi√©s: {n_clusters if numbers else 0}
üß† ENRICHISSEMENT AUTOMATIQUE:
{enrichment_status}
üîç DUMP HEXAD√âCIMAL (aper√ßu):
{hex_dump[:300]}...
‚ùì QUESTION: {query}
"""
    # PHASE 1: Analyses de base pour identifier le fichier
    try:
        entropy_result = entropy_analysis(file_bytes)
        pattern_result = pattern_recognition(file_bytes)
        metadata_result = metadata_extraction(file_bytes)
        compression_result = compression_ratio(file_bytes)
        frequency_result = frequency_analysis(file_bytes)
        base_analysis = f"""
üî¨ ANALYSES DE BASE R√âALIS√âES:
üìä ENTROPIE: {entropy_result}
üéØ PATTERNS: {pattern_result}
üìã M√âTADONN√âES: {metadata_result}
üóúÔ∏è COMPRESSION: {compression_result}
üìà FR√âQUENCE: {frequency_result}
"""
        # PHASE 2: Recherche dans la base RAG pour identifier le type et obtenir des connaissances
        rag_search_query = f"Type de fichier binaire: {pattern_result[:100]}... Entropie: {entropy_result[:50]}... M√©tadonn√©es: {metadata_result[:100]}..."
        rag_context = ""
        if st.session_state.vectorstore:
            try:
                rag_result = search_vectorstore(rag_search_query)
                rag_context = f"\n\nüìö CONNAISSANCES RAG:\n{rag_result}"
            except Exception as e:
                rag_context = f"\n\nüìö CONNAISSANCES RAG: Erreur - {e}"
        # PHASE 3: Recherche web cibl√©e bas√©e sur les analyses - AM√âLIOR√âE POUR ERT
        if numbers and len(numbers) > 10:
            # V√©rifier si potentiellement ERT
            import numpy as np
            arr = np.array(numbers)
            if 0.1 <= np.min(arr) <= 10000:
                web_search_query = f"ERT electrical resistivity tomography data interpretation {np.mean(arr):.1f} Ohm.m geophysical analysis subsurface"
            else:
                web_search_query = f"analyse fichier binaire {pattern_result.split(':')[0] if ':' in pattern_result else 'inconnu'} type format entropie cybers√©curit√©"
        else:
            web_search_query = f"analyse fichier binaire {pattern_result.split(':')[0] if ':' in pattern_result else 'inconnu'} type format entropie cybers√©curit√©"
          
        web_context = ""
        try:
            web_result = web_search_enhanced(web_search_query)
            web_context = f"\n\nüåê RECHERCHE WEB:\n{web_result}"
        except Exception as e:
            web_context = f"\n\nüåê RECHERCHE WEB: Erreur - {e}"
        # PHASE 4: Analyses statistiques avanc√©es si applicable
        stats_context = ""
        if numbers:
            try:
                stats_result = statistical_analysis(numbers)
                if len(numbers) >= 3:
                    correlation_result = correlation_analysis(numbers)
                    stats_context += f"\nüîó CORR√âLATIONS: {correlation_result}"
                if len(numbers) >= 10:
                    anomaly_result = anomaly_detection(numbers)
                    stats_context += f"\nüö® ANOMALIES: {anomaly_result}"
                if len(numbers) >= 32:
                    spectral_result = spectral_analysis(numbers)
                    stats_context += f"\nüåä SPECTRAL: {spectral_result}"
                stats_context = f"\n\nüìä ANALYSES STATISTIQUES:\n{stats_result}{stats_context}"
            except Exception as e:
                stats_context = f"\n\nüìä ANALYSES STATISTIQUES: Erreur - {e}"
        # PHASE 4.5: D√©tection et analyse sp√©cialis√©e ERT
        ert_context = ""
        ert_detected = False
        if numbers and len(numbers) > 10:
            try:
                ert_detection_result = ert_data_detection(file_bytes, numbers)
                # V√©rifier si les donn√©es semblent √™tre ERT (bas√© sur les crit√®res de la fonction)
                import numpy as np
                arr = np.array(numbers)
                if 0.1 <= np.min(arr) <= 10000 and len(numbers) >= 20:
                    ert_detected = True
                    # Analyses ERT sp√©cialis√©es
                    ert_inversion = ert_inversion_analysis(numbers)
                    ert_interpretation = ert_geophysical_interpretation(numbers)
                    ert_quality = ert_quality_assessment(numbers)
                    ert_context = f"\n\nüîç ANALYSES SP√âCIALIS√âES ERT:\n{ert_detection_result}\n\n{ert_inversion}\n\n{ert_interpretation}\n\n{ert_quality}"
                    # Recherche RAG sp√©cialis√©e ERT avec enrichissement automatique
                    ert_rag_query = f"ERT Electrical Resistivity Tomography donn√©es r√©sistivit√© {np.mean(arr):.1f} Ohm.m interpr√©tation g√©ophysique inversion sismique hydrog√©ologie couleurs profondeur nature mat√©riaux liquides min√©raux formules calcul r√©sistivit√© apparente Schlumberger Wenner Dipole-Dipole"
                    if st.session_state.vectorstore:
                        try:
                            ert_rag_result = search_vectorstore(ert_rag_query)
                            ert_context += f"\n\nüìö CONNAISSANCES ERT RAG:\n{ert_rag_result}"
                          
                            # Utiliser le syst√®me d'enrichissement pour obtenir plus de contexte
                            enriched_context = rag_enhanced_analysis(
                                ert_rag_query,
                                ert_rag_result,
                                ert_data={'mean': np.mean(arr), 'std': np.std(arr), 'min': np.min(arr), 'max': np.max(arr)}
                            )
                            ert_context += f"\n\nüî¨ ANALYSE RAG ENRICHIE:\n{enriched_context}"
                          
                        except Exception as e:
                            ert_context += f"\n\nüìö CONNAISSANCES ERT RAG: Erreur - {e}"
                    # Recherche web sp√©cialis√©e ERT avec requ√™tes multiples
                    ert_web_queries = [
                        f"ERT tomography r√©sistivit√© √©lectrique interpr√©tation donn√©es {np.mean(arr):.1f} Ohm.m g√©ophysique hydrog√©ologie couleurs visualisation",
                        f"electrical resistivity {np.mean(arr):.1f} ohm.m subsurface interpretation environmental depth nature",
                        "ERT data processing inversion algorithms geophysical survey materials comparison"
                    ]
                  
                    for i, ert_web_query in enumerate(ert_web_queries):
                        try:
                            ert_web_result = web_search_enhanced(ert_web_query, "ert_specialized")
                            ert_context += f"\n\nüåê RECHERCHE WEB ERT #{i+1}:\n{ert_web_result}"
                        except Exception as e:
                            ert_context += f"\n\nüåê RECHERCHE WEB ERT #{i+1}: Erreur - {e}"
            except Exception as e:
                ert_context = f"\n\nüîç ANALYSE ERT: Erreur lors de l'analyse sp√©cialis√©e - {e}"
        # PHASE 5: Synth√®se experte avec toutes les informations
        synthesis_context = f"""
{basic_info}
{base_analysis}
{rag_context}
{web_context}
{stats_context}
{ert_context}
üéØ PROTOCOLE D'ANALYSE EXPERTE:
1. Identifier le type de fichier bas√© sur les patterns et signatures d√©tect√©s
2. √âvaluer les risques de s√©curit√© (entropie √©lev√©e = possible cryptage/malware)
3. Analyser la structure et le contenu bas√© sur les connaissances RAG
4. Contextualiser avec les informations web r√©centes
5. Si donn√©es ERT d√©tect√©es, interpr√©ter g√©ophysiquement avec connaissances sp√©cialis√©es, incluant couleurs de visualisation, estimations de profondeur, nature des mat√©riaux, et comparaisons dynamiques avec liquides/min√©raux/roches via recherches internet
6. Pour fichiers .dat ERT, utilisez mathematical_calculator pour les formules de r√©sistivit√© apparente du document FicheERT.pdf: Schlumberger: pi*(L**2 - l**2)/(2*l) * V/I (L=AB/2, l=MN/2), Wenner: 2*pi*a * V/I (a=AM), Dipole-Dipole: pi*n*(n+1)*(n+2)*a * V/I (n=facteur s√©paration)
7. Fournir une interpr√©tation professionnelle du fichier, en rendant l'analyse la plus puissante possible en ERT et g√©ophysique
INSTRUCTION: En tant qu'expert mondial en cybers√©curit√©, analyse de fichiers binaires, g√©ophysique ERT/tomographie de r√©sistivit√© √©lectrique, fournissez une analyse compl√®te, professionnelle et s√©curis√©e de ce fichier. Pour ERT: d√©crivez nature, profondeur, couleurs, comparez avec mat√©riaux (recherchez dynamiquement liquides, min√©raux par cat√©gories), et r√©pondez dynamiquement aux comparaisons. Utilisez mathematical_calculator pour les calculs de r√©sistivit√© apparente si V, I et espacements sont disponibles.
"""
        # Utiliser le mod√®le Qwen pour la synth√®se finale avec optimisation GPU
        messages = [
            {"role": "system", "content": "Tu es un expert mondial en cybers√©curit√©, analyse de fichiers binaires, intelligence artificielle et g√©ophysique (ERT/tomographie de r√©sistivit√© √©lectrique). Analyse ce fichier de mani√®re professionnelle en utilisant toutes les informations disponibles. Identifie d'abord le type de fichier, √©value les risques de s√©curit√©, puis fournis une interpr√©tation compl√®te incluant l'interpr√©tation g√©ophysique si des donn√©es ERT sont d√©tect√©es. Pour ERT: d√©cris nature, profondeur, couleurs de visualisation, compare avec liquides/min√©raux/roches via recherches dynamiques, et rends l'analyse la plus puissante possible. Pour fichiers .dat, utilise mathematical_calculator avec les formules: Schlumberger: pi*(L**2 - l**2)/(2*l) * V/I, Wenner: 2*pi*a * V/I, Dipole-Dipole: pi*n*(n+1)*(n+2)*a * V/I."},
            {"role": "user", "content": synthesis_context}
        ]
       
        # Optimisation GPU: S'assurer que le mod√®le est sur le bon device
        if torch.cuda.is_available() and model.device.type != 'cuda':
            model = model.to('cuda')
       
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)
       
        # Create attention mask to avoid warnings when pad_token == eos_token
        attention_mask = (inputs != tokenizer.pad_token_id).long().to(model.device)
       
        # Optimisation pour GPU: utiliser torch.cuda.amp pour mixed precision si GPU disponible
        if model.device.type == 'cuda':
            with torch.no_grad(), torch.cuda.amp.autocast():
                outputs = model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_new_tokens=3000,  # AUGMENT√â pour analyses COMPL√àTES
                    temperature=0.6,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    use_cache=True, # Optimisation GPU
                    num_beams=1 # Plus rapide pour GPU
                )
        else:
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_new_tokens=3000,  # AUGMENT√â pour analyses COMPL√àTES
                    temperature=0.6,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
        final_analysis = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
       
        # Information sur les performances
        device_info = f"üñ•Ô∏è Device utilis√©: {model.device.type.upper()}"
        if model.device.type == 'cuda':
            memory_used = torch.cuda.memory_allocated() / 1024**3
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            device_info += f" | VRAM: {memory_used:.1f}/{memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)"
       
        return f"""üîç ANALYSE PROFESSIONNELLE DE FICHIER BINAIRE
{device_info}
{basic_info}
{base_analysis}
{rag_context}
{web_context}
{stats_context}
{ert_context}
üéØ ANALYSE EXPERTE FINALE:
{final_analysis}
‚úÖ Analyse termin√©e - Toutes les sources d'information ont √©t√© consult√©es et synth√©tis√©es.
‚ö° Performance: {'GPU acc√©l√©r√©' if model.device.type == 'cuda' else 'CPU standard'}"""
    except Exception as e:
        # Fallback avec analyse basique
        try:
            basic_entropy = entropy_analysis(file_bytes)
            basic_patterns = pattern_recognition(file_bytes)
            basic_metadata = metadata_extraction(file_bytes)
            return f"""‚ùå Erreur dans l'analyse compl√®te: {str(e)}
üî¨ ANALYSE DE BASE R√âALIS√âE:
üìä ENTROPIE: {basic_entropy}
üéØ PATTERNS: {basic_patterns}
üìã M√âTADONN√âES: {basic_metadata}
{basic_info}
Recommandation: Le fichier pr√©sente une entropie de {basic_entropy.split('/')[0] if '/' in basic_entropy else 'inconnue'}.
Type d√©tect√©: {basic_patterns.split(':')[0] if ':' in basic_patterns else 'inconnu'}."""
        except Exception as e2:
            return f"‚ùå Erreur critique lors de l'analyse: {str(e)}\nErreur de fallback: {str(e2)}\n\nInformations de base:\n{basic_info}"
def hex_ascii_view(file_bytes, bytes_per_line=16, max_lines=50):
    lines = []
    for i in range(0, min(len(file_bytes), bytes_per_line*max_lines), bytes_per_line):
        chunk = file_bytes[i:i+bytes_per_line]
        hex_bytes = " ".join(f"{b:02X}" for b in chunk)
        ascii_bytes = "".join([chr(b) if 32 <= b <= 126 else "." for b in chunk])
        lines.append(f"{i:08X} {hex_bytes:<48} |{ascii_bytes}|")
    return "\n".join(lines)
def extract_numbers(file_bytes):
    # On convertit les parties ASCII pour extraire float/int
    ascii_text = "".join([chr(b) if 32 <= b <= 126 else " " for b in file_bytes])
    # regex pour float ou int
    numbers = re.findall(r"[-+]?\d*\.\d+|\d+", ascii_text)
    numbers = [float(n) for n in numbers]
    return numbers
def cluster_numbers(numbers, n_clusters=3):
    if not numbers:
        return None
    X = np.array(numbers).reshape(-1,1)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
    labels = kmeans.labels_
    centers = kmeans.cluster_centers_
    return labels, centers
def load_model_state(file_path: Path) -> Dict[str, Any]:
    ext = file_path.suffix
    if ext == ".safetensors":
        state_dict = load_file(str(file_path), device="cpu")
    elif ext in [".bin", ".pt", ".ckpt"]:
        try:
            # Essayer avec weights_only=True pour PyTorch 2.6+
            state_dict = torch.load(file_path, map_location="cpu", weights_only=True)
        except TypeError:
            # Fallback pour anciennes versions de PyTorch
            state_dict = torch.load(file_path, map_location="cpu")
    else:
        raise ValueError(f"Extension non support√©e : {ext}")
    return state_dict
def summarize_state_dict(state_dict: Dict[str, torch.Tensor]) -> str:
    summary = []
    for key, tensor in state_dict.items():
        summary.append(f"Cl√©: {key}, Shape: {tensor.shape}, Dtype: {tensor.dtype}, Mean: {tensor.mean().item():.4f}, Std: {tensor.std().item():.4f}")
    return "\n".join(summary[:10]) # Limit to first 10 for brevity
# --------- Streamlit Interface ---------
st.title("üîç Streamlit Binary Viewer + KMeans Clustering + LLM Analysis Agent")
# Section for PDF uploads and indexing
st.subheader("üìö Upload PDFs for Knowledge Base")
uploaded_pdfs = st.file_uploader("Choisir des PDFs pour indexer (connaissances pour l'analyse)", type=["pdf"], accept_multiple_files=True)
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if uploaded_pdfs and st.button("Indexer les PDFs dans la base vectorielle"):
    with st.spinner("Indexation en cours..."):
        docs = []
        for pdf in uploaded_pdfs:
            # Save uploaded PDF to temp file
            temp_path = Path(f"/tmp/{pdf.name}")
            with open(temp_path, "wb") as f:
                f.write(pdf.getvalue())
            loader = PyPDFLoader(str(temp_path))
            loaded_docs = loader.load()
          
            # Check if text was extracted
            if not any(doc.page_content.strip() for doc in loaded_docs):
                st.write(f"No text extracted from {pdf.name}, trying OCR...")
                try:
                    images = convert_from_path(str(temp_path))
                    ocr_text = ""
                    for image in images:
                        ocr_text += pytesseract.image_to_string(image) + "\n"
                    # Replace with OCR document
                    loaded_docs = [Document(page_content=ocr_text, metadata={"source": pdf.name})]
                    st.write(f"OCR extracted {len(ocr_text)} characters from {pdf.name}")
                except Exception as e:
                    st.error(f"OCR failed for {pdf.name}: {e}")
                    loaded_docs = []
          
            docs.extend(loaded_docs)
            st.write(f"Loaded {len(loaded_docs)} pages/documents from {pdf.name}")
      
        st.write(f"Total documents loaded: {len(docs)}")
      
        # Debug: check content
        if docs:
            st.write(f"Sample content from first doc: '{docs[0].page_content[:200]}'")
            non_empty = sum(1 for doc in docs if doc.page_content.strip())
            st.write(f"Documents with non-empty content: {non_empty}/{len(docs)}")
      
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
      
        st.write(f"Total splits created: {len(splits)}")
      
        if not splits:
            st.error("Aucun document valide trouv√© dans les PDFs upload√©s. Assurez-vous que les PDFs contiennent du texte extractable (pas des images scann√©es). Si le PDF contient du texte mais n'est pas extrait, essayez un PDF diff√©rent ou utilisez OCR.")
        else:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            embeddings = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2', device=device)
          
            st.session_state.vectorstore = FAISS.from_documents(splits, embeddings)
            
            # Synchroniser avec vectordb pour que Kibali ait acc√®s
            if "vectordb" not in st.session_state:
                st.session_state.vectordb = None
            
            # Fusionner avec vectordb existant si pr√©sent, sinon cr√©er nouveau
            if st.session_state.vectordb is not None:
                try:
                    # Ajouter les nouveaux documents √† la base existante
                    st.session_state.vectordb.add_documents(splits)
                    st.info("üìö Documents ajout√©s √† la base vectorielle existante de Kibali Analyst")
                except:
                    # Si erreur, remplacer compl√®tement
                    st.session_state.vectordb = st.session_state.vectorstore
                    st.warning("‚ö†Ô∏è Remplacement de la base vectorielle")
            else:
                # Cr√©er nouvelle base
                st.session_state.vectordb = st.session_state.vectorstore
                st.info("‚ú® Nouvelle base vectorielle cr√©√©e pour Kibali Analyst")
      
            st.success("‚úÖ Base vectorielle cr√©√©e avec succ√®s ! Kibali Analyst peut maintenant acc√©der √† ces documents.")
            
            # Sauvegarder aussi dans le chemin standard si possible
            try:
                VECTORDB_PATH = "/root/chatbot_data/vectordb"
                os.makedirs(os.path.dirname(VECTORDB_PATH), exist_ok=True)
                st.session_state.vectordb.save_local(VECTORDB_PATH)
                st.info(f"üíæ Base sauvegard√©e dans {VECTORDB_PATH}")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Impossible de sauvegarder: {e}")
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SECTION MULTI-FICHIERS ERT MULTI-FR√âQUENCES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
st.markdown("---")
st.subheader("üìä Parseur Multi-Fr√©quences ERT - Fichiers Compl√©mentaires")
st.info("""
üéØ **Uploadez plusieurs fichiers .dat compl√©mentaires** avec multi-fr√©quences (MHz)

Format attendu :
- En-t√™te : `..., 1000 MHz, 500 MHz, 250 MHz, ...`
- Donn√©es : `project, survey_point, depth, res1, res2, res3, ...`

‚úÖ Fusion automatique des fichiers
‚úÖ Coordonn√©es spatiales correctes (X, Y, Z)
‚úÖ Visualisations 2D/3D par fr√©quence
""")

# Upload multiple files
uploaded_ert_files = st.file_uploader(
    "üì§ Chargez vos fichiers .dat ERT (plusieurs fichiers possibles)",
    type=['dat', 'txt'],
    accept_multiple_files=True,
    key="multi_freq_uploader"
)

if uploaded_ert_files and len(uploaded_ert_files) > 0:
    with st.spinner(f"üìä Parsing de {len(uploaded_ert_files)} fichier(s)..."):
        # Sauvegarder temporairement les fichiers
        temp_paths = []
        for uploaded_file in uploaded_ert_files:
            temp_path = f"/tmp/ert_{uploaded_file.name}"
            with open(temp_path, 'wb') as f:
                f.write(uploaded_file.getbuffer())
            temp_paths.append(temp_path)
        
        # Parser avec le nouveau parseur
        parser = MultiFreqERTParser()
        df_ert = parser.parse_multiple_files(temp_paths)
        
        if not df_ert.empty:
            st.success(f"‚úÖ {len(df_ert)} mesures charg√©es depuis {len(uploaded_ert_files)} fichier(s) !")
            
            # Afficher m√©tadonn√©es
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìÅ Fichiers", parser.metadata['num_files'])
                st.metric("üì° Fr√©quences", parser.metadata['num_frequencies'])
            with col2:
                st.metric("üìç Survey Points", parser.metadata['num_survey_points'])
                st.metric("üìè Profondeurs", parser.metadata['num_depths'])
            with col3:
                st.metric("üî¨ Projets", parser.metadata['num_projects'])
                st.metric("üìä Mesures", parser.metadata['total_measurements'])
            with col4:
                st.metric("‚¨áÔ∏è Prof. min", f"{parser.metadata['depth_range'][0]:.1f} m")
                st.metric("‚¨áÔ∏è Prof. max", f"{parser.metadata['depth_range'][1]:.1f} m")
            
            # Rapport statistique
            with st.expander("üìã Rapport Statistique Complet", expanded=False):
                report = parser.generate_statistics_report()
                st.text(report)
            
            # Afficher donn√©es brutes
            with st.expander("üî¢ Donn√©es Brutes Fusionn√©es", expanded=False):
                st.dataframe(df_ert.head(100), use_container_width=True)
            
            # COORDONN√âES CORRECTES
            st.markdown("### üéØ Coordonn√©es Spatiales Correctes (X, Y, Z)")
            coords_df = parser.get_coordinates_corrected()
            
            with st.expander("üìê Tableau des Coordonn√©es", expanded=False):
                st.dataframe(coords_df.head(50), use_container_width=True)
                
                # Export CSV
                csv_coords = coords_df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• T√©l√©charger Coordonn√©es (CSV)",
                    data=csv_coords,
                    file_name="ert_coordinates.csv",
                    mime="text/csv"
                )
            
            # VISUALISATIONS PAR FR√âQUENCE
            st.markdown("### üìä Visualisations Multi-Fr√©quences")
            
            freq_tabs = st.tabs([f"üì° {freq} MHz" for freq in parser.frequencies] + ["üîÑ Comparaison", "üåê 3D"])
            
            # Onglet pour chaque fr√©quence
            for i, freq in enumerate(parser.frequencies):
                with freq_tabs[i]:
                    st.markdown(f"#### Coupe 2D - {freq} MHz")
                    
                    fig_2d = parser.create_2d_section_by_frequency(freq)
                    if fig_2d:
                        st.plotly_chart(fig_2d, use_container_width=True)
                        
                        # Stats par fr√©quence
                        freq_data = df_ert[df_ert['frequency_MHz'] == freq]
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Mesures", len(freq_data))
                        with col2:
                            st.metric("R√©s. moy.", f"{freq_data['resistivity'].mean():.2f} Œ©¬∑m")
                        with col3:
                            st.metric("√âcart-type", f"{freq_data['resistivity'].std():.2f}")
            
            # Onglet comparaison
            with freq_tabs[-2]:
                st.markdown("#### Comparaison Multi-Fr√©quences")
                fig_comp = parser.create_frequency_comparison()
                if fig_comp:
                    st.plotly_chart(fig_comp, use_container_width=True)
                    
                    st.info("""
                    üí° **Interpr√©tation des courbes** :
                    - Les diff√©rentes fr√©quences p√©n√®trent √† diff√©rentes profondeurs
                    - Les √©carts entre courbes r√©v√®lent l'h√©t√©rog√©n√©it√© du sous-sol
                    - Les croisements indiquent des changements de mat√©riaux
                    """)
            
            # Onglet 3D
            with freq_tabs[-1]:
                st.markdown("#### Volume 3D Interactif")
                fig_3d = parser.create_3d_volume()
                if fig_3d:
                    st.plotly_chart(fig_3d, use_container_width=True)
                    
                    st.success("‚úÖ Cliquez-glissez pour rotation, molette pour zoom")
            
            # EXPORT EXCEL
            st.markdown("### üì• Export des Donn√©es")
            col1, col2 = st.columns(2)
            
            with col1:
                # Export CSV complet
                csv_full = df_ert.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üìä T√©l√©charger Donn√©es Compl√®tes (CSV)",
                    data=csv_full,
                    file_name="ert_multi_freq_complet.csv",
                    mime="text/csv"
                )
            
            with col2:
                # Export Excel
                if st.button("üì¶ G√©n√©rer Excel Multi-Feuilles", type="primary"):
                    excel_path = "/tmp/ert_multi_freq_export.xlsx"
                    msg = parser.export_to_excel(excel_path)
                    st.success(msg)
                    
                    if os.path.exists(excel_path):
                        with open(excel_path, 'rb') as f:
                            st.download_button(
                                label="üì• T√©l√©charger Excel",
                                data=f.read(),
                                file_name="ert_multi_freq_export.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
            
            # Sauvegarder dans session state pour utilisation par Kibali
            st.session_state['multi_freq_data'] = df_ert
            st.session_state['multi_freq_coords'] = coords_df
            st.session_state['multi_freq_parser'] = parser
        
        else:
            st.error("‚ùå Aucune donn√©e extraite. V√©rifiez le format des fichiers.")

# Section for binary file upload
st.markdown("---")
uploaded_file = st.file_uploader("Choisir un fichier binaire", type=["bin","dat","raw","bin","safetensors","pt","ckpt"])
if uploaded_file:
    file_bytes = uploaded_file.read()
    file_path = Path("/tmp/uploaded_file")
    file_path.write_bytes(file_bytes) # Save for potential model loading
    st.subheader("üìú Hex + ASCII Dump")
    hex_dump = hex_ascii_view(file_bytes, bytes_per_line=16, max_lines=100)
    st.text_area("Hex Dump", hex_dump, height=400)
    st.subheader("üî¢ Extraction des nombres")
    numbers = extract_numbers(file_bytes)
    if numbers:
        # SAUVEGARDER LES DONN√âES POUR VISUALISATION
        st.session_state.current_file_data = numbers
        st.session_state.current_filename = uploaded_file.name
        
        df = pd.DataFrame(numbers, columns=["Value"])
        st.dataframe(df)
        st.subheader("üìä Statistiques rapides")
        st.write(df.describe())
        
        # üß† ANALYSE INTELLIGENTE KIBALI POUR ERT
        if uploaded_file.name.lower().endswith('.dat'):
            st.markdown("---")
            
            # üîç ANALYSE AUTOMATIQUE DE LA STRUCTURE DU FICHIER
            st.subheader("üîç Analyse Automatique de Structure - Fichier .DAT")
            
            with st.expander("üìã Rapport de Structure D√©tect√©", expanded=True):
                try:
                    # Analyser structure du fichier avec MultiFreqERTParser
                    from multi_freq_ert_parser import MultiFreqERTParser
                    file_path_str = str(file_path)
                    
                    parser = MultiFreqERTParser()
                    
                    # Tester si c'est un fichier multi-fr√©quences
                    if parser.detect_format(file_path_str) is not None:
                        st.info("üî¨ **Fichier ERT Multi-Fr√©quences d√©tect√© !**")
                        
                        # Parser le fichier
                        df = parser.parse_file(file_path_str)
                        
                        if not df.empty:
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                st.metric("Type de fichier", "ERT Multi-Freq")
                                st.metric("Mesures", len(df))
                            
                            with col2:
                                st.metric("Fr√©quences", len(parser.frequencies) if parser.frequencies else "N/A")
                                st.metric("Survey Points", len(parser.survey_points) if parser.survey_points else "N/A")
                            
                            with col3:
                                freq_range = f"{min(parser.frequencies):.0f}-{max(parser.frequencies):.0f}" if parser.frequencies else "N/A"
                                st.metric("Plage MHz", freq_range)
                                depth_range = f"{parser.metadata['depth_range'][0]:.1f} √† {parser.metadata['depth_range'][1]:.1f}m" if 'depth_range' in parser.metadata else "N/A"
                                st.metric("Profondeur", depth_range)
                            
                            # Structure
                            st.markdown("#### üìä Structure des Donn√©es")
                            st.dataframe(df[['project', 'survey_point', 'depth', 'frequency_MHz', 'resistivity']].head(10))
                            
                            # Sauvegarder en session
                            st.session_state['multi_freq_data'] = df
                            st.session_state['multi_freq_parser'] = parser
                        else:
                            st.warning("‚ö†Ô∏è Fichier d√©tect√© mais aucune donn√©e pars√©e")
                    else:
                        # Format non reconnu - affichage basique
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("Type de fichier", "DAT (format inconnu)")
                            st.metric("Confiance", "N/A")
                        
                        with col2:
                            st.metric("Encodage", "Auto-d√©tect√©")
                            st.metric("Stations", "N/A")
                        
                        with col3:
                            st.metric("Colonnes", "N/A")
                            st.metric("Mesures", "N/A")
                
                except Exception as e:
                    st.error(f"‚ùå Erreur lors de l'analyse de structure: {e}")
                    import traceback
                    st.code(traceback.format_exc())
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # D√âTECTION FORMAT SURVEY-POINT / DEPTH / DATA
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            st.markdown("---")
            
            try:
                # survey_parser = SurveyDepthDataParser()
                # 
                # if survey_parser.detect_format(file_path_str):
                    st.success("üéØ **Format Survey-Point / Depth / Data d√©tect√© !**")
                    
                    with st.expander("üìä ANALYSE PROFIL VERTICAL - Survey Points", expanded=True):
                        with st.spinner("Analyse du profil survey-point/depth/data..."):
                            # Charger donn√©es
                            df_survey = survey_parser.load_data(file_path_str)
                            
                            # Afficher statistiques
                            st.markdown("##### üìà Structure D√©tect√©e")
                            col_s1, col_s2, col_s3, col_s4 = st.columns(4)
                            
                            with col_s1:
                                st.metric("üìç Survey Points", survey_parser.structure['num_survey_points'])
                            with col_s2:
                                st.metric("üìè Points Total", survey_parser.structure['num_points'])
                            with col_s3:
                                depth_range = survey_parser.structure['depth_range']
                                st.metric("üåä Profondeur Max", f"{abs(depth_range[1]):.1f} m")
                            with col_s4:
                                data_range = survey_parser.structure['data_range']
                                st.metric("üìä Plage Valeurs", f"{data_range[0]:.3f} - {data_range[1]:.3f}")
                            
                            # Rapport statistique
                            st.markdown("##### üìã Rapport Statistique Complet")
                            report = survey_parser.generate_statistics_report()
                            st.text(report)
                            
                            # Bouton t√©l√©charger rapport
                            st.download_button(
                                "üì• T√©l√©charger Rapport Survey (.txt)",
                                report,
                                f"survey_analysis_{uploaded_file.name}.txt",
                                "text/plain"
                            )
                            
                            # Aper√ßu donn√©es
                            st.markdown("##### üî¢ Aper√ßu Donn√©es Survey")
                            st.dataframe(df_survey.head(30), use_container_width=True)
                            
                            # VISUALISATIONS AUTOMATIQUES
                            st.markdown("---")
                            st.markdown("### üìä Visualisations Automatiques")
                            
                            viz_types = st.multiselect(
                                "S√©lectionnez les types de visualisation:",
                                ["Coupe 2D Interpol√©e", "Volume 3D", "Profils Verticaux", "Carte de Contours"],
                                default=["Coupe 2D Interpol√©e", "Profils Verticaux"]
                            )
                            
                            if st.button("üöÄ G√âN√âRER VISUALISATIONS", type="primary", use_container_width=True):
                                with st.spinner("G√©n√©ration des visualisations..."):
                                    
                                    if "Coupe 2D Interpol√©e" in viz_types:
                                        st.markdown("#### üó∫Ô∏è Coupe 2D - Profil de R√©sistivit√© Interpol√©")
                                        
                                        interp_method = st.selectbox(
                                            "M√©thode d'interpolation:",
                                            ["cubic", "linear", "nearest"],
                                            index=0
                                        )
                                        
                                        fig_2d, info_2d = survey_parser.create_2d_section(
                                            interpolation_method=interp_method,
                                            resolution=100,
                                            title="Coupe 2D - Profil de R√©sistivit√©"
                                        )
                                        
                                        st.plotly_chart(fig_2d, use_container_width=True)
                                        
                                        st.info(f"‚úÖ Interpolation {interp_method} | {info_2d['num_points']} points mesur√©s | R√©solution {info_2d['resolution']}x{info_2d['resolution']}")
                                    
                                    if "Volume 3D" in viz_types:
                                        st.markdown("#### üßä Volume 3D - Visualisation Interactive")
                                        
                                        fig_3d, info_3d = survey_parser.create_3d_volume()
                                        st.plotly_chart(fig_3d, use_container_width=True)
                                        
                                        st.info("‚úÖ Volume 3D g√©n√©r√© | Rotation interactive activ√©e")
                                    
                                    if "Profils Verticaux" in viz_types:
                                        st.markdown("#### üìâ Profils Verticaux par Survey Point")
                                        
                                        fig_prof, info_prof = survey_parser.create_vertical_profiles()
                                        st.plotly_chart(fig_prof, use_container_width=True)
                                        
                                        st.info(f"‚úÖ {info_prof['num_profiles']} profils verticaux g√©n√©r√©s")
                                    
                                    if "Carte de Contours" in viz_types:
                                        st.markdown("#### üó∫Ô∏è Carte de Contours - Isolignes")
                                        
                                        num_levels = st.slider("Nombre de niveaux:", 5, 30, 15)
                                        
                                        fig_cont, info_cont = survey_parser.create_contour_map(num_levels=num_levels)
                                        st.plotly_chart(fig_cont, use_container_width=True)
                                        
                                        st.info(f"‚úÖ Carte de contours avec {num_levels} niveaux")
                                    
                                    st.success("‚úÖ Toutes les visualisations g√©n√©r√©es avec succ√®s !")
                            
                            # Sauvegarder dans session state
                            st.session_state['survey_data'] = df_survey
                            st.session_state['survey_parser'] = survey_parser
            
            except Exception as e:
                # Pas de format survey d√©tect√©, continuer normalement
                pass
            
            st.markdown("---")
            st.subheader("üß† Analyse Intelligente Kibali - Donn√©es ERT")
            st.info("Kibali analyse les donn√©es avec son intelligence g√©ophysique pour d√©tecter incoh√©rences et rendre les donn√©es coh√©rentes")
            
            # D√©tecter si donn√©es ERT (r√©sistivit√©s)
            is_ert_data = any(10 <= val <= 10000 for val in numbers[:50])  # Plage typique r√©sistivit√©
            
            if is_ert_data:
                with st.expander("‚öôÔ∏è Configuration Contexte G√©ologique", expanded=False):
                    context_choice = st.selectbox(
                        "Zone g√©ographique / contexte:",
                        ["gabon", "sahel", "automatique"],
                        index=0,
                        help="Kibali adapte son analyse selon le contexte g√©ologique"
                    )
                
                if st.button("üöÄ LANCER ANALYSE INTELLIGENTE KIBALI", type="primary", use_container_width=True):
                    with st.spinner("üß† Kibali analyse les donn√©es avec son intelligence..."):
                        # Cr√©er profondeurs synth√©tiques si pas disponibles
                        n_points = len(numbers)
                        depths = list(np.linspace(0, n_points * 0.5, n_points))  # 0.5m spacing
                        resistivities = numbers
                        
                        # Analyse intelligente
                        kibali_results = kibali_analyze_ert(depths, resistivities, context=context_choice if context_choice != "automatique" else "gabon")
                        
                        # Afficher synth√®se intelligente
                        st.markdown("### üìã Synth√®se Intelligente Kibali")
                        st.markdown(kibali_results["synthese_intelligente"])
                        
                        # Onglets d√©taill√©s
                        tab1, tab2, tab3, tab4 = st.tabs(["‚úÖ Validation", "üîß Corrections", "ü™® Couches", "üíß Hydrog√©ologie"])
                        
                        with tab1:
                            validation = kibali_results["validation_stratigraphique"]
                            
                            col_v1, col_v2 = st.columns(2)
                            with col_v1:
                                if validation["valid"]:
                                    st.success(f"‚úÖ Donn√©es COH√âRENTES")
                                else:
                                    st.warning(f"‚ö†Ô∏è Anomalies d√©tect√©es")
                            with col_v2:
                                st.metric("Score Coh√©rence", f"{validation['score_coherence']}/100")
                            
                            if validation['anomalies']:
                                st.markdown("#### ‚ùå Anomalies Critiques")
                                for anom in validation['anomalies']:
                                    st.error(f"‚Ä¢ {anom['message']}")
                            
                            if validation['warnings']:
                                st.markdown("#### ‚ö†Ô∏è Avertissements")
                                for warn in validation['warnings']:
                                    st.warning(f"‚Ä¢ {warn['message']}")
                        
                        with tab2:
                            corrections = kibali_results["corrections_appliquees"]
                            if corrections:
                                st.info(f"üîß Kibali a appliqu√© {len(corrections)} corrections intelligentes")
                                corr_df = pd.DataFrame(corrections)
                                st.dataframe(corr_df, use_container_width=True)
                                
                                # Bouton t√©l√©charger donn√©es corrig√©es
                                corrected_data = kibali_results["donnees_corrigees"]
                                corrected_csv = pd.DataFrame({
                                    "Profondeur (m)": corrected_data["profondeurs"],
                                    "R√©sistivit√© Originale (Œ©.m)": corrected_data["resistivites_originales"],
                                    "R√©sistivit√© Corrig√©e (Œ©.m)": corrected_data["resistivites_corrigees"]
                                })
                                st.download_button(
                                    "üì• T√©l√©charger Donn√©es Corrig√©es (CSV)",
                                    corrected_csv.to_csv(index=False).encode('utf-8'),
                                    f"{uploaded_file.name}_kibali_corrected.csv",
                                    "text/csv"
                                )
                            else:
                                st.success("‚úÖ Aucune correction n√©cessaire - Donn√©es d√©j√† coh√©rentes")
                        
                        with tab3:
                            layers = kibali_results["couches_geologiques"]
                            st.markdown(f"#### ü™® {len(layers)} Couches G√©ologiques Identifi√©es")
                            
                            for i, layer in enumerate(layers, 1):
                                with st.expander(f"Couche {i}: {layer['profondeur_debut']:.1f}-{layer['profondeur_fin']:.1f}m ({layer['epaisseur']:.1f}m)"):
                                    col_l1, col_l2 = st.columns(2)
                                    with col_l1:
                                        st.metric("Type", layer['type_geologique'])
                                        st.metric("Profondeur", f"{layer['profondeur_debut']:.1f} - {layer['profondeur_fin']:.1f} m")
                                    with col_l2:
                                        st.metric("√âpaisseur", f"{layer['epaisseur']:.1f} m")
                                        st.metric("R√©sistivit√©", f"{layer['resistivite_moyenne']:.1f} Œ©.m")
                                    
                                    st.info(layer['description'])
                        
                        with tab4:
                            hydro = kibali_results["analyse_hydrogeologique"]
                            
                            col_h1, col_h2, col_h3 = st.columns(3)
                            with col_h1:
                                potential_color = {"faible": "üî¥", "moyen": "üü°", "bon": "üü¢", "excellent": "üü¢"}
                                st.metric("Potentiel Hydrique", f"{potential_color.get(hydro['potentiel_hydrique'], '‚ö™')} {hydro['potentiel_hydrique'].upper()}")
                            with col_h2:
                                if hydro['profondeur_nappe_estimee']:
                                    st.metric("Nappe Phr√©atique", f"{hydro['profondeur_nappe_estimee']:.1f} m")
                                else:
                                    st.metric("Nappe Phr√©atique", "Profonde")
                            with col_h3:
                                st.metric("Zones Aquif√®res", len(hydro['couches_aquiferes']))
                            
                            if hydro['recommandations']:
                                st.markdown("#### üìã Recommandations Kibali")
                                for rec in hydro['recommandations']:
                                    st.success(f"‚úì {rec}")
                        
                        # Sauvegarder r√©sultats dans session state
                        st.session_state['kibali_ert_analysis'] = kibali_results
                        st.success("‚úÖ Analyse intelligente Kibali termin√©e!")
            else:
                st.warning("‚ö†Ô∏è Donn√©es ne semblent pas √™tre de type ERT (r√©sistivit√©). Analyse intelligente non applicable.")
        
        st.markdown("---")
        st.subheader("üéØ Clustering KMeans")
        n_clusters = st.slider("Nombre de clusters", 2, 10, 3)
        labels, centers = cluster_numbers(numbers, n_clusters=n_clusters)
        df['Cluster'] = labels
        st.dataframe(df)
        st.subheader("üìà Visualisation des clusters")
        fig, ax = plt.subplots()
        for i in range(n_clusters):
            cluster_vals = df[df['Cluster']==i]['Value']
            ax.scatter([i]*len(cluster_vals), cluster_vals, label=f"Cluster {i}")
        ax.set_xlabel("Cluster")
        ax.set_ylabel("Valeurs")
        ax.legend()
        st.pyplot(fig)
        st.subheader("üíæ Export CSV")
        csv_bytes = df.to_csv(index=False).encode('utf-8')
        st.download_button("T√©l√©charger CSV", csv_bytes, file_name="binary_structured.csv")
    else:
        st.warning("Aucun nombre d√©tect√© dans ce fichier binaire.")
    
    # üîç FOUILLE INTELLIGENTE AUTOMATIQUE
    st.subheader("üîç Fouille Intelligente Multi-Sources")
    st.info("Combine: Hex+ASCII Dump + Base Vectorielle RAG + Base ERT + Web Search + Synth√®se IA")
    
    col_inv1, col_inv2 = st.columns(2)
    with col_inv1:
        if st.button("üî¨ LANCER INVESTIGATION COMPL√àTE", type="primary", use_container_width=True):
            with st.spinner("üîç Investigation en cours (7 phases)..."):
                investigation_result = deep_binary_investigation(file_bytes, uploaded_file.name)
                st.session_state.last_investigation = investigation_result
                st.success("‚úÖ Investigation termin√©e!")
    
    with col_inv2:
        if "last_investigation" in st.session_state:
            st.download_button(
                "üì• T√©l√©charger Rapport",
                st.session_state.last_investigation.get('full_report', ''),
                file_name=f"investigation_{uploaded_file.name}.txt",
                mime="text/plain",
                use_container_width=True
            )
    
    # Afficher le dernier rapport d'investigation en phases expandables
    if "last_investigation" in st.session_state:
        st.markdown("### üìã Rapport d'Investigation Complet")
        
        result = st.session_state.last_investigation
        phases = result.get('phases', {})
        
        # Add summary statistics for expander titles
        phase_summaries = {
            '1Ô∏è‚É£ PHASE 1: EXTRACTION HEX + ASCII': 'üìú Dump hexad√©cimal et extraction de nombres',
            '2Ô∏è‚É£ PHASE 2: ANALYSES TECHNIQUES': 'üìä Entropie, patterns, m√©tadonn√©es',
            '3Ô∏è‚É£ PHASE 3: FOUILLE BASE VECTORIELLE RAG': 'üîç Recherche dans la base de connaissances',
            '4Ô∏è‚É£ PHASE 4: FOUILLE SP√âCIALIS√âE ERT': 'üî¨ Analyse ERT, min√©raux, correspondances',
            '5Ô∏è‚É£ PHASE 5: RECHERCHE WEB INTELLIGENTE': 'üåê Recherche internet contextuelle',
            '6Ô∏è‚É£ PHASE 6: SYNTH√àSE MULTI-SOURCES': 'üéØ Consolidation des r√©sultats',
            '7Ô∏è‚É£ PHASE 7: RECOMMANDATIONS': 'üí° Actions sugg√©r√©es'
        }
        
        # Display each phase in its own expander
        for i, (phase_title, phase_content) in enumerate(phases.items()):
            # Extract phase number emoji for matching
            phase_key = phase_title.split('\n')[0] if '\n' in phase_title else phase_title
            summary = phase_summaries.get(phase_key, '')
            
            # Count lines for height estimation
            num_lines = len(phase_content.split('\n'))
            estimated_height = min(500, max(200, num_lines * 15))
            
            # Only first expander open by default
            with st.expander(f"{phase_key} - {summary}", expanded=(i==0)):
                st.text_area(
                    label="Contenu de la phase",
                    value=phase_content,
                    height=estimated_height,
                    key=f"phase_{i}",
                    label_visibility="collapsed"
                )
    
    # Analyse automatique du fichier d√®s l'upload
    if st.button("üöÄ Analyser automatiquement avec IA (GPU optimis√©)"):
        with st.spinner(f"üöÄ Analyse IA en cours sur {device.upper()}... {'(GPU acc√©l√©r√©)' if device == 'cuda' else '(CPU)'}"):
            # V√©rifier que le mod√®le utilise bien le GPU si disponible
            if device == 'cuda' and model.device.type != 'cuda':
                st.warning("üîß Migration du mod√®le vers GPU...")
                model = model.to('cuda')
                st.success(f"‚úÖ Mod√®le migr√© vers GPU - {gpu_info}")
           
            # Afficher les informations d'optimisation
            st.info(f"üñ•Ô∏è Device: {device.upper()} | Mod√®le: {model.device} | Precision: {model.dtype}")
           
            # Analyse optimis√©e avec GPU
            analysis_result = analyze_with_ai(
                f"Analyse compl√®te et d√©taill√©e de ce fichier binaire. Identifie le type de fichier, son contenu, et fournis une interpr√©tation experte g√©ophysique ERT si applicable. Utilise tous les outils disponibles pour une analyse maximale.",
                file_bytes, numbers, hex_dump, n_clusters,
                st.session_state.get('model'), st.session_state.get('tokenizer'), st.session_state.get('device')
            )
            st.subheader("üß† Analyse IA Automatique (GPU Optimis√©e)")
            st.markdown(analysis_result)
    elif not st.session_state.vectorstore:
        st.info("Veuillez d'abord uploader et indexer des PDFs pour activer l'analyse LLM.")
# Section Chat en Temps R√©el
st.subheader("üí¨ Chat d'Analyse en Temps R√©el")
# Configuration GPU pour le chat
col1, col2, col3 = st.columns([3, 2, 2])
with col1:
    if "gpu_mode_chat" not in st.session_state:
        st.session_state.gpu_mode_chat = torch.cuda.is_available()
with col2:
    gpu_mode_chat = st.checkbox(
        "üöÄ Mode GPU",
        value=st.session_state.gpu_mode_chat,
        help="Active l'acc√©l√©ration GPU pour le chat (plus rapide)",
        key="gpu_chat_toggle"
    )
    st.session_state.gpu_mode_chat = gpu_mode_chat
with col3:
    # Affichage du statut GPU
    if gpu_mode_chat and torch.cuda.is_available():
        st.success("‚úÖ GPU activ√©")
        gpu_info_chat = f"{torch.cuda.get_device_name(0)}"
        memory_used = torch.cuda.memory_allocated() / 1024**3
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        st.caption(f"üî• {memory_used:.1f}/{memory_total:.1f}GB")
    elif gpu_mode_chat and not torch.cuda.is_available():
        st.warning("‚ö†Ô∏è GPU indisponible")
        st.caption("üíª Utilisation CPU")
    else:
        st.info("üíª Mode CPU")
        st.caption("üêå Performance standard")
if "messages" not in st.session_state:
    st.session_state.messages = []
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
if prompt := st.chat_input("Posez votre question d'analyse..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    with st.chat_message("assistant"):
        # Affichage du mode de traitement
        mode_display = "üöÄ GPU" if st.session_state.gpu_mode_chat and torch.cuda.is_available() else "üíª CPU"
        spinner_text = f"{mode_display} Agent LangChain r√©fl√©chit..."
       
        # Migration du mod√®le si mode GPU activ√©
        if st.session_state.gpu_mode_chat and torch.cuda.is_available() and model.device.type != 'cuda':
            with st.spinner("üîÑ Migration vers GPU..."):
                model.to('cuda')
                st.success("‚úÖ Mod√®le migr√© vers GPU")
        elif not st.session_state.gpu_mode_chat and model.device.type == 'cuda':
            with st.spinner("üîÑ Migration vers CPU..."):
                model.to('cpu')
                st.success("‚úÖ Mod√®le migr√© vers CPU")
       
        with st.spinner(spinner_text):
            # Utiliser l'agent LangChain pour le chat avec optimisation GPU/CPU
            chat_prompt = f"""
Tu es un assistant expert en analyse de fichiers binaires. L'utilisateur pose une question d'analyse.
Question: {prompt}
Utilise les outils disponibles pour:
1. Rechercher dans la base de connaissances PDF si disponible
2. Effectuer des recherches web pour des informations compl√©mentaires
3. Analyser des patterns si des donn√©es binaires sont mentionn√©es
4. Si ERT/r√©sistivit√©: reproduire couleurs, comparer avec liquides/min√©raux via recherches internet, d√©crire nature/profondeur/couleur
5. Pour fichiers .dat ERT, utilise mathematical_calculator avec formules FicheERT.pdf: Schlumberger: pi*(L**2 - l**2)/(2*l) * V/I, Wenner: 2*pi*a * V/I, Dipole-Dipole: pi*n*(n+1)*(n+2)*a * V/I
R√©ponds de mani√®re pr√©cise et utile.
PERFORMANCE: Mode {mode_display} activ√© pour traitement optimis√©.
"""
            try:
                # Analyse avanc√©e avec outils pour chat
                enhanced_response = ""
               
                # D√©tecter le type de demande
                is_visualization_request = any(keyword in prompt.lower() for keyword in [
                    "coupe", "graphique", "visualisation", "visualise", "g√©n√®re", "g√©nerer", 
                    "graphe", "plot", "diagramme", "carte", "profil", "section", "image",
                    "montre", "affiche", "cr√©e", "dessine", "couleur", "couleurs"
                ])
                
                is_analysis_request = any(keyword in prompt.lower() for keyword in [
                    "recherche", "approfondie", "analyse", "donn√©es", "r√©sistivit√©"
                ])
                
                # PRIORIT√â 1: G√©n√©ration de visualisation graphique
                if is_visualization_request and st.session_state.current_file_data:
                    try:
                        st.info("üé® **G√©n√©ration de visualisation en cours...**")
                        
                        # Initialiser l'agent graphique si n√©cessaire
                        if st.session_state.graph_agent is None:
                            with st.spinner("üîß Chargement du moteur de visualisation avanc√©..."):
                                st.session_state.graph_agent = GraphGenerationAgent()
                        
                        # Initialiser le moteur avanc√© si n√©cessaire
                        if st.session_state.advanced_viz_engine is None:
                            with st.spinner("üöÄ Initialisation du moteur PyGIMLI + OpenCV..."):
                                st.session_state.advanced_viz_engine = AdvancedVisualizationEngine()
                        
                        # Extraire les donn√©es du fichier actuel
                        file_data = st.session_state.current_file_data
                        data_array = np.array(file_data)
                        
                        # G√©n√©rer la visualisation avec le moteur avanc√©
                        st.success("‚ú® **G√©n√©ration de la coupe avec PyGIMLI + Matplotlib...**")
                        
                        viz_result = st.session_state.advanced_viz_engine.create_complete_ert_section(
                            data=data_array,
                            title=f"Coupe ERT - {st.session_state.get('current_filename', 'Donn√©es')}"
                        )
                        
                        if viz_result and 'figure' in viz_result:
                            st.pyplot(viz_result['figure'])
                            st.success("‚úÖ **Visualisation g√©n√©r√©e avec succ√®s !**")
                            
                            # G√©n√©rer une explication intelligente avec l'agent
                            explanation = st.session_state.graph_agent.generate_explanation(
                                graph_type="2d_section",
                                data_summary={
                                    'min': float(np.min(data_array)),
                                    'max': float(np.max(data_array)),
                                    'mean': float(np.mean(data_array)),
                                    'points': len(data_array)
                                }
                            )
                            
                            st.markdown("### üìä Analyse de la coupe")
                            st.markdown(explanation)
                            
                            # Sauvegarder dans l'historique
                            assistant_response = f"""‚úÖ **Coupe de r√©sistivit√© g√©n√©r√©e !**

{explanation}

**Caract√©ristiques de la visualisation:**
- Type: Coupe 2D avec interpolation
- Donn√©es: {len(data_array)} points
- Plage: {np.min(data_array):.2f} - {np.max(data_array):.2f} Œ©.m
- Moteur: PyGIMLI + Matplotlib + OpenCV
"""
                            st.session_state.messages.append({"role": "assistant", "content": assistant_response})
                            st.stop()  # Arr√™ter ici, visualisation compl√®te
                            
                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de la g√©n√©ration graphique: {e}")
                        import traceback
                        st.code(traceback.format_exc())
                
                # PRIORIT√â 2: Analyse avec recherche
                if is_analysis_request:
                    # Effectuer recherche web
                    try:
                        web_results = web_search_enhanced(prompt + " ERT geophysics electrical resistivity")
                        enhanced_response += f"üåê RECHERCHE WEB EFFECTU√âE:\n{web_results}\n\n"
                    except:
                        pass
                   
                    # Recherche RAG
                    if st.session_state.vectorstore:
                        try:
                            rag_results = search_vectorstore(prompt)
                            enhanced_response += f"üìö BASE DE CONNAISSANCES:\n{rag_results}\n\n"
                        except:
                            pass
                   
                    # Analyse ERT compl√®te si pertinent
                    if any(keyword in prompt.lower() for keyword in ["ert", "r√©sistivit√©", "mat√©riaux", "analyse", "donn√©es"]):
                        try:
                            # G√©n√©ration du rapport complet avec outils
                            complete_report = create_advanced_analysis_report(prompt)
                            enhanced_response += f"üìä RAPPORT D'ANALYSE COMPLET:\n{complete_report}\n\n"
                           
                            # Donn√©es exemple pour d√©monstration visuelle
                            sample_data = [0.05, 0.3, 2.0, 10.0, 50.0, 200.0, 1000.0, 5000.0]
                            ert_analysis = resistivity_color_analysis(sample_data)
                            enhanced_response += f"üé® ANALYSE VISUELLE ERT:\n{ert_analysis}\n\n"
                        except Exception as e:
                            enhanced_response += f"‚ö†Ô∏è Analyse ERT partielle: {e}\n\n"
               
                # Utiliser directement le mod√®le Qwen pour le chat avec contexte enrichi
                system_content = f"""Tu es un expert mondial en g√©ophysique ERT avec acc√®s complet √† tous les outils d'analyse.
               
                CONTEXTE ENRICHI AVEC OUTILS EX√âCUT√âS:
                {enhanced_response}
                INSTRUCTIONS STRICTES:
                1. Utilise OBLIGATOIREMENT les donn√©es ci-dessus pour r√©pondre
                2. Pr√©sente les tableaux HTML et graphiques inclus
                3. Cite les r√©sultats de recherche web obtenus
                4. Fournis des analyses quantitatives pr√©cises
                5. Compare avec les mat√©riaux identifi√©s automatiquement
                6. Explique les couleurs de visualisation ERT
                7. Donne des recommandations techniques concr√®tes
               
                R√âPONSE ATTENDUE:
                - Structure professionnelle avec sections claires
                - Donn√©es num√©riques pr√©cises issues des analyses
                - R√©f√©rences aux sources trouv√©es
                - Visualisations d√©crites et expliqu√©es
                - Conclusions bas√©es sur les outils utilis√©s
               
                INTERDICTIONS:
                - Ne JAMAIS dire "je n'ai pas acc√®s"
                - Ne pas inventer de donn√©es - utiliser celles fournies
                - Ne pas √™tre g√©n√©rique - √™tre sp√©cifique aux r√©sultats obtenus"""
               
                chat_messages = [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": prompt}
                ]
               
                inputs = tokenizer.apply_chat_template(
                    chat_messages,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(model.device)
                # Create attention mask to avoid warnings when pad_token == eos_token
                attention_mask = (inputs != tokenizer.pad_token_id).long().to(model.device)
               
                # G√©n√©ration optimis√©e selon le mode GPU/CPU
                start_time = time.time()
                with torch.no_grad():
                    if st.session_state.gpu_mode_chat and torch.cuda.is_available() and model.device.type == 'cuda':
                        # Mode GPU optimis√© avec mixed precision
                        with torch.cuda.amp.autocast():
                            outputs = model.generate(
                                inputs,
                                attention_mask=attention_mask,
                                max_new_tokens=3000,  # AUGMENT√â pour r√©ponses COMPL√àTES
                                temperature=0.6,
                                do_sample=True,
                                top_p=0.9,
                                pad_token_id=tokenizer.eos_token_id,
                                use_cache=True,
                                num_beams=1
                            )
                    else:
                        # Mode CPU standard
                        outputs = model.generate(
                            inputs,
                            attention_mask=attention_mask,
                            max_new_tokens=3000,  # AUGMENT√â pour r√©ponses COMPL√àTES
                            temperature=0.6,
                            do_sample=True,
                            top_p=0.9,
                            pad_token_id=tokenizer.eos_token_id
                        )
               
                generation_time = time.time() - start_time
                assistant_response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
               
                # Ajouter informations de performance
                device_used = model.device.type.upper()
                performance_info = f"\n\n---\n**‚ö° Performance:** {device_used} | **‚è±Ô∏è Temps:** {generation_time:.2f}s"
               
                if model.device.type == 'cuda':
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    performance_info += f" | **üíæ VRAM:** {memory_used:.1f}/{memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)"
               
                assistant_response_with_perf = assistant_response + performance_info
               
            except Exception as e:
                # Fallback vers le syst√®me classique
                st.warning(f"Chat IA a √©chou√©: {e}. Utilisation du syst√®me classique...")
                fallback_start_time = time.time()
               
                # Recherche web
                tool = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=5)
                web_results = tool.invoke(prompt)
                web_context = "\n".join([r["content"] for r in web_results])
                context = f"Contexte web:\n{web_context}"
                # Contexte documents si disponible - RECHERCHE GLOBALE ILLIMIT√âE
                if st.session_state.vectorstore:
                    # R√©cup√©rer BEAUCOUP de documents pour une couverture GLOBALE COMPL√àTE
                    total_docs = st.session_state.vectorstore.index.ntotal if hasattr(st.session_state.vectorstore, 'index') else 1000
                    search_k = min(100, total_docs) if total_docs > 0 else 100  # 100+ documents au lieu de 30
                    
                    retriever = st.session_state.vectorstore.as_retriever(
                        search_type="similarity",
                        search_kwargs={
                            "k": search_k,  # Recherche ILLIMIT√âE profonde sur 100+ documents
                            "fetch_k": min(search_k * 3, total_docs)
                        }
                    )
                    docs = retriever.get_relevant_documents(prompt)
                    
                    # Grouper par source pour meilleure vue globale
                    sources = {}
                    for doc in docs:
                        source = doc.metadata.get('source', 'Unknown')
                        if source not in sources:
                            sources[source] = []
                        sources[source].append(doc.page_content[:400])
                    
                    doc_context = f"\nüìä FOUILLE GLOBALE: {len(docs)} passages trouv√©s dans {len(sources)} sources ({search_k}/{total_docs} docs analys√©s):\n"
                    for source, chunks in sources.items():
                        doc_context += f"\nüìÑ {source}: {len(chunks)} passages\n" + "\n".join(chunks[:3])
                    
                    context += f"\n\nContexte documents index√©s (recherche globale illimit√©e):\n{doc_context}"
                full_prompt = f"""Tu es un assistant expert en analyse de donn√©es et fichiers binaires. Utilise le contexte fourni pour donner des r√©ponses pr√©cises et utiles.
{context}
Question de l'utilisateur: {prompt}
R√©ponse d√©taill√©e:"""
                messages = [
                    {"role": "system", "content": "Tu es un assistant expert en analyse de fichiers binaires et mod√®les ML."},
                    {"role": "user", "content": full_prompt}
                ]
                inputs = tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(model.device)
                # Create attention mask to avoid warnings when pad_token == eos_token
                attention_mask = (inputs != tokenizer.pad_token_id).long().to(model.device)
               
                with torch.no_grad():
                    if st.session_state.gpu_mode_chat and torch.cuda.is_available() and model.device.type == 'cuda':
                        with torch.cuda.amp.autocast():
                            outputs = model.generate(
                                inputs,
                                attention_mask=attention_mask,
                                max_new_tokens=3000,  # AUGMENT√â pour r√©ponses COMPL√àTES
                                temperature=0.7,
                                do_sample=True,
                                top_p=0.9,
                                pad_token_id=tokenizer.eos_token_id,
                                use_cache=True,
                                num_beams=1
                            )
                    else:
                        outputs = model.generate(
                            inputs,
                            attention_mask=attention_mask,
                            max_new_tokens=3000,  # AUGMENT√â pour r√©ponses COMPL√àTES
                            temperature=0.7,
                            do_sample=True,
                            top_p=0.9,
                            pad_token_id=tokenizer.eos_token_id
                        )
               
                fallback_time = time.time() - fallback_start_time
                assistant_response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
               
                # Ajouter informations de performance pour fallback
                device_used = model.device.type.upper()
                performance_info = f"\n\n---\n**‚ö° Performance (Fallback):** {device_used} | **‚è±Ô∏è Temps:** {fallback_time:.2f}s"
               
                if model.device.type == 'cuda':
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    performance_info += f" | **üíæ VRAM:** {memory_used:.1f}/{memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)"
               
                assistant_response_with_perf = assistant_response + performance_info
           
            # Afficher la r√©ponse avec les informations de performance
            st.markdown(assistant_response_with_perf)
            st.session_state.messages.append({"role": "assistant", "content": assistant_response_with_perf})
def generate_resistivity_table(resistivity_values: list) -> str:
    """G√©n√®re un tableau HTML des valeurs de r√©sistivit√©"""
    if not resistivity_values:
        return "Aucune donn√©e pour g√©n√©rer le tableau"
   
    import numpy as np
    from resistivity_color_mapper import ResistivityColorMapper
   
    mapper = ResistivityColorMapper()
    arr = np.array(resistivity_values)
   
    # Cr√©er le tableau HTML
    table_html = """
    <div style='overflow-x: auto;'>
    <table style='border-collapse: collapse; width: 100%; font-family: Arial, sans-serif;'>
    <thead>
        <tr style='background-color: #2E86AB; color: white;'>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>Index</th>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>R√©sistivit√© (Œ©¬∑m)</th>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>Couleur</th>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>Classification</th>
            <th style='border: 1px solid #ddd; padding: 12px; text-align: center;'>Mat√©riau Probable</th>
        </tr>
    </thead>
    <tbody>
    """
   
    for i, rho in enumerate(arr[:20]): # Limiter √† 20 pour l'affichage
        color, desc = mapper.get_color_for_resistivity(rho)
       
        # Classification
        if rho < 10:
            classification = "Conducteur"
            material = "Argile, eau sal√©e"
        elif rho < 100:
            classification = "Semi-conducteur"
            material = "Sol humide, sable"
        elif rho < 1000:
            classification = "R√©sistant"
            material = "Calcaire, gr√®s"
        else:
            classification = "Tr√®s r√©sistant"
            material = "Granite, air"
       
        # Ligne du tableau avec couleur de fond
        bg_color = color if color != '#FFFFFF' else '#F0F0F0'
        text_color = 'white' if color in ['#000080', '#0000FF', '#FF0000'] else 'black'
       
        table_html += f"""
        <tr>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{i+1}</td>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center; font-weight: bold;'>{rho:.3f}</td>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center; background-color: {bg_color}; color: {text_color};'>{color}</td>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{classification}</td>
            <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{material}</td>
        </tr>
        """
   
    table_html += """
    </tbody>
    </table>
    </div>
   
    <div style='margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-radius: 5px;'>
    <h4>üìä Statistiques R√©sum√©es:</h4>
    <ul>
        <li><strong>Nombre de valeurs:</strong> {count}</li>
        <li><strong>R√©sistivit√© moyenne:</strong> {mean:.3f} Œ©¬∑m</li>
        <li><strong>M√©diane:</strong> {median:.3f} Œ©¬∑m</li>
        <li><strong>√âcart-type:</strong> {std:.3f} Œ©¬∑m</li>
        <li><strong>Plage:</strong> {min:.3f} - {max:.3f} Œ©¬∑m</li>
        <li><strong>Ratio max/min:</strong> {ratio:.1f}</li>
    </ul>
    </div>
    """.format(
        count=len(arr),
        mean=np.mean(arr),
        median=np.median(arr),
        std=np.std(arr),
        min=np.min(arr),
        max=np.max(arr),
        ratio=np.max(arr)/np.min(arr) if np.min(arr) > 0 else float('inf')
    )
   
    return table_html
def create_advanced_analysis_report(query: str, resistivity_values: list = None) -> str:
    """Cr√©e un rapport d'analyse avanc√© complet"""
    if not resistivity_values:
        # Donn√©es exemple repr√©sentatives de diff√©rents mat√©riaux
        resistivity_values = [
            0.05, 0.2, 0.3, # Eau sal√©e/saumure
            2.0, 5.0, 8.0, # Argile
            15.0, 25.0, 35.0, # Sol humide
            80.0, 120.0, 180.0, # Sable
            300.0, 500.0, 800.0, # Calcaire
            2000.0, 3500.0, 5000.0, # Granite
            0.0000024, 0.0000026, # Or
            900000.0, 1100000.0 # Diamant
        ]
   
    report = f"""
    üî¨ RAPPORT D'ANALYSE G√âOPHYSIQUE COMPLET
    =========================================
   
    üìã CONTEXTE DE LA DEMANDE:
    {query}
   
    üéØ M√âTHODOLOGIE APPLIQU√âE:
    ‚úÖ Recherche web automatis√©e pour donn√©es actualis√©es
    ‚úÖ Analyse comparative avec base de donn√©es g√©ophysique
    ‚úÖ Validation contre r√©f√©rences scientifiques
    ‚úÖ G√©n√©ration de visualisations et tableaux
    ‚úÖ Calculs statistiques avanc√©s
   
    üìä DONN√âES ANALYS√âES:
    ‚Ä¢ Nombre d'√©chantillons: {len(resistivity_values)}
    ‚Ä¢ Plage de r√©sistivit√©: {min(resistivity_values):.2e} - {max(resistivity_values):.2e} Œ©¬∑m
    ‚Ä¢ Ordre de grandeur: {max(resistivity_values)/min(resistivity_values):.1e}
   
    üîç IDENTIFICATION AUTOMATIQUE DES MAT√âRIAUX:
    """
   
    # Analyse d√©taill√©e par mat√©riau
    import numpy as np
    from resistivity_color_mapper import ResistivityColorMapper, DynamicERTAnalyzer
   
    try:
        mapper = ResistivityColorMapper()
        analyzer = DynamicERTAnalyzer()
       
        # Classification automatique
        materials_detected = {}
        for rho in resistivity_values:
            materials = mapper.find_similar_materials(rho, tolerance=0.3)
            if materials:
                top_material = materials[0]
                mat_name = top_material['name']
                if mat_name not in materials_detected:
                    materials_detected[mat_name] = {
                        'values': [],
                        'category': top_material['category'],
                        'typical': top_material['typical_value'],
                        'nature': top_material['nature']
                    }
                materials_detected[mat_name]['values'].append(rho)
       
        # Rapport par mat√©riau d√©tect√©
        for i, (mat_name, mat_data) in enumerate(materials_detected.items(), 1):
            avg_rho = np.mean(mat_data['values'])
            count = len(mat_data['values'])
            report += f"""
    {i}. {mat_name.upper()} ({mat_data['category']})
       ‚Ä¢ Occurrences d√©tect√©es: {count}
       ‚Ä¢ R√©sistivit√© moyenne mesur√©e: {avg_rho:.2e} Œ©¬∑m
       ‚Ä¢ R√©sistivit√© typique th√©orique: {mat_data['typical']:.2e} Œ©¬∑m
       ‚Ä¢ Nature: {mat_data['nature']}
       ‚Ä¢ Concordance: {100 - abs(np.log10(avg_rho) - np.log10(mat_data['typical']))*20:.1f}%
            """
       
        # Recherche web automatique pour validation
        try:
            web_validation = web_search_enhanced(
                f"electrical resistivity values {query} geophysics materials validation",
                "validation"
            )
            report += f"""
   
    üåê VALIDATION PAR RECHERCHE WEB:
    {web_validation}
    """
        except:
            report += "\nüåê VALIDATION WEB: En cours..."
       
        # Calculs g√©ophysiques avanc√©s
        arr = np.array(resistivity_values)
        report += f"""
   
    üìä ANALYSES STATISTIQUES AVANC√âES:
   
    üî¢ Param√®tres de base:
    ‚Ä¢ Moyenne g√©om√©trique: {np.exp(np.mean(np.log(arr))):.2e} Œ©¬∑m
    ‚Ä¢ M√©diane: {np.median(arr):.2e} Œ©¬∑m
    ‚Ä¢ √âcart-type logarithmique: {np.std(np.log10(arr)):.3f}
    ‚Ä¢ Coefficient de variation: {np.std(arr)/np.mean(arr):.3f}
   
    üéØ Classification g√©ophysique:
    ‚Ä¢ Conducteurs (<10 Œ©¬∑m): {len(arr[arr < 10])} √©chantillons
    ‚Ä¢ Semi-conducteurs (10-100 Œ©¬∑m): {len(arr[(arr >= 10) & (arr < 100)])} √©chantillons
    ‚Ä¢ R√©sistants (100-1000 Œ©¬∑m): {len(arr[(arr >= 100) & (arr < 1000)])} √©chantillons
    ‚Ä¢ Tr√®s r√©sistants (>1000 Œ©¬∑m): {len(arr[arr >= 1000])} √©chantillons
   
    üå°Ô∏è Estimation de profondeur (mod√®le empirique):
    ‚Ä¢ Profondeur d'investigation: {np.mean(arr)*0.1:.1f} m (approximative)
    ‚Ä¢ R√©solution verticale: {np.std(arr)*0.05:.1f} m
        """
       
    except Exception as e:
        report += f"\n‚ùå Erreur dans l'analyse: {e}"
   
    report += """
   
    üí° RECOMMANDATIONS TECHNIQUES:
    ‚Ä¢ Utiliser inversion 2D/3D pour structures complexes
    ‚Ä¢ Valider par forages si possible
    ‚Ä¢ Consid√©rer variations saisonni√®res
    ‚Ä¢ Appliquer corrections topographiques si n√©cessaire
   
    üìö R√âF√âRENCES SCIENTIFIQUES:
    ‚Ä¢ Loke, M.H. (2001). Tutorial: 2-D and 3-D electrical imaging surveys
    ‚Ä¢ Telford et al. (1990). Applied Geophysics, Cambridge University Press
    ‚Ä¢ Reynolds, J.M. (2011). An Introduction to Applied and Environmental Geophysics
   
    ‚úÖ RAPPORT G√âN√âR√â AUTOMATIQUEMENT AVEC OUTILS AVANC√âS
    """
   
    return report
def generate_resistivity_plot(resistivity_values: list) -> str:
    """G√©n√®re un graphique des valeurs de r√©sistivit√©"""
    if not resistivity_values:
        return "Aucune donn√©e pour g√©n√©rer le graphique"
   
    import numpy as np
    import matplotlib.pyplot as plt
    import io
    import base64
    from resistivity_color_mapper import ResistivityColorMapper
   
    try:
        mapper = ResistivityColorMapper()
        arr = np.array(resistivity_values)
       
        # Cr√©er la figure avec subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Analyse Compl√®te des R√©sistivit√©s ERT', fontsize=16, fontweight='bold')
       
        # 1. Profil de r√©sistivit√© avec couleurs
        colors = []
        for rho in arr:
            color, _ = mapper.get_color_for_resistivity(rho)
            colors.append(color)
       
        scatter = ax1.scatter(range(len(arr)), arr, c=colors, s=60, edgecolors='black', linewidth=0.5)
        ax1.plot(range(len(arr)), arr, 'k-', alpha=0.3, linewidth=1)
        ax1.set_xlabel('Position de mesure')
        ax1.set_ylabel('R√©sistivit√© (Œ©¬∑m)')
        ax1.set_title('Profil de R√©sistivit√© avec Couleurs ERT')
        ax1.set_yscale('log')
        ax1.grid(True, alpha=0.3)
       
        # 2. Histogramme
        ax2.hist(np.log10(arr), bins=15, color='skyblue', edgecolor='black', alpha=0.7)
        ax2.set_xlabel('Log10(R√©sistivit√©)')
        ax2.set_ylabel('Fr√©quence')
        ax2.set_title('Distribution des R√©sistivit√©s')
        ax2.grid(True, alpha=0.3)
       
        # 3. Classification par zones
        zones = {'Conducteur (<10)': arr[arr < 10],
                'Semi-conducteur (10-100)': arr[(arr >= 10) & (arr < 100)],
                'R√©sistant (100-1000)': arr[(arr >= 100) & (arr < 1000)],
                'Tr√®s r√©sistant (>1000)': arr[arr >= 1000]}
       
        zone_counts = [len(zone) for zone in zones.values()]
        zone_colors = ['#0000FF', '#00FF00', '#FFFF00', '#FF0000']
       
        wedges, texts, autotexts = ax3.pie(zone_counts, labels=zones.keys(), colors=zone_colors,
                                          autopct='%1.1f%%', startangle=90)
        ax3.set_title('Classification des Mat√©riaux')
       
        # 4. √âvolution temporelle simul√©e
        ax4.plot(range(len(arr)), arr, 'b-', linewidth=2, marker='o', markersize=4)
        ax4.fill_between(range(len(arr)), arr, alpha=0.3, color='lightblue')
        ax4.set_xlabel('S√©quence de mesure')
        ax4.set_ylabel('R√©sistivit√© (Œ©¬∑m)')
        ax4.set_title('√âvolution des Mesures')
        ax4.set_yscale('log')
        ax4.grid(True, alpha=0.3)
       
        # Ajuster la mise en page
        plt.tight_layout()
       
        # Convertir en base64 pour affichage HTML
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        plot_data = buffer.getvalue()
        buffer.close()
        plt.close()
       
        plot_base64 = base64.b64encode(plot_data).decode()
       
        return f'<img src="data:image/png;base64,{plot_base64}" style="max-width: 100%; height: auto;" alt="Graphique ERT">'
       
    except Exception as e:
        return f"Erreur lors de la g√©n√©ration du graphique: {e}"
def resistivity_color_analysis(resistivity_values: list, dat_file_path: str = None) -> str:
    """Analyse les couleurs de r√©sistivit√© ERT avec validation contre fichiers .dat et d√©tection de mat√©riaux r√©els"""
    if not resistivity_values:
        return "‚ùå Aucune valeur de r√©sistivit√© fournie pour l'analyse"
   
    import numpy as np
    from resistivity_color_mapper import ResistivityColorMapper, DynamicERTAnalyzer
   
    analysis = "üé® ANALYSE DES COULEURS DE R√âSISTIVIT√â ERT\n"
    analysis += "=" * 50 + "\n\n"
   
    # Initialisation des analyseurs
    mapper = ResistivityColorMapper()
    analyzer = DynamicERTAnalyzer()
   
    # Conversion en array numpy
    rho_data = np.array(resistivity_values)
   
    # Statistiques de base
    analysis += f"üìä STATISTIQUES DES R√âSISTIVIT√âS:\n"
    analysis += f" ‚Ä¢ Nombre de valeurs: {len(rho_data)}\n"
    analysis += f" ‚Ä¢ R√©sistivit√© moyenne: {np.mean(rho_data):.2f} Œ©.m\n"
    analysis += f" ‚Ä¢ M√©diane: {np.median(rho_data):.2f} Œ©.m\n"
    analysis += f" ‚Ä¢ √âcart-type: {np.std(rho_data):.2f} Œ©.m\n"
    analysis += f" ‚Ä¢ Plage: {np.min(rho_data):.2f} - {np.max(rho_data):.2f} Œ©.m\n"
    analysis += f" ‚Ä¢ Coefficient de variation: {np.std(rho_data)/np.mean(rho_data):.3f}\n\n"
   
    # Analyse des couleurs par valeur
    analysis += f"üé® CARTOGRAPHIE COULEUR PAR VALEUR:\n"
    sample_values = np.unique(np.round(rho_data, 2))[:15] # √âchantillon pour √©viter surcharge
   
    for rho in sample_values:
        color, desc = mapper.get_color_for_resistivity(rho)
        analysis += f" ‚Ä¢ œÅ = {rho:.2f} Œ©.m ‚Üí Couleur: {color} ({desc})\n"
    analysis += "\n"
   
    # D√©tection de mat√©riaux r√©els avec validation .dat
    analysis += f"üîç D√âTECTION DE MAT√âRIAUX R√âELS:\n"
   
    # Analyse compl√®te du profil
    profile_analysis = analyzer.analyze_resistivity_profile(rho_data, dat_file_path=dat_file_path)
   
    # Mat√©riaux identifi√©s
    materials = profile_analysis.get('materials', [])
    if materials:
        analysis += f"Mat√©riaux potentiels d√©tect√©s (avec validation r√©elle):\n"
        for i, material in enumerate(materials[:8], 1): # Top 8 mat√©riaux
            name = material.get('name', 'inconnu')
            category = material.get('category', 'inconnue')
            similarity = material.get('similarity_score', 0) * 100
            typical_rho = material.get('typical_value', 0)
            nature = material.get('nature', '')
            depth = material.get('depth_range', '')
           
            analysis += f" {i}. {name.upper()} ({category})\n"
            analysis += f" ‚Üí R√©sistivit√© typique: {typical_rho:.2e} Œ©.m\n"
            analysis += f" ‚Üí Score de similarit√©: {similarity:.1f}%\n"
            analysis += f" ‚Üí Nature: {nature}\n"
            if depth:
                analysis += f" ‚Üí Profondeur typique: {depth}\n"
           
            # Validation .dat
            if material.get('dat_validated', False):
                confidence = material.get('dat_confidence', 'low')
                analysis += f" ‚úÖ VALID√â PAR FICHIER .DAT (confiance: {confidence})\n"
            else:
                analysis += f" ‚ö†Ô∏è Non valid√© par fichier .dat\n"
           
            # Validation monde r√©el
            real_validation = analyzer.get_real_world_validation(name)
            if real_validation.get('confidence_level') != 'unknown':
                verified_range = real_validation.get('resistivity_range_verified')
                if verified_range:
                    analysis += f" üåç VALIDATION MONDE R√âEL: {verified_range[0]:.2e} - {verified_range[1]:.2e} Œ©.m\n"
                sources = real_validation.get('sources', [])
                if sources:
                    analysis += f" üìö Sources: {len(sources)} r√©f√©rences trouv√©es\n"
           
            analysis += "\n"
    else:
        analysis += "Aucun mat√©riau sp√©cifique d√©tect√© dans la base de donn√©es.\n\n"
   
    # Couches g√©ologiques identifi√©es
    layers = profile_analysis.get('layers', [])
    if layers:
        analysis += f"üèîÔ∏è COUCHES G√âOLOGIQUES IDENTIFI√âES:\n"
        for layer in layers:
            layer_id = layer.get('layer_id', 0)
            mean_rho = layer.get('mean_resistivity', 0)
            thickness = layer.get('thickness_estimate', 0) * 100
            color = layer.get('color', '#000000')
            desc = layer.get('description', '')
           
            analysis += f" ‚Ä¢ Couche {layer_id}: œÅ = {mean_rho:.1f} Œ©.m ({thickness:.1f}% du profil)\n"
            analysis += f" Couleur: {color} - {desc}\n"
        analysis += "\n"
   
    # Interpr√©tation g√©ologique
    geo_interp = profile_analysis.get('geological_interpretation', '')
    if geo_interp:
        analysis += f"üåç INTERPR√âTATION G√âOLOGIQUE:\n{geo_interp}\n\n"
   
    # Validation .dat globale
    dat_validation = profile_analysis.get('dat_validation')
    if dat_validation:
        analysis += f"üìÅ VALIDATION FICHIER .DAT:\n"
        if dat_validation.get('data_loaded', False):
            score = dat_validation.get('validation_score', 0) * 100
            confidence = dat_validation.get('confidence_level', 'low')
            matches = dat_validation.get('matching_materials', [])
           
            analysis += f" ‚Ä¢ Fichier charg√©: ‚úÖ\n"
            analysis += f" ‚Ä¢ Score de validation: {score:.1f}%\n"
            analysis += f" ‚Ä¢ Niveau de confiance: {confidence.upper()}\n"
            analysis += f" ‚Ä¢ Mat√©riaux correspondants: {len(matches)}\n"
        else:
            analysis += f" ‚Ä¢ Fichier non charg√© ou invalide: ‚ùå\n"
        analysis += "\n"
   
    # Recommandations
    recommendations = profile_analysis.get('recommendations', [])
    if recommendations:
        analysis += f"üí° RECOMMANDATIONS:\n"
        for rec in recommendations:
            analysis += f" ‚Ä¢ {rec}\n"
        analysis += "\n"
   
    # Recherche dynamique de comparaisons suppl√©mentaires
    analysis += f"üîç COMPARAISONS DYNAMIQUES SUPPL√âMENTAIRES:\n"
   
    # Recherche pour les cat√©gories principales
    categories_to_search = ['eau sal√©e', 'minerais m√©talliques', 'roches cristallines', 'sols argileux']
    for category in categories_to_search:
        try:
            search_results = analyzer.data_searcher.search_material_resistivity(category, "ERT geophysical")
            if search_results:
                extracted_values = analyzer.data_searcher.extract_resistivity_values(search_results)
                if extracted_values:
                    avg_rho = np.mean(extracted_values)
                    analysis += f" ‚Ä¢ {category.title()}: œÅ moyenne trouv√©e = {avg_rho:.2f} Œ©.m "
                    analysis += f"(plage: {min(extracted_values):.2f} - {max(extracted_values):.2f} Œ©.m)\n"
        except Exception as e:
            analysis += f" ‚Ä¢ {category.title()}: Erreur recherche - {e}\n"
   
    analysis += "\n"
   
    # Ajouter le tableau et les graphiques
    analysis += f"üìä TABLEAU D√âTAILL√â DES R√âSISTIVIT√âS:\n"
    table_html = generate_resistivity_table(resistivity_values)
    analysis += f"{table_html}\n\n"
   
    analysis += f"üìà GRAPHIQUES D'ANALYSE:\n"
    plot_html = generate_resistivity_plot(resistivity_values)
    analysis += f"{plot_html}\n\n"
   
    analysis += f"‚úÖ Analyse termin√©e - Toutes les d√©tections sont bas√©es sur des valeurs de r√©sistivit√© R√âELLES\n"
    analysis += f"et valid√©es contre des donn√©es scientifiques et fichiers .dat de r√©f√©rence."
   
    return analysis
# ========================================
# Configuration - CHEMINS UNIFI√âS
# ========================================
# D√©finir dynamiquement les chemins bas√©s sur le r√©pertoire KIbalione8
PROJECT_DIR = os.path.expanduser('~/KIbalione8') # Chemin corrig√© vers le dossier contenant les donn√©es et poids
CHATBOT_DIR = PROJECT_DIR
VECTORDB_PATH = os.path.join(CHATBOT_DIR, "vectordb")
CHAT_VECTORDB_PATH = os.path.join(CHATBOT_DIR, "chat_vectordb") # AJOUT M√âMOIRE VECTORIELLE: Base d√©di√©e pour l'historique chat
PDFS_PATH = os.path.join(CHATBOT_DIR, "pdfs")
GRAPHS_PATH = os.path.join(CHATBOT_DIR, "graphs")
MAPS_PATH = os.path.join(CHATBOT_DIR, "maps")
METADATA_PATH = os.path.join(CHATBOT_DIR, "metadata.json")
TRAJECTORIES_PATH = os.path.join(CHATBOT_DIR, "trajectories.json")
WEB_CACHE_PATH = os.path.join(CHATBOT_DIR, "web_cache.json")
GENERATED_PATH = os.path.join(CHATBOT_DIR, "generated")
SUBMODELS_PATH = os.path.join(CHATBOT_DIR, "submodels") # Nouveau: Chemin pour les sous-mod√®les sklearn
MODEL_PATH = os.path.expanduser("~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-V3-0324/snapshots/e9b33add76883f293d6bf61f6bd89b497e80e335")
# Mod√®les qui fonctionnent
WORKING_MODELS = {
    "DeepSeek V3 (Puissant)": "deepseek-ai/DeepSeek-V3-0324",
    "Gemma 2B (Rapide)": "google/gemma-2-2b-it",
    "Llama 3.1 8B (√âquilibr√©)": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "Qwen 2.5 7B (Polyvalent)": "Qwen/Qwen2.5-7B-Instruct",
    "SmolLM 3B (L√©ger)": "HuggingFaceTB/SmolLM3-3B",
}
# ========================================
# Configuration HuggingFace Token depuis .env
# ========================================
# Charger le token depuis .env dans le dossier corrig√©
env_path = os.path.join(CHATBOT_DIR, ".env")
if os.path.exists(env_path):
    load_dotenv(env_path)
    st.write(f"‚úÖ Fichier .env trouv√©: {env_path}")
else:
    st.write(f"‚ö†Ô∏è Aucun fichier .env trouv√© √† {env_path}")
    st.write("Cr√©ez un fichier .env dans ~/KIbalione8 avec: HF_TOKEN=hf_votre_token")
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    raise ValueError("‚ùå HF_TOKEN non trouv√© ! V√©rifiez votre fichier .env")
else:
    st.write(f"üîë Token HF configur√©: {HF_TOKEN[:10]}...")
# D√©finir la variable d'environnement pour huggingface_hub
os.environ["HF_TOKEN"] = HF_TOKEN
os.environ["HUGGINGFACE_HUB_TOKEN"] = HF_TOKEN
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
if not TAVILY_API_KEY:
    raise ValueError("‚ùå TAVILY_API_KEY non trouv√© ! V√©rifiez votre fichier .env")
# ========================================
# Test de connexion HuggingFace
# ========================================
def test_hf_connection():
    """Teste la connexion √† HuggingFace"""
    try:
        from huggingface_hub import whoami
        user_info = whoami(token=HF_TOKEN)
        st.write(f"‚úÖ Connexion HuggingFace r√©ussie: {user_info.get('name', 'Utilisateur')}")
        return True
    except Exception as e:
        st.write(f"‚ùå Erreur connexion HuggingFace: {e}")
        return False
# Tester la connexion au d√©marrage
if not test_hf_connection():
    st.write("‚ö†Ô∏è Probl√®me de connexion HuggingFace, v√©rifiez votre token")
# ========================================
# Fonctions utilitaires
# ========================================
def setup_drive():
    """Cr√©e les dossiers"""
    st.write("üìÅ Configuration des dossiers...")
    os.makedirs(CHATBOT_DIR, exist_ok=True)
    os.makedirs(PDFS_PATH, exist_ok=True)
    os.makedirs(GRAPHS_PATH, exist_ok=True)
    os.makedirs(MAPS_PATH, exist_ok=True)
    os.makedirs(GENERATED_PATH, exist_ok=True)
    os.makedirs(os.path.dirname(CHAT_VECTORDB_PATH), exist_ok=True) # AJOUT M√âMOIRE VECTORIELLE: Dossier pour chat_vectordb
    os.makedirs(SUBMODELS_PATH, exist_ok=True) # Nouveau: Dossier pour sous-mod√®les
    st.write(f"üìÅ Dossier principal : {CHATBOT_DIR}")
    return True
def extract_text_from_pdf(pdf_path):
    """Extraire le texte d'un PDF"""
    text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page_num, page in enumerate(doc):
                page_text = page.get_text()
                text += f"\n[Page {page_num + 1}]\n{page_text}\n"
        return text
    except Exception as e:
        st.write(f"‚ùå Erreur PDF {pdf_path}: {e}")
        return ""
def upload_and_process_pbf(pbf_file):
    """Traitement du fichier PBF upload√©"""
    if pbf_file is None:
        return None, None, "‚ùå Aucun fichier upload√©"
    pbf_path = pbf_file.name
    with open(pbf_path, "wb") as f:
        f.write(pbf_file.getvalue())
    st.write("‚öôÔ∏è Lecture du PBF et construction du graphe...")
    handler = RoadPOIHandler()
    handler.apply_file(pbf_path, locations=True)
    G = handler.graph
    pois = handler.pois
    # Sauvegarder dans le dossier chatbot
    graph_name = os.path.basename(pbf_path).replace('.osm.pbf', '_graph.graphml')
    graph_path = os.path.join(GRAPHS_PATH, graph_name)
    nx.write_graphml(G, graph_path)
    # Sauvegarder les POIs
    pois_name = graph_name.replace('_graph.graphml', '_pois.json')
    pois_path = os.path.join(GRAPHS_PATH, pois_name)
    with open(pois_path, 'w', encoding='utf-8') as f:
        json.dump(pois, f, indent=2, ensure_ascii=False)
    st.write(f"‚úÖ Graphe: {len(G)} n≈ìuds, {G.size()} ar√™tes")
    st.write(f"‚úÖ POIs: {len(pois)} points")
    st.write(f"üíæ Sauvegard√©: {graph_path}")
    return G, pois, f"‚úÖ Graphe cr√©√©: {len(G)} n≈ìuds, {len(pois)} POIs"
def load_existing_graph():
    """Charge un graphe existant"""
    graph_files = [f for f in os.listdir(GRAPHS_PATH) if f.endswith('_graph.graphml')] if os.path.exists(GRAPHS_PATH) else []
    if not graph_files:
        return None, None, "‚ùå Aucun graphe trouv√©"
    graph_file = graph_files[0]
    graph_path = os.path.join(GRAPHS_PATH, graph_file)
    pois_path = os.path.join(GRAPHS_PATH, graph_file.replace('_graph.graphml', '_pois.json'))
    try:
        G = nx.read_graphml(graph_path)
        pois = []
        if os.path.exists(pois_path):
            with open(pois_path, 'r', encoding='utf-8') as f:
                pois = json.load(f)
        return G, pois, f"‚úÖ Graphe charg√©: {len(G)} n≈ìuds, {len(pois)} POIs"
    except Exception as e:
        return None, None, f"‚ùå Erreur: {e}"
@st.cache_resource
def get_embedding_model():
    """Mod√®le d'embedding en cache pour √©viter rechargement"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # Configuration simple pour √©viter conflits de param√®tres
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': device}
    )
# AJOUT M√âMOIRE VECTORIELLE: Fonctions pour la m√©moire chat
def load_chat_vectordb():
    """Charger la base vectorielle pour l'historique chat"""
    if not os.path.exists(CHAT_VECTORDB_PATH):
        return None, "‚ö†Ô∏è Aucune base chat trouv√©e"
    embedding_model = get_embedding_model()
    try:
        chat_vectordb = FAISS.load_local(CHAT_VECTORDB_PATH, embedding_model, allow_dangerous_deserialization=True)
        return chat_vectordb, "‚úÖ Base chat charg√©e"
    except Exception as e:
        return None, f"‚ùå Erreur chat: {e}"
def add_to_chat_db(user_msg, ai_msg, chat_vectordb):
    """Ajouter un √©change user-AI √† la base chat"""
    if chat_vectordb is None:
        embedding_model = get_embedding_model()
        chat_vectordb = FAISS.from_texts([""], embedding_model) # Cr√©er si vide
    exchange = f"User: {user_msg} ||| Assistant: {ai_msg}"
    doc = Document(
        page_content=exchange,
        metadata={"type": "chat_exchange", "timestamp": time.time()}
    )
    chat_vectordb.add_documents([doc])
    chat_vectordb.save_local(CHAT_VECTORDB_PATH)
    return chat_vectordb
def chat_rag_search(question, chat_vectordb, k=3):
    """Rechercher dans l'historique chat pour contexte"""
    if not chat_vectordb:
        return []
    try:
        return chat_vectordb.similarity_search(question, k=k)
    except Exception as e:
        st.write(f"‚ùå Erreur recherche chat: {e}")
        return []
def process_pdfs():
    """Traiter les PDFs"""
    st.write("üìÑ Traitement des PDFs...")
    embedding_model = get_embedding_model()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    # Charger vectordb existante si elle existe
    vectordb = None
    if os.path.exists(VECTORDB_PATH):
        try:
            vectordb, _ = load_vectordb()
        except Exception as e:
            st.write(f"‚ö†Ô∏è Erreur chargement vectordb existante: {e}. Cr√©ation nouvelle.")
            vectordb = None
    # Charger m√©tadonn√©es existantes
    if os.path.exists(METADATA_PATH):
        with open(METADATA_PATH, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
    else:
        metadata = {"processed_files": [], "total_chunks": 0}
    processed_filenames = {p["filename"] for p in metadata["processed_files"]}
    all_documents = []
    pdf_files = [f for f in os.listdir(PDFS_PATH) if f.endswith('.pdf')] if os.path.exists(PDFS_PATH) else []
    if not pdf_files:
        return vectordb, "‚ö†Ô∏è Aucun PDF trouv√©"
  
    # Check pr√©liminaire : si aucun nouveau, skip
    new_pdfs = [f for f in pdf_files if f not in processed_filenames]
    if not new_pdfs:
        return vectordb, "‚úÖ Tous les PDFs d√©j√† trait√©s. Base √† jour !"
  
    progress_bar = st.progress(0)
    status_text = st.empty()
    new_chunks_count = 0
    new_processed = []
    total_pdfs = len(new_pdfs)
    current_pdf = 0
    for pdf_file in pdf_files:
        if pdf_file in processed_filenames:
            st.write(f" üìñ {pdf_file} d√©j√† trait√©, saut√©.")
            continue
        pdf_path = os.path.join(PDFS_PATH, pdf_file)
        st.write(f" üìñ Traitement nouveau PDF : {pdf_file}")
        status_text.text(f"Traitement de {pdf_file}...")
        text = extract_text_from_pdf(pdf_path)
        if not text.strip():
            continue
        try:
            chunks = text_splitter.split_text(text)
        except Exception as e:
            st.write(f"‚ùå Erreur split text pour {pdf_file}: {e}")
            continue
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk,
                metadata={
                    "source": pdf_file,
                    "chunk_id": i,
                    "type": "pdf"
                }
            )
            all_documents.append(doc)
        new_processed.append({"filename": pdf_file, "chunks": len(chunks)})
        new_chunks_count += len(chunks)
        current_pdf += 1
        progress = current_pdf / total_pdfs if total_pdfs > 0 else 1
        progress_bar.progress(progress)
    status_text.text("Finalisation...")
    # Ajouter les trajets sauvegard√©s (toujours, car ils peuvent changer)
    if os.path.exists(TRAJECTORIES_PATH):
        with open(TRAJECTORIES_PATH, 'r', encoding='utf-8') as f:
            trajectories = json.load(f)
        for traj in trajectories:
            traj_text = f"""Trajet: {traj.get('question', '')}
D√©part: {traj.get('start_name', '')}
Arriv√©e: {traj.get('end_name', '')}
Distance: {traj.get('distance', 0)/1000:.2f} km"""
            doc = Document(
                page_content=traj_text,
                metadata={"source": "trajectories", "type": "trajectory"}
            )
            all_documents.append(doc)
    if all_documents:
        try:
            if vectordb is None:
                vectordb = FAISS.from_documents(all_documents, embedding_model)
            else:
                vectordb.add_documents(all_documents)
            vectordb.save_local(VECTORDB_PATH)
        except Exception as e:
            st.write(f"‚ùå Erreur sauvegarde vectordb: {e}")
            return None, "‚ùå √âchec sauvegarde base"
    # Mettre √† jour m√©tadonn√©es seulement si changements
    if new_processed:
        metadata["processed_files"].extend(new_processed)
        metadata["total_chunks"] += new_chunks_count
        with open(METADATA_PATH, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    progress_bar.progress(1)
    status_text.text("Termin√© !")
    return vectordb, f"‚úÖ Base mise √† jour : {len(new_processed)} nouveaux PDFs trait√©s, {new_chunks_count} nouveaux chunks (total : {metadata['total_chunks']})"
def load_vectordb():
    """Charge la base vectorielle"""
    if not os.path.exists(VECTORDB_PATH):
        return None, "‚ö†Ô∏è Aucune base trouv√©e"
    embedding_model = get_embedding_model()
    try:
        vectordb = FAISS.load_local(VECTORDB_PATH, embedding_model, allow_dangerous_deserialization=True)
        return vectordb, "‚úÖ Base charg√©e"
    except Exception as e:
        return None, f"‚ùå Erreur: {e}"
def save_trajectory(question, response, trajectory_info):
    """Sauvegarde un trajet"""
    trajectories = []
    if os.path.exists(TRAJECTORIES_PATH):
        with open(TRAJECTORIES_PATH, 'r', encoding='utf-8') as f:
            trajectories = json.load(f)
    new_trajectory = {
        "question": question,
        "response": response,
        "start_name": trajectory_info.get('start', {}).get('name', ''),
        "end_name": trajectory_info.get('end', {}).get('name', ''),
        "distance": trajectory_info.get('distance', 0)
    }
    trajectories.append(new_trajectory)
    with open(TRAJECTORIES_PATH, 'w', encoding='utf-8') as f:
        json.dump(trajectories, f, indent=2, ensure_ascii=False)
def upload_pdfs(uploaded_files):
    """Upload des PDFs"""
    if uploaded_files is None:
        return []
    saved_files = []
    for file in uploaded_files:
        filename = file.name
        filepath = os.path.join(PDFS_PATH, filename)
        with open(filepath, "wb") as f:
            f.write(file.getvalue())
        saved_files.append(filename)
    return saved_files
# ========================================
# Syst√®me de Cache Web Intelligent
# ========================================
def load_web_cache():
    """Charge le cache web"""
    if os.path.exists(WEB_CACHE_PATH):
        try:
            with open(WEB_CACHE_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {}
def save_web_cache(cache):
    """Sauvegarde le cache web"""
    try:
        with open(WEB_CACHE_PATH, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
    except Exception as e:
        st.write(f"Erreur sauvegarde cache: {e}")
def get_cache_key(query, source="text"):
    """G√©n√®re une cl√© de cache pour une requ√™te"""
    return f"{source}:{query.lower().strip()}"
def is_cache_expired(cache_entry, max_age_hours=24):
    """V√©rifie si l'entr√©e du cache a expir√©"""
    current_time = time.time()
    return (current_time - cache_entry.get('timestamp', 0)) > (max_age_hours * 3600)
def get_cache_stats():
    """Obtient les statistiques du cache"""
    try:
        cache = load_web_cache()
        if not cache:
            return "Cache vide"
        total_entries = len(cache)
        expired_count = sum(1 for entry in cache.values() if is_cache_expired(entry))
        valid_count = total_entries - expired_count
        return f"üìä Cache: {total_entries} entr√©es total, {valid_count} valides, {expired_count} expir√©es"
    except Exception as e:
        return f"‚ùå Erreur stats: {e}"
# ========================================
# Fonctions RAG et Web Search Am√©lior√©es
# ========================================
class LocalClient:
    def __init__(self):
        from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
       
        MODEL_PATH = os.path.expanduser("~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-V3-0324/snapshots/e9b33add76883f293d6bf61f6bd89b497e80e335")
       
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
       
        # Load model with device_map for large models
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            local_files_only=True,
            device_map="auto",
            torch_dtype="auto"
        )
       
        self.model.eval()
    def chat_completion(self, messages, model, max_tokens, temperature, stream=False):
        try:
            # Use chat template for proper formatting
            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(inputs, max_new_tokens=max_tokens, temperature=temperature, do_sample=temperature > 0, pad_token_id=self.tokenizer.eos_token_id)
            generated_ids = outputs[0][inputs.shape[-1]:]
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            class Choice:
                def __init__(self, content):
                    self.message = type('msg', (), {'content': content})()
            class Resp:
                def __init__(self, choice):
                    self.choices = [choice]
            return Resp(Choice(response))
        except Exception as e:
            class Choice:
                def __init__(self, content):
                    self.message = type('msg', (), {'content': content})()
            class Resp:
                def __init__(self, choice):
                    self.choices = [choice]
            return Resp(Choice(f"Erreur locale: {str(e)}"))
@st.cache_resource
def create_client():
    """Cr√©er le client Inference avec gestion d'erreurs am√©lior√©e"""
    try:
        client = InferenceClient(token=HF_TOKEN)
        return client
    except Exception as e:
        st.write(f"‚ùå Erreur cr√©ation client: {e}. Passage en mode local.")
        return LocalClient()
def rag_search(question, vectordb, k=3):
    """Rechercher dans la base vectorielle avec recherche GLOBALE ILLIMIT√âE"""
    if not vectordb:
        return []
    try:
        # RECHERCHE ILLIMIT√âE: pas de limite arbitraire
        # Si k petit, forcer minimum 100 pour fouille exhaustive
        effective_k = max(k, 100) if k < 100 else k
        
        # R√©cup√©rer le nombre total de documents
        total_docs = vectordb.index.ntotal if hasattr(vectordb, 'index') else 1000
        
        # Ajuster k au minimum entre le demand√© et le total disponible
        # Cap √† 500 pour performance raisonnable
        final_k = min(effective_k, total_docs, 500) if total_docs > 0 else effective_k
        
        return vectordb.similarity_search(question, k=final_k)
    except Exception as e:
        st.write(f"‚ùå Erreur recherche: {e}")
        return []
def enhanced_web_search(query, max_results=5, search_type="text", use_cache=True):
    """
    Recherche web avanc√©e avec cache intelligent et multiple sources
    Args:
        query: Requ√™te de recherche
        max_results: Nombre max de r√©sultats
        search_type: Type de recherche ("text", "news", "both")
        use_cache: Utiliser le cache
    Returns:
        Liste de r√©sultats enrichis
    """
    cache = load_web_cache() if use_cache else {}
    results = []
    try:
        # Recherche texte
        if search_type in ["text", "both"]:
            cache_key = get_cache_key(query, "text")
            if cache_key in cache and not is_cache_expired(cache[cache_key]):
                st.write(f"üìã Utilisation cache pour: {query}")
                text_results = cache[cache_key]['results']
            else:
                st.write(f"üîç Recherche web pour: {query}")
                tavily = TavilyClient(api_key=TAVILY_API_KEY)
                text_results = []
                try:
                    raw_results = tavily.search(query, max_results=max_results, search_depth="advanced", topic="general")
                    for r in raw_results.get('results', []):
                        text_results.append({
                            'title': r.get('title', ''),
                            'body': r.get('content', ''),
                            'href': r.get('url', ''),
                            'source_type': 'web_search'
                        })
                    # Sauvegarder en cache
                    cache[cache_key] = {
                        'results': text_results,
                        'timestamp': time.time()
                    }
                    if use_cache:
                        save_web_cache(cache)
                except Exception as e:
                    st.write(f"Erreur recherche texte: {e}")
                    text_results = []
            results.extend(text_results)
        # Recherche actualit√©s
        if search_type in ["news", "both"]:
            cache_key = get_cache_key(query, "news")
            if cache_key in cache and not is_cache_expired(cache[cache_key], max_age_hours=6):
                news_results = cache[cache_key]['results']
            else:
                tavily = TavilyClient(api_key=TAVILY_API_KEY)
                news_results = []
                try:
                    raw_news = tavily.search(query, max_results=max_results//2 if search_type == "both" else max_results, search_depth="advanced", topic="news")
                    for r in raw_news.get('results', []):
                        news_results.append({
                            'title': r.get('title', ''),
                            'body': r.get('content', ''),
                            'url': r.get('url', ''),
                            'date': r.get('published_date', ''),
                            'source': r.get('source', ''),
                            'source_type': 'news'
                        })
                    # Sauvegarder en cache (6h pour les news)
                    cache[cache_key] = {
                        'results': news_results,
                        'timestamp': time.time()
                    }
                    if use_cache:
                        save_web_cache(cache)
                except Exception as e:
                    st.write(f"Erreur recherche news: {e}")
                    news_results = []
            results.extend(news_results)
    except Exception as e:
        st.write(f"‚ùå Erreur recherche web globale: {e}")
        results = [{'title': 'Erreur de recherche', 'body': f'Erreur: {e}', 'source_type': 'error'}]
    return results
def smart_content_extraction(url, max_length=1000):
    """
    Extraction intelligente du contenu d'une page web
    Args:
        url: URL √† scraper
        max_length: Longueur max du contenu
    Returns:
        Contenu extrait et nettoy√©
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        # Supprimer les √©l√©ments non pertinents
        for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
            element.decompose()
        # Extraire le texte principal
        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content') or soup.body
        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)
        # Nettoyer et tronquer
        text = ' '.join(text.split()) # Normaliser les espaces
        return text[:max_length] + ('...' if len(text) > max_length else '')
    except Exception as e:
        st.write(f"Erreur extraction contenu {url}: {e}")
        return f"Impossible d'extraire le contenu de {url}"
def intelligent_query_expansion(query):
    """
    Expansion intelligente des requ√™tes pour am√©liorer les r√©sultats
    Args:
        query: Requ√™te originale
    Returns:
        Liste de requ√™tes expandues
    """
    expanded_queries = [query] # Toujours inclure la requ√™te originale
    # D√©tection de mots-cl√©s pour expansion contextuelle
    keywords = {
        'actualit√©': ['news', 'derni√®res nouvelles', 'r√©cent'],
        'comment': ['tutorial', 'guide', '√©tapes'],
        'pourquoi': ['raison', 'cause', 'explication'],
        'comparaison': ['vs', 'diff√©rence', 'comparatif'],
        'prix': ['co√ªt', 'tarif', 'budget'],
        'avis': ['opinion', 'critique', 'review']
    }
    query_lower = query.lower()
    for trigger, expansions in keywords.items():
        if trigger in query_lower:
            for expansion in expansions:
                expanded_queries.append(f"{query} {expansion}")
    return expanded_queries[:3] # Limiter √† 3 requ√™tes max
def hybrid_search_enhanced(query, vectordb, k=3, web_search_enabled=True, search_type="both", chat_vectordb=None): # AJOUT M√âMOIRE VECTORIELLE: Param pour chat_vectordb
    """
    Recherche hybride ILLIMIT√âE combinant RAG local GLOBAL et web avec intelligence maximale
    Args:
        query: Requ√™te de recherche
        vectordb: Base vectorielle locale
        k: Nombre MINIMUM de r√©sultats RAG (sera augment√© pour recherche ILLIMIT√âE)
        web_search_enabled: Activer la recherche web
        search_type: Type de recherche web
        chat_vectordb: Base pour historique chat (optionnel)
    Returns:
        Liste de documents combin√©s et enrichis
    """
    all_results = []
    
    # 1. Recherche RAG locale ILLIMIT√âE avec k maximis√©
    # Pour une fouille COMPL√àTE, utiliser au moins 100 documents (pas de limite!)
    global_k = max(k, 100)
    local_docs = rag_search(query, vectordb, global_k)
    for doc in local_docs:
        doc.metadata['search_source'] = 'local_rag'
        doc.metadata['relevance_score'] = 1.0 # Score max pour les docs locaux
    all_results.extend(local_docs)
    
    st.write(f"üìö Fouille GLOBALE ILLIMIT√âE: {len(local_docs)} documents trouv√©s dans la base locale compl√®te")
    # AJOUT M√âMOIRE VECTORIELLE: Recherche dans historique chat pour contexte conversationnel
    if chat_vectordb:
        chat_docs = chat_rag_search(query, chat_vectordb, k=3)
        for doc in chat_docs:
            doc.metadata['search_source'] = 'chat_history'
            doc.metadata['relevance_score'] = 0.9
        all_results.extend(chat_docs[:2]) # Limiter √† 2 pour √©viter surcharge
    # 2. Recherche web intelligente si activ√©e
    if web_search_enabled:
        st.write(f"üåê Recherche web activ√©e pour: {query}")
        # Expansion de requ√™te pour de meilleurs r√©sultats
        expanded_queries = intelligent_query_expansion(query)
        web_results = []
        for exp_query in expanded_queries:
            try:
                search_results = enhanced_web_search(
                    exp_query,
                    max_results=3,
                    search_type=search_type
                )
                for result in search_results:
                    # Cr√©er un document √† partir du r√©sultat web
                    content = f"Titre: {result.get('title', '')}\n"
                    content += f"Contenu: {result.get('body', '')}\n"
                    if result.get('source_type') == 'news' and result.get('date'):
                        content += f"Date: {result.get('date')}\n"
                        content += f"Source: {result.get('source', '')}\n"
                    # Extraction de contenu suppl√©mentaire si URL disponible
                    url = result.get('href') or result.get('url')
                    if url and len(result.get('body', '')) < 200:
                        st.write(f"üìÑ Extraction contenu de: {url}")
                        extra_content = smart_content_extraction(url)
                        if extra_content and "Impossible d'extraire" not in extra_content:
                            content += f"\nContenu d√©taill√©: {extra_content}"
                    doc = Document(
                        page_content=content,
                        metadata={
                            'source': url or 'web_search',
                            'type': result.get('source_type', 'web'),
                            'search_source': 'web',
                            'query_used': exp_query,
                            'relevance_score': 0.8 if exp_query == query else 0.6
                        }
                    )
                    web_results.append(doc)
            except Exception as e:
                st.write(f"Erreur recherche pour '{exp_query}': {e}")
                continue
        # Filtrer les doublons et trier par pertinence
        unique_web_results = []
        seen_urls = set()
        for doc in web_results:
            url = doc.metadata.get('source', '')
            if url not in seen_urls:
                seen_urls.add(url)
                unique_web_results.append(doc)
        # Trier par score de pertinence
        unique_web_results.sort(key=lambda x: x.metadata.get('relevance_score', 0), reverse=True)
        all_results.extend(unique_web_results[:5]) # Max 5 r√©sultats web
    return all_results
def generate_answer_enhanced(question, context_docs, model_name, include_sources=True):
    """
    G√©n√©ration de r√©ponse am√©lior√©e avec gestion des sources multiples
    Args:
        question: Question pos√©e
        context_docs: Documents de contexte
        model_name: Mod√®le √† utiliser
        include_sources: Inclure les sources dans la r√©ponse
    Returns:
        R√©ponse g√©n√©r√©e avec sources
    """
    if not context_docs:
        context = "Aucun contexte sp√©cifique trouv√©."
    else:
        context_parts = []
        local_sources = []
        web_sources = []
        chat_sources = [] # AJOUT M√âMOIRE VECTORIELLE: Sources pour historique chat
        for i, doc in enumerate(context_docs):
            source = doc.metadata.get('source', 'Document inconnu')
            doc_type = doc.metadata.get('type', 'unknown')
            search_source = doc.metadata.get('search_source', 'unknown')
            content = doc.page_content.strip()
            # Classifier les sources
            if search_source == 'local_rag':
                local_sources.append(f"[{i+1}] {source} ({doc_type})")
            elif search_source == 'chat_history':
                chat_sources.append(f"[{i+1}] Historique pr√©c√©dent: {source}")
            else:
                web_sources.append(f"[{i+1}] {source}")
            context_parts.append(f"[Source {i+1} - {doc_type}]\n{content}")
        context = "\n\n".join(context_parts)
    # Prompt am√©lior√© avec instructions pour les sources (ajout chat)
    prompt = f"""Tu es un assistant IA intelligent qui r√©pond aux questions en utilisant √† la fois des documents locaux, l'historique des conversations pass√©es, et des informations web r√©centes.
CONTEXTE DISPONIBLE (incluant historique pour continuit√©):
{context}
QUESTION: {question}
INSTRUCTIONS:
- Utilise l'historique chat pour maintenir la fluidit√© et rappeler les √©changes pr√©c√©dents
- Utilise toutes les sources disponibles pour donner une r√©ponse compl√®te et pr√©cise
- Si les informations web contredisent les documents locaux ou l'historique, mentionne les deux perspectives
- Privil√©gie les informations r√©centes pour les sujets d'actualit√©
- Sois pr√©cis et cite tes sources si n√©cessaire
- Si certaines informations manquent, dis-le clairement et propose de clarifier bas√© sur l'historique
R√âPONSE D√âTAILL√âE:"""
    try:
        client = create_client()
        messages = [{"role": "user", "content": prompt}]
        response = client.chat_completion(
            messages=messages,
            model=model_name,
            max_tokens=600,
            temperature=0.3
        )
        answer = response.choices[0].message.content
        # Ajouter les sources si demand√©
        if include_sources and context_docs:
            sources_text = "\n\nüìö **Sources consult√©es:**\n"
            if chat_sources: # AJOUT M√âMOIRE VECTORIELLE
                sources_text += "**Historique conversation:**\n"
                for source in chat_sources[:2]:
                    sources_text += f"‚Ä¢ {source}\n"
            if local_sources:
                sources_text += "**Documents locaux:**\n"
                for source in local_sources[:3]: # Limiter l'affichage
                    sources_text += f"‚Ä¢ {source}\n"
            if web_sources:
                sources_text += "**Sources web:**\n"
                for source in web_sources[:3]: # Limiter l'affichage
                    sources_text += f"‚Ä¢ {source}\n"
            answer += sources_text
        return answer
    except Exception as e:
        error_str = str(e)
        # Check for payment error and retry with LocalClient
        if "402" in error_str or "Payment Required" in error_str:
            try:
                # Retry with LocalClient
                local_client = LocalClient()
                messages = [{"role": "user", "content": prompt}]
                response = local_client.chat_completion(
                    messages=messages,
                    model=model_name,
                    max_tokens=600,
                    temperature=0.3
                )
                answer = response.choices[0].message.content
                # Ajouter les sources si demand√©
                if include_sources and context_docs:
                    sources_text = "\n\nüìö **Sources consult√©es (mode local):**\n"
                    if chat_sources:
                        sources_text += "**Historique conversation:**\n"
                        for source in chat_sources[:2]:
                            sources_text += f"‚Ä¢ {source}\n"
                    if local_sources:
                        sources_text += "**Documents locaux:**\n"
                        for source in local_sources[:3]:
                            sources_text += f"‚Ä¢ {source}\n"
                    if web_sources:
                        sources_text += "**Sources web:**\n"
                        for source in web_sources[:3]:
                            sources_text += f"‚Ä¢ {source}\n"
                    answer += sources_text
                return answer + "\n\n‚ö†Ô∏è R√©ponse g√©n√©r√©e en mode local (API distante indisponible)."
            except Exception as local_e:
                return f"‚ùå Erreur g√©n√©ration (m√™me en local): {str(local_e)}"
        else:
            return f"‚ùå Erreur g√©n√©ration: {error_str}"
# ========================================
# Fonctions Web Search et Hybrid (Mises √† jour)
# ========================================
def web_search(query, max_results=5):
    """Version simplifi√©e pour compatibilit√©"""
    try:
        results = enhanced_web_search(query, max_results, "text")
        return [f"{r.get('title', '')}: {r.get('href', r.get('url', ''))} - {r.get('body', '')}" for r in results]
    except Exception as e:
        return [f"‚ùå Erreur recherche web: {e}"]
def hybrid_search(query, vectordb, k=3):
    """Version simplifi√©e pour compatibilit√©"""
    return hybrid_search_enhanced(query, vectordb, k, web_search_enabled=True)
def final_search(question, vectordb, graph, pois):
    """Recherche finale combinant toutes les sources"""
    results = hybrid_search_enhanced(question, vectordb, k=3, web_search_enabled=True)
    # OSM si mention lieu
    if any(keyword in question.lower() for keyword in ["aller", "trajet", "itin√©raire", "route", "navigation"]):
        try:
            carte, reponse, traj = calculer_trajet(question, graph, pois)
            if traj:
                results.append(Document(
                    page_content=reponse,
                    metadata={"source": "trajet_osm", "type": "navigation"}
                ))
        except:
            pass
    return results
# ========================================
# Fonctions Mod√®les Hugging Face Sp√©cialis√©s
# ========================================
@st.cache_resource
def initialize_specialized_models():
    """Initialise les mod√®les sp√©cialis√©s avec gestion d'erreurs"""
    device_id = 0 if torch.cuda.is_available() else -1
    models = {}
    try:
        model_name = "facebook/bart-large-cnn"
        # V√©rifier cache local
        cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        model_cache = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
        use_local = os.path.exists(model_cache)
        
        if use_local:
            st.write(f"üì¶ Mod√®le {model_name} trouv√© en cache")
        
        try:
            models['summarizer'] = pipeline("summarization", model=model_name, device=device_id, local_files_only=use_local)
        except Exception:
            st.write(f"‚¨áÔ∏è T√©l√©chargement de {model_name}")
            models['summarizer'] = pipeline("summarization", model=model_name, device=device_id)
        
        st.write("‚úÖ Mod√®le de r√©sum√© charg√©")
    except Exception as e:
        st.write(f"‚ö†Ô∏è Erreur chargement summarizer: {e}")
        models['summarizer'] = None
    try:
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
        model_name = "Helsinki-NLP/opus-mt-fr-en"
        
        # V√©rifier cache local
        cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        model_cache = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
        use_local = os.path.exists(model_cache)
        
        if use_local:
            st.write(f"üì¶ Mod√®le {model_name} trouv√© en cache")
        
        # Essayer avec cache local d'abord
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=use_local)
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name,
                use_safetensors=True,
                device_map="auto" if device_id != -1 else None,
                low_cpu_mem_usage=True,
                local_files_only=use_local
            )
        except Exception:
            # Fallback sans local_files_only
            st.write(f"‚¨áÔ∏è T√©l√©chargement de {model_name}")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name,
                use_safetensors=True,
                device_map="auto" if device_id != -1 else None,
                low_cpu_mem_usage=True
            )
        
        models['translator'] = pipeline("translation", model=model, tokenizer=tokenizer, device=device_id)
        st.write("‚úÖ Mod√®le de traduction charg√©")
    except Exception as e:
        st.write(f"‚ö†Ô∏è Erreur chargement translator: {e}")
        models['translator'] = None
    try:
        models['captioner'] = None
        st.write("‚úÖ Captioner configur√© pour utiliser LLM (llava)")
    except Exception as e:
        st.write(f"‚ö†Ô∏è Erreur chargement captioner: {e}")
        models['captioner'] = None
    try:
        model_name = "dbmdz/bert-large-cased-finetuned-conll03-english"
        # V√©rifier cache local
        cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        model_cache = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
        use_local = os.path.exists(model_cache)
        
        if use_local:
            st.write(f"üì¶ Mod√®le {model_name} trouv√© en cache")
        
        try:
            models['ner'] = pipeline("ner", model=model_name, device=device_id, local_files_only=use_local)
        except Exception:
            st.write(f"‚¨áÔ∏è T√©l√©chargement de {model_name}")
            models['ner'] = pipeline("ner", model=model_name, device=device_id)
        
        st.write("‚úÖ Mod√®le NER charg√©")
        st.write("‚ö†Ô∏è Warning NER ignor√© : weights pooler non utilis√©s (normal pour ce checkpoint).")
    except Exception as e:
        st.write(f"‚ö†Ô∏è Erreur chargement NER: {e}")
        models['ner'] = None
    return models
# Initialiser les mod√®les
SPECIALIZED_MODELS = initialize_specialized_models()
def summarize_text(text):
    if SPECIALIZED_MODELS['summarizer'] is None:
        return "‚ùå Mod√®le de r√©sum√© non disponible"
    try:
        return SPECIALIZED_MODELS['summarizer'](text[:1024], max_length=200, min_length=30, do_sample=False)[0]['summary_text']
    except Exception as e:
        return f"‚ùå Erreur r√©sum√©: {e}"
def translate_text(text, src_lang="fr", tgt_lang="en"):
    if SPECIALIZED_MODELS['translator'] is None:
        return "‚ùå Mod√®le de traduction non disponible"
    try:
        return SPECIALIZED_MODELS['translator'](text)[0]['translation_text']
    except Exception as e:
        return f"‚ùå Erreur traduction: {e}"
def caption_image(image_path):
    client = create_client()
    model = "llava-hf/llava-1.5-7b-hf"
    prompt = "Generate a detailed caption for this image."
    try:
        return client.image_to_text(image_path, prompt=prompt, model=model, max_tokens=500)
    except Exception as e:
        return f"‚ùå Erreur caption: {e}"
def extract_entities(text):
    if SPECIALIZED_MODELS['ner'] is None:
        return "‚ùå Mod√®le NER non disponible"
    try:
        return SPECIALIZED_MODELS['ner'](text)
    except Exception as e:
        return f"‚ùå Erreur NER: {e}"
# ========================================
# Fonctions de g√©n√©ration avec Stable Diffusion et similaires
# ========================================
def generate_text_to_image(prompt):
    """G√©n√®re une image √† partir de texte"""
    if not load_diffusers():
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = DiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe.to(device)
        image = pipe(prompt).images[0]
        path = os.path.join(GENERATED_PATH, f"image_{int(time.time())}.png")
        image.save(path)
        return f"Image g√©n√©r√©e et sauvegard√©e √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration image: {e}"
def generate_text_to_video(prompt):
    """G√©n√®re une vid√©o √† partir de texte"""
    if not load_diffusers():
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = DiffusionPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16", use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        if device == "cuda":
            pipe.enable_model_cpu_offload()
        else:
            pipe.to(device)
        gen = pipe(prompt, num_inference_steps=25)
        frames = gen.frames[0] # Assuming batch size 1
        path = os.path.join(GENERATED_PATH, f"video_{int(time.time())}.gif")
        imageio.mimsave(path, frames, fps=5)
        return f"Vid√©o g√©n√©r√©e et sauvegard√©e √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration vid√©o: {e}"
def generate_text_to_audio(prompt):
    """G√©n√®re un son √† partir de texte"""
    if not load_diffusers():
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = AudioLDMPipeline.from_pretrained("cvssp/audio-ldm", torch_dtype=torch.float16, use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe.to(device)
        audio = pipe(prompt, audio_length_in_s=5.0).audios[0]
        path = os.path.join(GENERATED_PATH, f"audio_{int(time.time())}.wav")
        wavfile.write(path, rate=16000, data=audio) # Assuming 16kHz sample rate
        return f"Son g√©n√©r√© et sauvegard√© √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration son: {e}"
def generate_text_to_3d(prompt):
    """G√©n√®re un mod√®le 3D √† partir de texte (rendue image)"""
    if not load_diffusers():
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = ShapEPipeline.from_pretrained("openai/shap-e", use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe.to(device)
        output = pipe(prompt, num_inference_steps=64)
        image = output.images[0]
        path = os.path.join(GENERATED_PATH, f"3d_text_{int(time.time())}.png")
        image.save(path)
        return f"Rendu 3D g√©n√©r√© et sauvegard√© √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration 3D (texte): {e}"
def generate_image_to_3d(image_path):
    """G√©n√®re un mod√®le 3D √† partir d'une image (rendue image)"""
    if not load_diffusers():
        return "‚ùå Diffusers non disponible - fonctionnalit√© d√©sactiv√©e"
    try:
        pipe = ShapEImg2ImgPipeline.from_pretrained("openai/shap-e", use_auth_token=HF_TOKEN)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe.to(device)
        image = Image.open(image_path)
        output = pipe(image, num_inference_steps=64)
        rendered_image = output.images[0]
        path = os.path.join(GENERATED_PATH, f"3d_image_{int(time.time())}.png")
        rendered_image.save(path)
        return f"Rendu 3D g√©n√©r√© √† partir de l'image et sauvegard√© √† {path}"
    except Exception as e:
        return f"‚ùå Erreur g√©n√©ration 3D (image): {e}"
# ========================================
# Agent LangChain Am√©lior√© avec Recherche Web
# ========================================
def get_llm(model_name):
    """Fonction dynamique pour obtenir LLM: API si disponible, local sinon"""
    try:
        llm = HuggingFaceEndpoint(
            repo_id=model_name,
            huggingfacehub_api_token=HF_TOKEN,
            temperature=0.3,
            max_new_tokens=3000  # AUGMENT√â pour r√©ponses COMPL√àTES
        )
        st.write(f"‚úÖ Utilisation API pour {model_name}")
        return llm
    except Exception as e:
        st.write(f"‚ö†Ô∏è API indisponible ({e}). Fallback sur LLM local Qwen.")
        return st.session_state.qwen_llm  # Utilise le Qwen local

def create_enhanced_agent(model_name, vectordb, graph, pois, chat_vectordb=None): # AJOUT M√âMOIRE VECTORIELLE: Param pour chat
    """
    Cr√©e un agent LangChain am√©lior√© avec capacit√©s de recherche web
    Args:
        model_name: Nom du mod√®le HuggingFace
        vectordb: Base vectorielle locale
        graph: Graphe OSM
        pois: Points d'int√©r√™t
        chat_vectordb: Base pour historique chat (optionnel)
    Returns:
        Agent configur√© avec tous les outils
    """
    llm = get_llm(model_name)  # Switch dynamique ici
    # Configuration des outils de recherche web avec Tavily
    # Note: DuckDuckGoSearch n'est plus utilis√©, Tavily est pr√©f√©r√© pour la qualit√©
    search_tool = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=5)
    search_results_tool = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=5, include_raw_content=True)
    tools = [
        # Outils de base RAG et recherche
        Tool(
            name="Local_Knowledge_Base",
            func=lambda q: search_vectorstore(q),
            description="üîç FOUILLE GLOBALE ILLIMIT√âE dans TOUS les documents locaux (PDFs, rapports). Recherche exhaustive sans limite de documents. Utilise EN PREMIER pour questions sur donn√©es internes. Retourne TOUS les passages pertinents avec sources group√©es."
        ),
        Tool(
            name="Chat_History_Search", # AJOUT M√âMOIRE VECTORIELLE: Nouvel outil pour historique
            func=lambda q: "\n\n".join([d.page_content for d in chat_rag_search(q, chat_vectordb, k=3)]) if chat_vectordb else "‚ùå Historique chat non disponible",
            description="Recherche dans l'historique des conversations pass√©es pour maintenir la continuit√©. Utilise pour les questions de suites de discussion."
        ),
        Tool(
            name="Web_Search",
            func=lambda q: search_tool.run(q),
            description="Recherche sur Internet pour des informations r√©centes, actualit√©s, ou des connaissances g√©n√©rales non disponibles localement."
        ),
        Tool(
            name="Web_Search_Detailed",
            func=lambda q: search_results_tool.run(q),
            description="Recherche web d√©taill√©e avec sources et liens. Utilise pour obtenir des r√©sultats web structur√©s avec URLs."
        ),
        Tool(
            name="Hybrid_Search",
            func=lambda q: "\n\n".join([d.page_content for d in hybrid_search_enhanced(q, vectordb, k=100, web_search_enabled=True, chat_vectordb=chat_vectordb)]) if vectordb else search_vectorstore(q),
            description="üåê RECHERCHE HYBRIDE ILLIMIT√âE: Combine TOUTE la base locale (100+ docs), historique chat complet ET web multi-sources. Fouille exhaustive GLOBALE pour maximum de contexte. Id√©al pour questions complexes n√©cessitant synth√®se compl√®te."
        ),
        Tool(
            name="Current_News_Search",
            func=lambda q: "\n\n".join([f"{r.get('title', '')}: {r.get('body', '')}" for r in enhanced_web_search(q, search_type="news")]),
            description="Recherche sp√©cialis√©e pour les actualit√©s r√©centes et informations temporelles."
        ),
        # Outils sp√©cialis√©s
        Tool(
            name="OSM_Route_Calculator",
            func=lambda q: calculer_trajet(q, graph, pois)[1] if graph and pois else "‚ùå Aucune carte OSM disponible",
            description="Calcule des itin√©raires routiers entre deux lieux. Utilise pour les questions de navigation, trajets, ou g√©olocalisation."
        ),
        Tool(
            name="Smart_Content_Extractor",
            func=lambda url: smart_content_extraction(url) if url.startswith('http') else "‚ùå URL invalide",
            description="Extrait le contenu d√©taill√© d'une page web sp√©cifique. Fournis une URL compl√®te."
        ),
        Tool(
            name="Text_Summarizer",
            func=summarize_text,
            description="R√©sume un texte long en version concise. Utile pour synth√©tiser des informations volumineuses."
        ),
        Tool(
            name="Language_Translator",
            func=translate_text,
            description="Traduit du fran√ßais vers l'anglais. Utile pour traiter des sources en langue √©trang√®re."
        ),
        Tool(
            name="Image_Analyzer",
            func=caption_image,
            description="Analyse et d√©crit le contenu d'une image. Fournis le chemin vers un fichier image."
        ),
        Tool(
            name="Entity_Extractor",
            func=lambda t: json.dumps(extract_entities(t)),
            description="Extrait des entit√©s nomm√©es (personnes, lieux, organisations) d'un texte."
        ),
        # Nouveaux outils Stable Diffusion via API
        Tool(
            name="Text_To_Image_Generator",
            func=generate_text_to_image,
            description="G√©n√®re une image √† partir d'une description textuelle. Fournis un prompt descriptif."
        ),
        Tool(
            name="Text_To_Video_Generator",
            func=generate_text_to_video,
            description="G√©n√®re une vid√©o √† partir d'une description textuelle. Fournis un prompt descriptif."
        ),
        Tool(
            name="Text_To_Audio_Generator",
            func=generate_text_to_audio,
            description="G√©n√®re un son ou audio √† partir d'une description textuelle. Fournis un prompt descriptif."
        ),
        Tool(
            name="Text_To_3D_Generator",
            func=generate_text_to_3d,
            description="G√©n√®re un mod√®le 3D (rendue image) √† partir d'une description textuelle. Fournis un prompt descriptif."
        ),
        Tool(
            name="Image_To_3D_Generator",
            func=generate_image_to_3d,
            description="G√©n√®re un mod√®le 3D (rendue image) √† partir d'une image. Fournis le chemin vers un fichier image."
        ),
        # OUTILS IA SP√âCIALIS√âS (1-2GB)
        Tool(
            name="AI_Code_Generator",
            func=generate_code_with_ai,
            description="G√©n√®re du code Python/JavaScript/etc parfait avec DeepSeek-Coder-1.3B. Expert en programmation, debugging, optimisation. Fournis une description du code souhait√©."
        ),
        Tool(
            name="AI_Plot_Generator",
            func=generate_plot_code,
            description="G√©n√®re du code matplotlib/seaborn pour cr√©er des graphiques scientifiques professionnels. Fournis: description donn√©es + type graphique souhait√©."
        ),
        # Ajout des outils ERT/Binary du premier code
        Tool(
            name="Binary_Analysis",
            func=lambda q: analyze_with_ai(q, file_bytes, numbers, hex_dump, n_clusters=3) if 'file_bytes' in globals() else "‚ùå Fichier binaire requis",
            description="Analyse compl√®te d'un fichier binaire avec outils ERT, statistiques, entropie. Fournis une requ√™te d'analyse."
        ),
        Tool(
            name="Deep_Binary_Investigation",
            func=lambda file_name: deep_binary_investigation(file_bytes, file_name).get('full_report', '') if 'file_bytes' in globals() else "‚ùå Fichier binaire requis",
            description="üîç FOUILLE INTELLIGENTE d'un fichier binaire upload√©: Combine Hex+ASCII Dump + Base Vectorielle RAG + Base ERT pour interpr√©tation scientifique. Analyse d√©j√† effectu√©e sur fichiers upload√©s. Fournis le nom du fichier."
        ),
        Tool(
            name="ERT_Interpretation",
            func=lambda numbers_str: ert_geophysical_interpretation(eval(numbers_str)) if numbers_str else "‚ùå Liste de nombres requise",
            description="Interpr√®te des donn√©es ERT (r√©sistivit√©s). Fournis une liste de nombres comme '[10.5, 20.3, ...]'."
        ),
    ]
    # Configuration de l'agent avec prompt ultra-optimis√© pour autonomie et pr√©cision
    agent_prompt = PromptTemplate.from_template("""Tu es Kibali Analyst, l'assistant IA le plus avanc√© au monde, combinant les meilleurs aspects de ChatGPT, Claude, Grok et GPT-4.

üåü PERSONNALIT√â & APPROCHE:
‚Ä¢ Naturel, conversationnel et empathique comme ChatGPT
‚Ä¢ Analytique, m√©thodique et √©thique comme Claude  
‚Ä¢ Cr√©atif, humoristique et audacieux comme Grok
‚Ä¢ Pr√©cis, technique et exhaustif comme GPT-4
‚Ä¢ TOUJOURS utile, jamais condescendant
‚Ä¢ Adapte le ton selon le contexte (casual ‚Üî formel)

üéØ M√âTHODOLOGIE SUP√âRIEURE (10 √âTAPES):

1Ô∏è‚É£ COMPR√âHENSION PROFONDE:
   ‚úì Analyse s√©mantique multi-niveau de la question
   ‚úì D√©tecte intentions cach√©es et besoins implicites
   ‚úì Identifie contexte culturel, temporel et √©motionnel
   ‚úì Reformule mentalement en 3 angles diff√©rents

2Ô∏è‚É£ RECHERCHE GLOBALE ILLIMIT√âE:
   ‚úì FOUILLE EXHAUSTIVE base locale (AUCUNE limite de documents)
   ‚úì Recherche web MULTI-SOURCES (minimum 10 r√©sultats analys√©s)
   ‚úì Consultation historique conversations (continuit√©)
   ‚úì V√©rification crois√©e informations contradictoires
   ‚úì Sources acad√©miques, news, forums, documentation officielle

3Ô∏è‚É£ ANALYSE CRITIQUE & SYNTH√àSE:
   ‚úì √âvalue cr√©dibilit√© chaque source (‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ = expert reconnu)
   ‚úì Identifie biais potentiels et angles morts
   ‚úì Compare perspectives multiples (pour/contre/nuances)
   ‚úì Synth√®se intelligente √©liminant redondances

4Ô∏è‚É£ ANTICIPATION PROACTIVE:
   ‚úì Pr√©dit 5-7 questions de suivi probables
   ‚úì Identifie informations manquantes critiques
   ‚úì D√©tecte implications long-terme
   ‚úì Propose extensions cr√©atives pertinentes

5Ô∏è‚É£ G√âN√âRATION STRUCTUR√âE:
   ‚úì R√©ponse directe imm√©diate (TL;DR)
   ‚úì Explication d√©taill√©e en sections logiques
   ‚úì Exemples concrets et cas d'usage
   ‚úì Visualisations (tableaux, listes, sch√©mas)
   ‚úì Code ex√©cutable si applicable

6Ô∏è‚É£ VALIDATION & V√âRIFICATION:
   ‚úì Double-check faits contre sources multiples
   ‚úì Test logique coh√©rence interne
   ‚úì Validation code (syntaxe + ex√©cution)
   ‚úì Signale incertitudes avec transparence

7Ô∏è‚É£ ENRICHISSEMENT CONTEXTUEL:
   ‚úì Ajoute d√©finitions termes techniques
   ‚úì Contexte historique si pertinent
   ‚úì Comparaisons internationales/culturelles
   ‚úì Statistiques et donn√©es chiffr√©es r√©centes

8Ô∏è‚É£ SUGGESTIONS INTELLIGENTES:
   ‚úì 3 questions approfondissement pertinentes
   ‚úì 2 perspectives alternatives int√©ressantes
   ‚úì 1-2 ressources compl√©mentaires recommand√©es
   ‚úì Actions concr√®tes sugg√©r√©es ("Et si vous...")

9Ô∏è‚É£ ADAPTATION DYNAMIQUE:
   ‚úì Ajuste complexit√© selon niveau utilisateur
   ‚úì D√©tecte frustration ‚Üí simplifie
   ‚úì D√©tecte expertise ‚Üí approfondit
   ‚úì Switch langue si n√©cessaire (FR/EN)

üîü AM√âLIORATION CONTINUE:
   ‚úì Apprend des interactions pr√©c√©dentes
   ‚úì M√©morise pr√©f√©rences utilisateur
   ‚úì Auto-critique et am√©lioration r√©ponses
   ‚úì Sugg√®re am√©liorations processus

üìö OUTILS DISPONIBLES (21+):
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚îÇ üîç RECHERCHE ILLIMIT√âE:
‚îú‚îÄ Local_Knowledge_Base: FOUILLE GLOBALE documents (AUCUNE limite k)
‚îú‚îÄ Hybrid_Search: Combinaison locale + web + historique (ILLIMIT√â)
‚îú‚îÄ Web_Search: Internet temps r√©el (10-50 r√©sultats analys√©s)
‚îú‚îÄ Web_Search_Detailed: Sources compl√®tes avec URLs
‚îú‚îÄ Current_News_Search: Actualit√©s derni√®res 24h-7j
‚îî‚îÄ Chat_History_Search: Continuit√© conversationnelle

‚îÇ ü§ñ IA SP√âCIALIS√âES:
‚îú‚îÄ AI_Code_Generator: DeepSeek-Coder (meilleur que GPT pour code)
‚îú‚îÄ AI_Plot_Generator: Graphiques scientifiques professionnels
‚îú‚îÄ Image_Analyzer: Vision IA pour images
‚îú‚îÄ Entity_Extractor: NER extraction entit√©s
‚îî‚îÄ Binary_Analysis: Analyse fichiers binaires avanc√©e

‚îÇ üé® G√âN√âRATION CR√âATIVE:
‚îú‚îÄ Text_To_Image_Generator: FLUX/Stable Diffusion
‚îú‚îÄ Text_To_Video_Generator: Vid√©os IA
‚îú‚îÄ Text_To_Audio_Generator: Musique/Audio IA
‚îú‚îÄ Text_To_3D_Generator: Mod√®les 3D
‚îî‚îÄ Image_To_3D_Generator: 3D depuis photos

‚îÇ üìä ANALYSE & OUTILS:
‚îú‚îÄ Smart_Content_Extractor: Extraction web compl√®te
‚îú‚îÄ Text_Summarizer: R√©sum√©s intelligents
‚îú‚îÄ Language_Translator: FR‚ÜîEN
‚îú‚îÄ ERT_Interpretation: G√©ophysique
‚îî‚îÄ OSM_Route_Calculator: Navigation GPS
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üí° PATTERNS DE R√âPONSE (60+ PROMPTS INT√âGR√âS):

ÔøΩÔ∏è CONVERSATIONNEL (ChatGPT-style):
‚Ä¢ "Excellente question ! Laisse-moi te d√©composer √ßa..."
‚Ä¢ "Je comprends exactement ce que tu cherches..."
‚Ä¢ "Voici ce qui est int√©ressant √† ce sujet..."
‚Ä¢ "Permets-moi d'ajouter une nuance importante..."
‚Ä¢ "Tu touches un point crucial ici..."

üß† ANALYTIQUE (Claude-style):
‚Ä¢ "Examinons cette question sous plusieurs angles..."
‚Ä¢ "Il est important de consid√©rer les implications suivantes..."
‚Ä¢ "Voici une analyse structur√©e en 3 parties..."
‚Ä¢ "Je dois souligner quelques consid√©rations √©thiques..."
‚Ä¢ "Contextuellement, il faut noter que..."

‚ö° CR√âATIF (Grok-style):
‚Ä¢ "Plot twist: la r√©ponse est plus fascinante que pr√©vu..."
‚Ä¢ "Fun fact qui va te surprendre..."
‚Ä¢ "Spoiler alert: c'est contre-intuitif mais..."
‚Ä¢ "Imagine un monde o√π..."
‚Ä¢ "Voici un angle auquel personne ne pense..."

üéì EXPERT (GPT-4-style):
‚Ä¢ "D'un point de vue technique pr√©cis..."
‚Ä¢ "Les donn√©es empiriques montrent que..."
‚Ä¢ "Selon la litt√©rature acad√©mique r√©cente (2023-2025)..."
‚Ä¢ "Une analyse rigoureuse r√©v√®le..."
‚Ä¢ "M√©thodologiquement, l'approche optimale consiste √†..."

üìã STRUCTURES TYPES:

A) R√âPONSE RAPIDE:
"üéØ **R√©ponse Directe**: [1-2 phrases essentielles]

üìñ **Explication**:
[D√©veloppement structur√©]

üìä **Sources**: [X sources v√©rifi√©es, confiance ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ]

üí° **Suggestions**:
1. [Question approfondissement]
2. [Perspective alternative]
3. [Action concr√®te]"

B) R√âPONSE TECHNIQUE:
"‚öôÔ∏è **Solution Technique**:
```[langage]
[code test√© et comment√©]
```

üìù **Explication ligne par ligne**:
[D√©tails impl√©mentation]

‚úÖ **Validation**: [Tests effectu√©s]

üîß **Alternatives**: [2-3 approches diff√©rentes]

üí° **Prochaines √©tapes**: [Suggestions am√©lioration]"

C) R√âPONSE CR√âATIVE:
"‚ú® **Concept Principal**: [Id√©e centrale]

üé® **Variations Cr√©atives**:
1. [Option A - classique]
2. [Option B - innovante]
3. [Option C - audacieuse]

üöÄ **Impl√©mentation**: [√âtapes concr√®tes]

üí° **Inspirations**: [R√©f√©rences pertinentes]"

D) R√âPONSE COMPARATIVE:
"üìä **Comparaison D√©taill√©e**:

| Crit√®re | Option A | Option B | Option C |
|---------|----------|----------|----------|
[Tableau complet]

üèÜ **Recommandation**: [Meilleur choix selon contexte]

‚öñÔ∏è **Trade-offs**: [Avantages/Inconv√©nients]

üí° **Conseil personnalis√©**: [Selon situation utilisateur]"

üéØ CONSIGNES D'EX√âCUTION:

‚úì RECHERCHE ILLIMIT√âE: Utilise k=100+ pour fouille globale (pas de limite!)
‚úì MULTI-SOURCES: Combine MINIMUM 3 sources diff√©rentes
‚úì V√âRIFICATION: Croise-v√©rifie informations contradictoires
‚úì CITATIONS: Indique sources avec niveau confiance
‚úì SUGGESTIONS: TOUJOURS 3+ questions de suivi pertinentes
‚úì ADAPTABILIT√â: Ajuste ton/complexit√© selon utilisateur
‚úì TRANSPARENCE: Signale incertitudes et limites
‚úì PROACTIVIT√â: Anticipe besoins non exprim√©s
‚úì CR√âATIVIT√â: Propose solutions innovantes
‚úì EMPATHIE: Comprends contexte √©motionnel

OUTILS: {tools}

EX√âCUTION:
Question: {input}
Thought: [Analyse multi-niveau: Que veut vraiment l'utilisateur? Quelles sources combiner? Quelle strat√©gie optimale? Quelles suggestions proposer?]
Action: [outil_optimal avec recherche ILLIMIT√âE]
Action Input: [requ√™te optimis√©e]
Observation: [r√©sultat]
... [R√©p√©ter jusqu'√† synth√®se compl√®te de TOUTES sources pertinentes]
Thought: J'ai maintenant une vue GLOBALE compl√®te avec sources multiples v√©rifi√©es
Final Answer:
üéØ **R√©ponse Directe**: [Essentiel en 1-2 phrases]

üìñ **D√©veloppement D√©taill√©**:
[Sections structur√©es avec exemples]

üìä **Sources V√©rifi√©es**: 
‚Ä¢ [Source 1 - ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ - Type]
‚Ä¢ [Source 2 - ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ - Type]
‚Ä¢ [Source 3+ - ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ - Type]

üí° **Suggestions Intelligentes**:
1. üîç [Question approfondissement pertinente]
2. üåü [Perspective alternative int√©ressante]  
3. üöÄ [Action concr√®te recommand√©e]

{agent_scratchpad}""")

    
    # V√©rifier si les agents sont disponibles
    if create_react_agent is None:
        st.warning("‚ö†Ô∏è Agents non disponibles - Mode simplifi√© activ√©")
        return None
    
    # Cr√©er l'agent avec LangChain 1.0+ / LangGraph V1.0+
    # create_agent retourne directement un ex√©cuteur compil√©
    try:
        agent_executor = create_react_agent(llm, tools)
        st.write(f"‚úÖ Agent cr√©√© avec {len(tools)} outils disponibles")
        return agent_executor
    except Exception as e:
        st.error(f"‚ùå Erreur cr√©ation agent: {e}")
        return None
# Alias pour compatibilit√©
def create_agent(model_name, vectordb, graph, pois):
    """Version simplifi√©e pour compatibilit√©"""
    return create_enhanced_agent(model_name, vectordb, graph, pois)
# ========================================
# Fonctions OSM et Graphe Routier
# ========================================
def haversine(lon1, lat1, lon2, lat2):
    """Calcul distance haversine en m√®tres"""
    R = 6371000
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi/2.0)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2.0)**2
    return R * (2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)))
class RoadPOIHandler(osmium.SimpleHandler):
    """Handler pour extraire routes et POIs depuis OSM"""
    def __init__(self):
        super().__init__()
        self.graph = nx.Graph()
        self.pois = []
    def node(self, n):
        """Extraire les POIs (points d'int√©r√™t)"""
        if n.location.valid() and n.tags:
            name = n.tags.get('name', '')
            amenity = n.tags.get('amenity', '')
            if name or amenity:
                self.pois.append({
                    'name': name,
                    'amenity': amenity,
                    'lon': n.location.lon,
                    'lat': n.location.lat,
                    'tags': dict(n.tags)
                })
    def way(self, w):
        """Extraire les routes"""
        if 'highway' in w.tags:
            coords = []
            for n in w.nodes:
                if n.location.valid():
                    coords.append((n.location.lon, n.location.lat))
            for i in range(len(coords)-1):
                lon1, lat1 = coords[i]
                lon2, lat2 = coords[i+1]
                n1, n2 = (lon1, lat1), (lon2, lat2)
                dist = haversine(lon1, lat1, lon2, lat2)
                self.graph.add_node(n1, x=lon1, y=lat1)
                self.graph.add_node(n2, x=lon2, y=lat2)
                self.graph.add_edge(n1, n2, length=dist, highway=w.tags.get("highway"))
def trouver_noeud_plus_proche(lon, lat, graph):
    """Trouve le n≈ìud du graphe le plus proche"""
    min_dist = float("inf")
    closest_node = None
    for node, data in graph.nodes(data=True):
        nlon, nlat = float(data["x"]), float(data["y"])
        dist = haversine(lon, lat, nlon, nlat)
        if dist < min_dist:
            min_dist = dist
            closest_node = node
    return closest_node
def chercher_poi_par_nom(nom, pois_list):
    """Recherche un POI par nom"""
    nom_lower = nom.lower()
    for poi in pois_list:
        if nom_lower in poi['name'].lower() or nom_lower in poi['amenity'].lower():
            return poi
    return None
def generer_carte_trajet(graph, path, pois_list, start_poi=None, end_poi=None):
    """G√©n√®re une carte 2D du trajet"""
    fig, ax = plt.subplots(figsize=(12, 10))
    # Dessiner le graphe en arri√®re-plan
    for edge in list(graph.edges())[:1000]: # Limiter pour la performance
        node1, node2 = edge
        x1, y1 = node1[0], node1[1]
        x2, y2 = node2[0], node2[1]
        ax.plot([x1, x2], [y1, y2], 'lightgray', alpha=0.3, linewidth=0.5)
    # Dessiner le trajet
    if path and len(path) > 1:
        path_x = [node[0] for node in path]
        path_y = [node[1] for node in path]
        ax.plot(path_x, path_y, 'red', linewidth=3, label='Trajet')
        # Marquer d√©but et fin
        ax.scatter(path_x[0], path_y[0], color='green', s=100, label='D√©part', zorder=5)
        ax.scatter(path_x[-1], path_y[-1], color='red', s=100, label='Arriv√©e', zorder=5)
    # Ajouter quelques POIs
    for poi in pois_list[:20]:
        if poi['name']:
            ax.scatter(poi['lon'], poi['lat'], color='blue', s=20, alpha=0.6)
    ax.set_xlabel('Longitude')
    ax.set_ylabel('Latitude')
    ax.set_title('Trajet calcul√© sur la carte OSM')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal')
    # Sauvegarder en m√©moire
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    buf.seek(0)
    plt.close()
    return buf
def calculer_trajet(question, graph, pois_list):
    """Calcule un trajet bas√© sur une question textuelle"""
    if not graph or not pois_list:
        return None, "‚ùå Graphe ou POIs non disponibles", None
    # Utiliser LLM pour extraire d√©part et arriv√©e
    try:
        client = create_client()
        prompt = f"""Extraie le lieu de d√©part et le lieu d'arriv√©e de cette question de trajet.
Question: {question}
R√©ponds au format exact:
D√©part: [nom du lieu de d√©part]
Arriv√©e: [nom du lieu d'arriv√©e]"""
        messages = [{"role": "user", "content": prompt}]
        response = client.chat_completion(
            messages=messages,
            model=WORKING_MODELS["Llama 3.1 8B (√âquilibr√©)"],
            max_tokens=100,
            temperature=0.1
        )
        extraction = response.choices[0].message.content
        start_line = [line for line in extraction.split('\n') if line.startswith('D√©part: ')]
        end_line = [line for line in extraction.split('\n') if line.startswith('Arriv√©e: ')]
        if start_line and end_line:
            start_place = start_line[0].replace('D√©part: ', '').strip()
            end_place = end_line[0].replace('Arriv√©e: ', '').strip()
        else:
            return None, "‚ùå Impossible d'extraire les lieux de la question.", None
    except Exception as e:
        st.write(f"‚ùå Erreur extraction LLM: {e}")
        return None, "‚ùå Erreur lors de l'extraction des lieux.", None
    start_poi = chercher_poi_par_nom(start_place, pois_list)
    end_poi = chercher_poi_par_nom(end_place, pois_list)
    if not start_poi or not end_poi:
        return None, f"‚ùå Impossible de trouver les lieux: {start_place} ou {end_place}.", None
    # Trouver les n≈ìuds dans le graphe
    start_node = trouver_noeud_plus_proche(start_poi['lon'], start_poi['lat'], graph)
    end_node = trouver_noeud_plus_proche(end_poi['lon'], end_poi['lat'], graph)
    if not start_node or not end_node:
        return None, "‚ùå Impossible de trouver les n≈ìuds dans le graphe routier.", None
    try:
        # Calculer le chemin
        path = nx.shortest_path(graph, source=start_node, target=end_node, weight="length")
        # Calculer la distance
        distance_totale = 0
        for i in range(len(path)-1):
            distance_totale += graph[path[i]][path[i+1]]['length']
        # G√©n√©rer la carte
        carte_buf = generer_carte_trajet(graph, path, pois_list, start_poi, end_poi)
        # R√©ponse textuelle
        reponse = f"""üó∫Ô∏è **Trajet calcul√©**
üìç **D√©part**: {start_poi['name']} ({start_poi['amenity']})
üéØ **Arriv√©e**: {end_poi['name']} ({end_poi['amenity']})
üìè **Distance**: {distance_totale/1000:.2f} km
‚è±Ô∏è **Temps estim√©**: {int(distance_totale/83.33):.0f} min √† pied | {int(distance_totale/833.33):.0f} min en voiture
üõ£Ô∏è **√âtapes**: {len(path)} points"""
        return carte_buf, reponse, {
            'start': start_poi,
            'end': end_poi,
            'distance': distance_totale,
            'path_length': len(path)
        }
    except nx.NetworkXNoPath:
        return None, f"‚ùå Aucun chemin trouv√© entre {start_poi['name']} et {end_poi['name']}", None
    except Exception as e:
        return None, f"‚ùå Erreur: {str(e)}", None
# ========================================
# Fonctions utilitaires pour images
# ========================================
def fig_to_pil(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    buf.seek(0)
    plt.close(fig)
    return Image.open(buf)
def df_to_html(df, max_rows=10):
    # R√©duire le tableau si trop long
    if len(df) > max_rows:
        summary_row = pd.DataFrame({col: ['...'] for col in df.columns})
        df = pd.concat([df.head(max_rows // 2), summary_row, df.tail(max_rows // 2)])
    return df.to_html(index=False, escape=False)
# ========================================
# Fonctions Image Analysis
# ========================================
def classify_soil(image: np.ndarray):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    mean_hue = np.mean(hsv[:,:,0])
    mean_sat = np.mean(hsv[:,:,1])
    mean_val = np.mean(hsv[:,:,2])
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    texture_variance = np.var(gray)
    soil_type = "Inconnu"
    possible_contents = "Inconnu"
    possible_minerals = "Inconnu"
    if mean_val < 100 and texture_variance > 5000:
        soil_type = "Argileux (riche en mati√®re organique)"
        possible_contents = "Peut contenir de l'eau, nutriments, adapt√© aux cultures racines"
        possible_minerals = "Argiles comme kaolinite, illite; possible fer, aluminium"
    elif mean_sat > 100 and texture_variance < 3000:
        soil_type = "Sableux (drainant)"
        possible_contents = "Peut contenir peu d'eau, adapt√© aux plantes r√©sistantes √† la s√©cheresse"
        possible_minerals = "Quartz, feldspath; silice abondante"
    elif mean_hue > 20 and mean_hue < 40:
        soil_type = "Limoneux (√©quilibr√©)"
        possible_contents = "Peut contenir min√©raux, bon pour l'agriculture g√©n√©rale"
        possible_minerals = "Silt avec mica, quartz; calcium, potassium"
    # Graphisme : Histogramme des couleurs HSV
    fig, ax = plt.subplots()
    ax.hist(hsv[:,:,0].ravel(), bins=50, color='b', alpha=0.5, label='Hue')
    ax.hist(hsv[:,:,1].ravel(), bins=50, color='g', alpha=0.5, label='Saturation')
    ax.hist(hsv[:,:,2].ravel(), bins=50, color='r', alpha=0.5, label='Value')
    ax.set_title('Histogramme des Composantes HSV')
    ax.legend()
    hist_img = fig_to_pil(fig)
    # Tableau des metrics
    metrics_df = pd.DataFrame({
        'M√©trique': ['Hue Moyenne', 'Saturation Moyenne', 'Valeur Moyenne', 'Variance Texture'],
        'Valeur': [mean_hue, mean_sat, mean_val, texture_variance],
        'Explication': ['Moyenne de la teinte', 'Moyenne de la saturation des couleurs', 'Moyenne de la luminosit√©', 'Variance de la texture pour rugosit√©']
    })
    metrics_html = df_to_html(metrics_df)
    return {
        "soil_type": soil_type,
        "possible_contents": possible_contents,
        "possible_minerals": possible_minerals
    }, hist_img, metrics_html
def simulate_infrared(image: np.ndarray):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    ir_img = cv2.applyColorMap(gray, cv2.COLORMAP_JET)
    fig, ax = plt.subplots()
    ax.imshow(cv2.cvtColor(ir_img, cv2.COLOR_BGR2RGB))
    ax.set_title('Simulation Infrarouge (Colormap JET)')
    ax.axis('off')
    ir_pil = fig_to_pil(fig)
    # Analyse simple (fake temp based on intensity)
    mean_intensity = np.mean(gray)
    ir_analysis = f"Simulation IR: Intensit√© moyenne {mean_intensity:.2f} (plus rouge = plus chaud, bleu = plus froid)"
    return ir_pil, ir_analysis
def detect_objects(image: np.ndarray, scale_factor=0.1):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    img_with_contours = image.copy()
    dimensions = []
    types = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        if w < 10 or h < 10: continue # skip small
        cv2.rectangle(img_with_contours, (x, y), (x+w, y+h), (0, 255, 0), 2)
        w_m = w * scale_factor
        h_m = h * scale_factor
        aspect = w / h if h != 0 else 0
        if aspect > 5: obj_type = 'Route'
        elif aspect < 0.2: obj_type = 'Cl√¥ture'
        elif 0.5 < aspect < 2: obj_type = 'B√¢timent'
        else: obj_type = 'Autre'
        dimensions.append((w_m, h_m))
        types.append(obj_type)
        cv2.putText(img_with_contours, f"{obj_type}: {w_m:.4f}m x {h_m:.4f}m", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
    num_objects = len(contours)
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.imshow(cv2.cvtColor(img_with_contours, cv2.COLOR_BGR2RGB))
    ax.set_title(f"Objets D√©tect√©s avec Contours ({num_objects})")
    ax.axis('off')
    obj_img = fig_to_pil(fig)
    if dimensions:
        dim_df = pd.DataFrame({
            'Type': types,
            'Largeur (m)': [d[0] for d in dimensions],
            'Hauteur (m)': [d[1] for d in dimensions],
            'Explication': ['Dimension estim√©e avec contours OpenCV' for _ in types]
        })
        dim_html = df_to_html(dim_df)
    else:
        dim_html = ""
    return num_objects, obj_img, dim_html
def detect_fences(image: np.ndarray, scale_factor=0.1):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 100, 200)
    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=100, maxLineGap=10)
    img_with_lines = image.copy()
    lengths = []
    if lines is not None:
        line_list = [line[0] for line in lines]
        filtered_lines = [l for l in line_list if abs(l[0] - l[2]) < 10 or abs(l[1] - l[3]) < 10 or abs((l[1]-l[3]) / (l[0]-l[2] + 1e-5)) < 0.1 or abs((l[1]-l[3]) / (l[0]-l[2] + 1e-5)) > 10]
        line_lengths = [np.sqrt((x2 - x1)**2 + (y2 - y1)**2) for x1,y1,x2,y2 in filtered_lines]
        sorted_indices = np.argsort(line_lengths)[::-1]
        sorted_lines = [filtered_lines[i] for i in sorted_indices]
        for x1,y1,x2,y2 in sorted_lines:
            cv2.line(img_with_lines, (x1, y1), (x2, y2), (255, 0, 0), 2)
            length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) * scale_factor
            lengths.append(length)
            mid_x = (x1 + x2) // 2
            mid_y = (y1 + y2) // 2
            cv2.putText(img_with_lines, f"{length:.4f}m", (mid_x, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.imshow(cv2.cvtColor(img_with_lines, cv2.COLOR_BGR2RGB))
    ax.set_title(f"Cl√¥tures/Bordures D√©tect√©es avec ({len(lengths)})")
    ax.axis('off')
    fence_img = fig_to_pil(fig)
    if lengths:
        fence_df = pd.DataFrame({
            'Longueur (m)': lengths,
            'Explication': ['Longueur de bordure filtr√©e et tri√©e pour pr√©cision' for _ in lengths]
        })
        fence_html = df_to_html(fence_df)
    else:
        fence_html = ""
    return len(lengths), fence_img, fence_html
def detect_anomalies(image: np.ndarray):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 100, 200)
    num_edges = np.sum(edges > 0)
    mean_variance = np.mean(cv2.Laplacian(gray, cv2.CV_64F).var())
    anomalies = []
    if num_edges > 10000:
        anomalies.append("Anomalies structurelles d√©tect√©es (ex. : fissures, d√©fauts)")
    if mean_variance > 500:
        anomalies.append("Textures inhabituelles (ex. : zones irr√©guli√®res)")
    # Simulation photogramm√©trie basique avec Open3D (si disponible)
    num_points = 0
    if OPEN3D_AVAILABLE:
        depth = np.random.rand(*gray.shape) * 255
        point_cloud = o3d.geometry.PointCloud.create_from_rgbd_image(
            o3d.geometry.RGBDImage.create_from_color_and_depth(
                o3d.geometry.Image(image),
                o3d.geometry.Image(depth.astype(np.float32))
            ),
            o3d.camera.PinholeCameraIntrinsic(640, 480, 525, 525, 320, 240)
        )
        num_points = len(point_cloud.points)
    else:
        # Fallback: estimation simplifi√©e du nombre de points
        num_points = gray.shape[0] * gray.shape[1]
    # Graphisme : Histogramme des variances
    fig, ax = plt.subplots()
    ax.hist(cv2.Laplacian(gray, cv2.CV_64F).ravel(), bins=50)
    ax.set_title('Histogramme des Variances Locales (Anomalies)')
    var_hist_img = fig_to_pil(fig)
    # Tableau des metrics anomalies
    anomaly_df = pd.DataFrame({
        'M√©trique': ['Nombre de Bords', 'Variance Moyenne', 'Points dans Point Cloud'],
        'Valeur': [num_edges, mean_variance, num_points],
        'Explication': ['Indique complexit√© structurelle (haut = anomalies)', 'Mesure irr√©gularit√©s texture', 'Simulation 3D pour volume']
    })
    anomaly_html = df_to_html(anomaly_df)
    anomaly_desc_df = pd.DataFrame({
        'Anomalie': anomalies,
        'Explication': ['D√©fauts potentiels dans le terrain ou structures' for _ in anomalies]
    })
    anomaly_desc_html = df_to_html(anomaly_desc_df)
    return anomalies, var_hist_img, anomaly_html, anomaly_desc_html
def advanced_analyses(image: np.ndarray):
    analyses = {}
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    edges = cv2.Canny(gray, 100, 200)
    adv_images = []
    adv_tables = []
    # 1. Analyse G√©ologique
    kmeans = KMeans(n_clusters=3).fit(gray.reshape(-1, 1))
    clustered = kmeans.labels_.reshape(gray.shape)
    analyses['G√©ologique'] = 'Clusters de textures : ' + str(np.unique(kmeans.labels_))
    fig, ax = plt.subplots()
    ax.imshow(clustered, cmap='viridis')
    ax.set_title('Analyse G√©ologique: Clustering Textures')
    ax.axis('off')
    adv_images.append(fig_to_pil(fig))
    geo_df = pd.DataFrame({'Cluster': np.unique(kmeans.labels_), 'Compte': np.bincount(kmeans.labels_), 'Explication': ['Groupe de texture g√©ologique' for _ in np.unique(kmeans.labels_)]})
    adv_tables.append(df_to_html(geo_df))
    # 2. Analyse Hydrologique
    blue_mask = cv2.inRange(hsv, (100, 50, 50), (130, 255, 255))
    water_area = np.sum(blue_mask > 0) / blue_mask.size * 100
    analyses['Hydrologique'] = f'Pourcentage eau : {water_area:.2f}%'
    fig, ax = plt.subplots()
    ax.imshow(blue_mask, cmap='gray')
    ax.set_title('Analyse Hydrologique: Masque Eau')
    ax.axis('off')
    adv_images.append(fig_to_pil(fig))
    hydro_df = pd.DataFrame({'M√©trique': ['Pourcentage Eau'], 'Valeur': [water_area], 'Explication': ['Zone potentielle pour ressources hydriques']})
    adv_tables.append(df_to_html(hydro_df))
    return analyses, {}, adv_images, adv_tables
def process_image(uploaded_file):
    image = Image.open(BytesIO(uploaded_file))
    img_array = np.array(image)
    proc_images = [image]
    captions = ['Image Originale']
    tables_html = []
    # IR
    ir_pil, ir_analysis = simulate_infrared(img_array)
    proc_images.append(ir_pil)
    captions.append('Simulation Infrarouge')
    tables_html.append('<h3>Analyse IR</h3><p>' + ir_analysis + '</p>')
    # Soil
    soil, hist_img, metrics_html = classify_soil(img_array)
    proc_images.append(hist_img)
    captions.append('Histogramme HSV')
    tables_html.append('<h3>M√©triques Sol</h3>' + metrics_html)
    # Objects
    num_objects, obj_img, dim_html = detect_objects(img_array)
    proc_images.append(obj_img)
    captions.append('Objets D√©tect√©s')
    if dim_html:
        tables_html.append('<h3>Dimensions Objets</h3>' + dim_html)
    # Fences
    num_fences, fence_img, fence_html = detect_fences(img_array)
    proc_images.append(fence_img)
    captions.append('Cl√¥tures D√©tect√©es')
    if fence_html:
        tables_html.append('<h3>Longueurs Cl√¥tures</h3>' + fence_html)
    # Anomalies
    anomalies, var_hist_img, anomaly_html, anomaly_desc_html = detect_anomalies(img_array)
    proc_images.append(var_hist_img)
    captions.append('Histogramme Variances')
    tables_html.append('<h3>M√©triques Anomalies</h3>' + anomaly_html)
    # Advanced
    analyses, predictions, adv_images, adv_tables = advanced_analyses(img_array)
    proc_images += adv_images[:5] # Limiter le nombre d'images
    captions += ['Analyse Avanc√©e'] * len(adv_images[:5])
    tables_html += adv_tables[:3] # Limiter le nombre de tableaux
    analysis_data = {
        "soil": soil,
        "ir_analysis": ir_analysis,
        "num_objects": num_objects,
        "num_fences": num_fences,
        "anomalies": anomalies,
        "analyses": analyses,
        "predictions": predictions
    }
    tables_str = '<br>'.join(tables_html)
    return analysis_data, proc_images, tables_str
def improve_analysis_with_llm(analysis_data, model_name):
    prompt = f"""Analyse les donn√©es suivantes de l'image et fournis une analyse naturelle am√©lior√©e:
DONN√âES:
{json.dumps(analysis_data, indent=2)}
ANALYSE AM√âLIOR√âE:"""
    try:
        client = create_client()
        messages = [{"role": "user", "content": prompt}]
        response = client.chat_completion(
            messages=messages,
            model=model_name,
            max_tokens=800,
            temperature=0.5
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"‚ùå Erreur: {str(e)}"
def update_agent(model_choice, vectordb, graph, pois, chat_vectordb=None): # AJOUT M√âMOIRE VECTORIELLE
    model_name = WORKING_MODELS[model_choice]
    agent = create_enhanced_agent(model_name, vectordb, graph, pois, chat_vectordb)
    cache_info = get_cache_stats()
    return model_name, agent, cache_info
def handle_clear_cache():
    """Vide le cache web"""
    try:
        if os.path.exists(WEB_CACHE_PATH):
            os.remove(WEB_CACHE_PATH)
        return "‚úÖ Cache web vid√©"
    except Exception as e:
        return f"‚ùå Erreur: {e}"
def highlight_important_words(text):
    """Met en √©vidence les mots importants avec effet scintillante et tooltip"""
    # Mots-cl√©s simples pour exemple (peut √™tre √©tendu avec NER)
    important_keywords = ['important', 'cl√©', 'essentiel', 'critique', 'principal', 'trajet', 'p√©trole', 'topographie']
    for keyword in important_keywords:
        text = re.sub(rf'\b({keyword})\b', r'<span class="sparkle-word" title="\1: Terme cl√© pour la compr√©hension du contexte">\1</span>', text, flags=re.IGNORECASE)
    return text
def install_code_model():
    """Installe un mod√®le de code l√©ger et performant (CodeLlama-7B ou DeepSeek-Coder-1.3B)"""
    try:
        st.info("üì¶ T√©l√©chargement de DeepSeek-Coder-1.3B-Instruct (mod√®le l√©ger ~1.3GB)...")
        
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"
        cache_dir = os.path.expanduser("~/.cache/huggingface/code_models")
        
        # T√©l√©charger et cacher le mod√®le
        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        st.success(f"‚úÖ Mod√®le de code install√© avec succ√®s!")
        st.info(f"üìç Emplacement: {cache_dir}")
        st.info("üöÄ Utilisation: Le mod√®le sera automatiquement utilis√© en Mode Code Expert")
        
        return True
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'installation: {e}")
        st.warning("üí° Alternative: Le mode utilisera AI_Code_Generator par d√©faut")
        return False

# ========================================
# ANALYSEUR DE FICHIERS TEMPS R√âEL
# ========================================

def analyze_uploaded_file_realtime(uploaded_file, chat_history, agent, vectordb, graph, pois, web_enabled):
    """
    Analyse PROACTIVE et INTELLIGENTE d'un fichier par Kibali
    1. Analyse technique compl√®te automatique
    2. Recherche web contextuelle pour enrichir
    3. Suggestions d'actions possibles
    4. Explications d√©taill√©es en temps r√©el
    """
    import io
    from PIL import Image
    
    # Cr√©er un conteneur pour l'analyse en temps r√©el
    analysis_container = st.container()
    
    with analysis_container:
        st.markdown("### ü§ñ Kibali analyse votre fichier...")
        
        # === √âTAPE 1: ANALYSE TECHNIQUE ===
        with st.status("ÔøΩ Analyse technique en cours...", expanded=True) as status:
            file_extension = uploaded_file.name.split('.')[-1].lower()
            file_size = uploaded_file.size
            file_bytes = uploaded_file.read()
            uploaded_file.seek(0)
            
            st.write(f"üìÑ **{uploaded_file.name}**")
            st.write(f"üìä **Taille:** {file_size:,} octets ({file_size/1024:.2f} KB)")
            
            # D√©tection magic bytes
            magic_bytes = file_bytes[:16]
            magic_hex = ' '.join(f'{b:02X}' for b in magic_bytes[:8])
            st.code(f"Signature: {magic_hex}", language="text")
            
            # ‚≠ê ANALYSE SCIENTIFIQUE IMM√âDIATE pour fichiers binaires
            scientific_context = ""
            if file_extension in ['dat', 'bin']:
                st.write("üî¨ **D√©tection**: Possible fichier ERT - Activation capacit√©s g√©ophysiques")
                scientific_context = "GEOPHYSICS_MODE"
            
            status.update(label=f"‚úÖ Scan initial termin√©{' - Mode g√©ophysique' if scientific_context else ''}", state="complete")
        
        # Donn√©es extraites pour Kibali
        extracted_data = {
            "filename": uploaded_file.name,
            "extension": file_extension,
            "size": file_size,
            "scientific_domain": scientific_context if scientific_context else "general",
            "content": "",
            "metadata": {}
        }
        
        # === √âTAPE 2: ANALYSE SP√âCIALIS√âE SELON TYPE ===
        technical_summary = ""
        
        # IMAGES
        if file_extension in ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp', 'svg']:
            with st.status("üñºÔ∏è Analyse d'image...", expanded=True) as status:
                try:
                    img = Image.open(io.BytesIO(file_bytes))
                    st.image(img, caption=uploaded_file.name, use_container_width=True)
                    
                    extracted_data["metadata"] = {
                        "dimensions": f"{img.size[0]}x{img.size[1]}",
                        "mode": img.mode,
                        "format": img.format,
                        "pixels": img.size[0] * img.size[1]
                    }
                    
                    technical_summary = f"Image {img.format} de {img.size[0]}x{img.size[1]} pixels, mode {img.mode}"
                    st.success(f"‚úÖ {technical_summary}")
                    status.update(label="‚úÖ Image analys√©e", state="complete")
                except Exception as e:
                    technical_summary = f"Image {file_extension}"
                    st.warning(f"‚ö†Ô∏è Analyse partielle: {e}")
                    status.update(label="‚ö†Ô∏è Analyse partielle", state="complete")
        
        # PDF
        elif file_extension == 'pdf':
            with st.status("üìÑ Extraction PDF...", expanded=True) as status:
                try:
                    import fitz
                    doc = fitz.open(stream=file_bytes, filetype="pdf")
                    num_pages = doc.page_count
                    
                    # Extraire tout le texte
                    full_text = ""
                    for page_num in range(num_pages):
                        full_text += doc[page_num].get_text()
                    
                    word_count = len(full_text.split())
                    extracted_data["content"] = full_text[:5000]  # 5000 premiers caract√®res
                    extracted_data["metadata"] = {
                        "pages": num_pages,
                        "words": word_count
                    }
                    
                    technical_summary = f"PDF de {num_pages} pages contenant {word_count} mots"
                    st.success(f"‚úÖ {technical_summary}")
                    
                    with st.expander("üìñ Aper√ßu du contenu"):
                        st.text(full_text[:800] + "...")
                    
                    status.update(label="‚úÖ PDF extrait", state="complete")
                except Exception as e:
                    technical_summary = f"PDF de taille {file_size/1024:.2f} KB"
                    st.warning(f"‚ö†Ô∏è Extraction partielle: {e}")
                    status.update(label="‚ö†Ô∏è Extraction partielle", state="complete")
        
        # CSV/EXCEL
        elif file_extension in ['csv', 'xlsx', 'xls']:
            with st.status("ÔøΩ Analyse des donn√©es tabulaires...", expanded=True) as status:
                try:
                    import pandas as pd
                    if file_extension == 'csv':
                        df = pd.read_csv(io.BytesIO(file_bytes))
                    else:
                        df = pd.read_excel(io.BytesIO(file_bytes))
                    
                    st.dataframe(df.head(10))
                    
                    # Statistiques
                    stats = df.describe().to_string()
                    extracted_data["content"] = f"Colonnes: {list(df.columns)}\n\nStatistiques:\n{stats}"
                    extracted_data["metadata"] = {
                        "rows": len(df),
                        "columns": len(df.columns),
                        "column_names": list(df.columns),
                        "dtypes": df.dtypes.to_dict()
                    }
                    
                    technical_summary = f"Tableau de {len(df)} lignes √ó {len(df.columns)} colonnes"
                    st.success(f"‚úÖ {technical_summary}")
                    status.update(label="‚úÖ Donn√©es charg√©es", state="complete")
                except Exception as e:
                    technical_summary = f"Fichier tabulaire {file_extension}"
                    st.warning(f"‚ö†Ô∏è Chargement partiel: {e}")
                    status.update(label="‚ö†Ô∏è Chargement partiel", state="complete")
        
        # JSON
        elif file_extension == 'json':
            with st.status("üìã Parsing JSON...", expanded=True) as status:
                try:
                    import json
                    text_content = file_bytes.decode('utf-8')
                    data = json.loads(text_content)
                    
                    st.json(data)
                    
                    extracted_data["content"] = json.dumps(data, indent=2)[:5000]
                    extracted_data["metadata"] = {
                        "keys": list(data.keys()) if isinstance(data, dict) else "array",
                        "size": len(str(data))
                    }
                    
                    technical_summary = f"JSON contenant {len(data)} √©l√©ments"
                    st.success(f"‚úÖ {technical_summary}")
                    status.update(label="‚úÖ JSON pars√©", state="complete")
                except Exception as e:
                    technical_summary = "Fichier JSON"
                    st.warning(f"‚ö†Ô∏è Parsing partiel: {e}")
                    status.update(label="‚ö†Ô∏è Parsing partiel", state="complete")
        
        # TEXTE
        elif file_extension in ['txt', 'md', 'log', 'py', 'js', 'html', 'css', 'xml']:
            with st.status("üìù Lecture du texte...", expanded=True) as status:
                try:
                    text_content = file_bytes.decode('utf-8', errors='ignore')
                    lines = text_content.split('\n')
                    
                    extracted_data["content"] = text_content[:5000]
                    extracted_data["metadata"] = {
                        "lines": len(lines),
                        "characters": len(text_content)
                    }
                    
                    with st.expander("üìÑ Contenu"):
                        st.code(text_content[:1000], language=file_extension if file_extension in ['py', 'js', 'html', 'css'] else 'text')
                    
                    technical_summary = f"Fichier texte de {len(lines)} lignes"
                    st.success(f"‚úÖ {technical_summary}")
                    status.update(label="‚úÖ Texte lu", state="complete")
                except Exception as e:
                    technical_summary = "Fichier texte"
                    st.warning(f"‚ö†Ô∏è Lecture partielle: {e}")
                    status.update(label="‚ö†Ô∏è Lecture partielle", state="complete")
        
        # BINAIRE
        else:
            with st.status("üî¢ Analyse binaire...", expanded=True) as status:
                # üÜï D√âTECTION ET PARSING AUTOMATIQUE MULTI-FR√âQUENCES pour .dat
                if file_extension == 'dat':
                    st.write("üî¨ **D√©tection fichier .dat - Test parseur multi-fr√©quences...**")
                    
                    try:
                        # Sauvegarder temporairement le fichier
                        import tempfile
                        temp_dir = tempfile.gettempdir()
                        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                        
                        with open(temp_file_path, 'wb') as f:
                            f.write(file_bytes)
                        
                        # Tenter le parsing multi-fr√©quences
                        from multi_freq_ert_parser import MultiFreqERTParser
                        parser = MultiFreqERTParser()
                        df = parser.parse_multiple_files([temp_file_path])
                        
                        if not df.empty and len(df) > 0:
                            st.success(f"‚úÖ **PARSEUR MULTI-FR√âQUENCES ACTIV√â !**")
                            st.write(f"üìä {len(df)} mesures ERT charg√©es")
                            st.write(f"üì° {len(parser.frequencies)} fr√©quences d√©tect√©es")
                            st.write(f"üìç {len(parser.survey_points)} survey points")
                            
                            # Afficher aper√ßu de la structure
                            with st.expander("üìã Structure des donn√©es (5 premi√®res lignes)"):
                                st.dataframe(df[['project', 'survey_point', 'depth', 'frequency_MHz', 'resistivity']].head())
                            
                            # Coordonn√©es spatiales
                            coords_df = parser.get_coordinates_corrected()
                            with st.expander("üìê Coordonn√©es spatiales (X, Y, Z)"):
                                st.dataframe(coords_df[['x', 'y', 'z', 'resistivity', 'frequency_MHz']].head())
                            
                            # Sauvegarder dans session_state
                            st.session_state['multi_freq_data'] = df
                            st.session_state['multi_freq_coords'] = coords_df
                            st.session_state['multi_freq_parser'] = parser
                            
                            # Mettre √† jour extracted_data
                            extracted_data["metadata"] = {
                                "type": "ERT_MULTI_FREQ",
                                "num_measurements": len(df),
                                "num_frequencies": len(parser.frequencies),
                                "num_survey_points": len(parser.survey_points),
                                "frequencies_MHz": [float(f) for f in parser.frequencies[:10]],  # 10 premi√®res
                                "depth_range": (float(parser.metadata['depth_range'][0]), float(parser.metadata['depth_range'][1])),
                                "resistivity_range": (float(parser.metadata['resistivity_range'][0]), float(parser.metadata['resistivity_range'][1]))
                            }
                            
                            extracted_data["content"] = f"""Donn√©es ERT Multi-Fr√©quences:
- {len(df)} mesures
- {len(parser.frequencies)} fr√©quences (de {min(parser.frequencies):.2f} √† {max(parser.frequencies):.2f} MHz)
- {len(parser.survey_points)} survey points
- Profondeurs: {parser.metadata['depth_range'][0]:.1f} √† {parser.metadata['depth_range'][1]:.1f} m
- R√©sistivit√©s: {parser.metadata['resistivity_range'][0]:.2f} √† {parser.metadata['resistivity_range'][1]:.2f} Œ©¬∑m

Structure correcte valid√©e:
project | survey_point | depth | frequency_MHz | resistivity

Coordonn√©es spatiales (X,Y,Z) calcul√©es automatiquement.
"""
                            
                            technical_summary = f"Fichier ERT multi-fr√©quences: {len(df)} mesures, {len(parser.frequencies)} fr√©quences"
                            status.update(label="‚úÖ Fichier ERT pars√© avec succ√®s!", state="complete")
                            
                        else:
                            # Fallback vers analyse binaire classique
                            st.info("‚ÑπÔ∏è Format ERT non d√©tect√©, analyse binaire standard...")
                            entropy_result = entropy_analysis(file_bytes)
                            compression_result = compression_ratio(file_bytes)
                            pattern_result = pattern_recognition(file_bytes)
                            
                            extracted_data["metadata"] = {
                                "entropy": entropy_result,
                                "compression": compression_result,
                                "patterns": pattern_result
                            }
                            
                            with st.expander("üîç Analyse d√©taill√©e"):
                                st.write(f"**Entropie:** {entropy_result}")
                                st.write(f"**Compression:** {compression_result}")
                                st.write(f"**Patterns:** {pattern_result}")
                            
                            technical_summary = f"Fichier binaire .dat de {file_size:,} octets"
                            status.update(label="‚úÖ Analyse binaire termin√©e", state="complete")
                    
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Parseur multi-fr√©quences √©chou√©: {e}")
                        st.info("Passage en mode analyse binaire standard...")
                        
                        # Fallback vers analyse binaire
                        entropy_result = entropy_analysis(file_bytes)
                        compression_result = compression_ratio(file_bytes)
                        pattern_result = pattern_recognition(file_bytes)
                        
                        extracted_data["metadata"] = {
                            "entropy": entropy_result,
                            "compression": compression_result,
                            "patterns": pattern_result
                        }
                        
                        technical_summary = f"Fichier binaire .dat de {file_size:,} octets"
                        status.update(label="‚ö†Ô∏è Analyse binaire (fallback)", state="complete")
                
                else:
                    # Fichiers binaires non-.dat
                    try:
                        entropy_result = entropy_analysis(file_bytes)
                        compression_result = compression_ratio(file_bytes)
                        pattern_result = pattern_recognition(file_bytes)
                        
                        extracted_data["metadata"] = {
                            "entropy": entropy_result,
                            "compression": compression_result,
                            "patterns": pattern_result
                        }
                        
                        with st.expander("üîç Analyse d√©taill√©e"):
                            st.write(f"**Entropie:** {entropy_result}")
                            st.write(f"**Compression:** {compression_result}")
                            st.write(f"**Patterns:** {pattern_result}")
                        
                        technical_summary = f"Fichier binaire .{file_extension} de {file_size/1024:.2f} KB"
                        st.success(f"‚úÖ {technical_summary}")
                        status.update(label="‚úÖ Analyse binaire termin√©e", state="complete")
                    
                    except Exception as e:
                        technical_summary = f"Fichier binaire .{file_extension}"
                        st.warning(f"‚ö†Ô∏è Analyse partielle: {e}")
                        status.update(label="‚ö†Ô∏è Analyse partielle", state="complete")
        
        # === √âTAPE 3: RECHERCHE WEB CONTEXTUELLE ===
        web_context = ""
        if web_enabled:
            with st.status("üåê Recherche d'informations contextuelles...", expanded=True) as status:
                try:
                    # Cr√©er une requ√™te intelligente bas√©e sur le type de fichier
                    search_query = f"analyse {file_extension} fichier utilisation cas pratiques"
                    
                    st.write(f"üîé Recherche: *{search_query}*")
                    
                    # Utiliser Tavily pour recherche
                    from langchain_tavily import TavilySearch
                    tavily = TavilySearch()
                    results = tavily.run(search_query)
                    
                    web_context = f"\n\n**Contexte Web:**\n{results[:500]}"
                    st.success("‚úÖ Informations trouv√©es sur le web")
                    status.update(label="‚úÖ Recherche termin√©e", state="complete")
                except Exception as e:
                    st.info("‚ÑπÔ∏è Recherche web non disponible")
                    status.update(label="‚ÑπÔ∏è Recherche ignor√©e", state="complete")
        
        # === √âTAPE 4: KIBALI G√âN√àRE L'ANALYSE INTELLIGENTE ===
        with st.status("ü§ñ Kibali pr√©pare son analyse...", expanded=True) as status:
            st.write("üí≠ R√©flexion en cours...")
            
            # Construire le prompt pour Kibali - ULTRA-OPTIMIS√â ET PUISSANT
            kibali_prompt = f"""
Tu es Kibali, expert IA g√©ophysique ERT. Analyse RAPIDE, PR√âCISE, COMPL√àTE.

üìÅ {uploaded_file.name} | {file_extension.upper()} | {file_size:,} octets
üìä {extracted_data['content'][:500] if extracted_data['content'] else 'Binaire'}
üîç {extracted_data['metadata']}
{web_context}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéØ 10 R√àGLES D'ANALYSE PUISSANTE:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
1. IDENTIFIE type donn√©es (ERT/g√©o/autre) en 1 phrase
2. EXTRAIS valeurs cl√©s (profondeurs, r√©sistivit√©s, coords)
3. D√âTECTE patterns (grille, anomalies, zones)
4. INTERPR√àTE g√©ologiquement (formations, mat√©riaux)
5. QUANTIFIE pr√©cis√©ment (min/max/moyenne + unit√©s)
6. CONTEXTUALISE scientifiquement (normes, refs)
7. VISUALISE structure (2D/3D, coupes, profils)
8. ANTICIPE questions (profondeur? zones? roches?)
9. PROPOSE 3 actions IMM√âDIATES
10. STRUCTURE sections claires (üîçüìäüí°üéØ)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
ÔøΩ 5 R√àGLES DE PR√âCISION MAXIMALE:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
1. **VALEURS EXACTES**: Donne TOUTES les valeurs num√©riques avec UNIT√âS
   Exemple: "R√©sistivit√©s: 0.5-120.3 Œ©¬∑m, Profondeurs: 0.0-15.5 m"
2. **INCERTITUDES**: Mentionne pr√©cision/erreurs si d√©tectables
   Exemple: "¬±0.1 m en profondeur, ¬±5% en r√©sistivit√©"
3. **COORDONN√âES GPS**: Extrais latitude/longitude si pr√©sentes
   Exemple: "Position: 1¬∞23'45\"S, 13¬∞45'12\"E"
4. **TIMESTAMPS**: Note dates/heures d'acquisition si trouv√©es
   Exemple: "Mesures: 2024-11-06 14:30 UTC"
5. **PARAM√àTRES TECHNIQUE**: Liste config instrument (espacement, fr√©quence)
   Exemple: "Wenner Œ±=2m, f=100Hz, 48 √©lectrodes"

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìä 5 R√àGLES DE SYNTH√àSE/RAPPORTS/STATS/GRAPHIQUES:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
1. **STATS COMPL√àTES**: Calcule ET affiche:
   ‚Ä¢ Moyenne, m√©diane, √©cart-type
   ‚Ä¢ Min, max, quartiles (Q1, Q3)
   ‚Ä¢ Distribution normale? Asym√©trie (skewness)?
   
2. **GRAPHIQUES AUTO**: G√©n√®re IMM√âDIATEMENT:
   ‚úì Profil vertical (r√©sistivit√© vs profondeur)
   ‚úì Histogramme avec courbe normale
   ‚úì Coupe 2D si grille d√©tect√©e
   ‚úì L√©gende couleurs mat√©riaux
   
3. **TABLEAU COMPARATIF**: Cr√©e table mat√©riaux:
   | Mat√©riau | R√©sistivit√© typique | D√©tect√©? | Profondeur |
   |----------|-------------------|----------|------------|
   | Eau | 0.5-10 Œ©¬∑m | OUI | 0-5m |
   | Argile | 1-100 Œ©¬∑m | OUI | 5-10m |
   
4. **RAPPORT SYNTH√àSE**: Structure professionnelle:
   ## üìã RAPPORT D'ANALYSE ERT
   ### 1. R√âSUM√â EX√âCUTIF (3 lignes)
   ### 2. STATISTIQUES CL√âS (tableau)
   ### 3. INTERPR√âTATION G√âOLOGIQUE (bullet points)
   ### 4. RECOMMANDATIONS (num√©rot√©es)
   ### 5. ANNEXES (graphiques t√©l√©chargeables)
   
5. **EXPORT MULTI-FORMAT**: Propose t√©l√©chargements:
   üì• CSV (donn√©es brutes)
   üì• PDF (rapport complet)
   üì• PNG (graphiques HD 300dpi)
   üì• HTML (interactif Plotly)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üõ†Ô∏è 10 R√àGLES UTILISATION COH√âRENTE OUTILS:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
1. code_agent ‚Üí extractions pr√©cises
2. web_search ‚Üí contexte scientifique
3. auto-viz ‚Üí si >50 points
4. numpy ‚Üí stats si >10 valeurs
5. profils verticaux ‚Üí donn√©es profondeur
6. l√©gendes couleur ‚Üí r√©sistivit√©s
7. histogrammes ‚Üí distributions
8. tableaux comparatifs ‚Üí mat√©riaux
9. export multi-formats (PNG/PDF/HTML)
10. workflows combin√©s (extract‚Üístats‚Üíviz)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã STRUCTURE R√âPONSE OBLIGATOIRE:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üîç **IDENTIFICATION** (1 ligne pr√©cise)
üìä **STATISTIQUES** (tableau format√©)
üìà **GRAPHIQUES** (g√©n√©ration automatique)
üí° **INTERPR√âTATION** (g√©ologie + hydro)
üéØ **ACTIONS** (3 commandes concr√®tes)
‚ùì **QUESTIONS** (3 probables)

SOIS: CONCIS | PR√âCIS | ACTIONNABLE | VISUEL
"""
            
            # ‚ö° ANALYSE SIMPLIFI√âE : Utiliser uniquement l'agent de code (pas de LLM)
            # Le LLM interviendra apr√®s dans le chat pour r√©pondre aux questions
            status.update(label="‚úÖ Analyse technique termin√©e - Pr√™t pour vos questions", state="complete")
            
            # Cr√©er un r√©sum√© simple sans LLM
            simple_summary = f"""
## üìä Analyse du fichier `{uploaded_file.name}`

### üîç IDENTIFICATION
- **Type**: {file_extension.upper()}
- **Taille**: {file_size:,} octets ({file_size/1024:.2f} KB)
- **Statut**: ‚úÖ Fichier charg√© et pr√™t √† analyser

### üìù R√âSUM√â TECHNIQUE
{technical_summary if technical_summary else "Fichier binaire d√©tect√©"}

### üéØ ACTIONS DISPONIBLES
Vous pouvez maintenant me poser des questions sur ce fichier :
- "Extrais les donn√©es du fichier"
- "Analyse les r√©sistivit√©s"
- "Cr√©e un graphique"
- "Quelles sont les statistiques ?"

üí¨ **Kibali est pr√™t √† r√©pondre √† vos questions !**
"""
            
            return simple_summary, extracted_data

# ========================================
# SYST√àME D'OUTILS AUTONOMES POUR KIBALI
# ========================================

class KibaliToolsManager:
    """
    Gestionnaire d'outils autonomes pour Kibali
    Permet √† Kibali de choisir et utiliser le bon outil selon la requ√™te
    """
    
    def __init__(self):
        self.tools = {
            "resistivity_plot": {
                "name": "G√©n√©rateur de Coupe de R√©sistivit√© ERT",
                "description": "G√©n√®re des graphiques de r√©sistivit√© avec couleurs exactes selon normes g√©ophysiques",
                "function": self.generate_resistivity_section,
                "keywords": ["r√©sistivit√©", "ert", "coupe", "profil", "graphique", "dat", "ohm"]
            },
            "resistivity_analysis": {
                "name": "Analyseur de Mat√©riaux par R√©sistivit√©",
                "description": "Identifie les mat√©riaux g√©ologiques √† partir de valeurs de r√©sistivit√©",
                "function": self.analyze_resistivity_materials,
                "keywords": ["mat√©riau", "min√©ral", "roche", "sol", "argile", "sable"]
            },
            "data_extraction": {
                "name": "Extracteur de Donn√©es .DAT",
                "description": "Lit et extrait les valeurs num√©riques des fichiers .dat ERT",
                "function": self.extract_dat_data,
                "keywords": ["dat", "fichier", "extraire", "lire", "donn√©es"]
            },
            "statistical_analysis": {
                "name": "Analyse Statistique Avanc√©e",
                "description": "Calcule statistiques, corr√©lations, distributions",
                "function": self.perform_statistical_analysis,
                "keywords": ["statistique", "moyenne", "√©cart-type", "corr√©lation", "distribution"]
            },
            "web_research": {
                "name": "Recherche Web Contextuelle",
                "description": "Recherche des informations suppl√©mentaires sur internet",
                "function": self.perform_web_search,
                "keywords": ["recherche", "internet", "web", "information", "documentation"]
            },
            "data_visualization": {
                "name": "Visualiseur de Donn√©es G√©n√©riques",
                "description": "Cr√©e des graphiques pour tout type de donn√©es",
                "function": self.create_generic_plot,
                "keywords": ["graphique", "courbe", "histogramme", "scatter", "visualisation"]
            },
            "color_mapper": {
                "name": "Carte de Couleurs de R√©sistivit√©",
                "description": "Affiche la correspondance couleur-r√©sistivit√© selon normes ERT",
                "function": self.show_color_mapping,
                "keywords": ["couleur", "palette", "colormap", "l√©gende"]
            },
            "dat_structure_analyzer": {
                "name": "Analyseur de Structure de Fichiers .DAT",
                "description": "Analyse automatiquement la structure de fichiers .DAT (encodage, d√©limiteur, colonnes)",
                "function": self.analyze_dat_structure,
                "keywords": ["structure", "dat", "analyse", "format", "encodage", "d√©limiteur", "colonnes"]
            },
            "survey_depth_parser": {
                "name": "Parseur Survey-Point / Depth / Data",
                "description": "Parseur sp√©cialis√© pour fichiers avec colonnes survey-point, depth, data (profils ERT verticaux)",
                "function": self.parse_survey_depth_data,
                "keywords": ["survey", "point", "depth", "profondeur", "profil", "vertical", "coupe", "3d", "volume"]
            },
            "multi_freq_parser": {
                "name": "Parseur Multi-Fr√©quences ERT (Fichiers Compl√©mentaires)",
                "description": "Parse et fusionne plusieurs fichiers .dat avec multi-fr√©quences (MHz), calcule coordonn√©es spatiales correctes (X, Y, Z)",
                "function": self.parse_multi_freq_data,
                "keywords": ["multi", "fr√©quence", "mhz", "compl√©mentaire", "fusionner", "fusion", "coordonn√©es", "spatial", "xyz", "plusieurs fichiers"]
            }
        }
    
    def detect_needed_tools(self, user_query: str, file_data: dict = None) -> list:
        """
        D√©tecte automatiquement les outils n√©cessaires pour r√©pondre √† la requ√™te
        """
        query_lower = user_query.lower()
        needed_tools = []
        
        # V√©rifier chaque outil
        for tool_id, tool_info in self.tools.items():
            for keyword in tool_info["keywords"]:
                if keyword in query_lower:
                    needed_tools.append(tool_id)
                    break
        
        # Ajout automatique selon contexte fichier
        if file_data:
            if file_data.get("extension") == "dat":
                if "data_extraction" not in needed_tools:
                    needed_tools.insert(0, "data_extraction")
                if "r√©sistivit√©" in query_lower or "ert" in query_lower:
                    if "resistivity_plot" not in needed_tools:
                        needed_tools.append("resistivity_plot")
        
        return needed_tools
    
    def generate_resistivity_section(self, data: dict, query: str = "") -> dict:
        """G√©n√®re une coupe de r√©sistivit√© ERT avec couleurs exactes"""
        try:
            resistivity_values = data.get("resistivity_values", [])
            
            if not resistivity_values:
                # Essayer d'extraire des valeurs du contenu
                content = data.get("content", "")
                import re
                numbers = re.findall(r'\d+\.?\d*', content)
                resistivity_values = [float(n) for n in numbers if float(n) > 0][:100]
            
            if resistivity_values:
                # Utiliser la fonction existante
                plot_html = generate_resistivity_plot(resistivity_values)
                
                # Analyse des mat√©riaux
                materials_analysis = analyze_minerals_from_resistivity(resistivity_values)
                
                # Table de couleurs
                color_table = generate_resistivity_table(resistivity_values)
                
                return {
                    "success": True,
                    "plot": plot_html,
                    "analysis": materials_analysis,
                    "color_table": color_table,
                    "message": f"‚úÖ Coupe de r√©sistivit√© g√©n√©r√©e avec {len(resistivity_values)} points"
                }
            else:
                return {
                    "success": False,
                    "message": "‚ùå Aucune valeur de r√©sistivit√© trouv√©e"
                }
                
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå Erreur g√©n√©ration coupe: {e}"
            }
    
    def analyze_resistivity_materials(self, data: dict, query: str = "") -> dict:
        """Analyse les mat√©riaux √† partir des r√©sistivit√©s"""
        try:
            resistivity_values = data.get("resistivity_values", [])
            filename = data.get("filename", "unknown")
            
            if resistivity_values:
                analysis = analyze_minerals_from_resistivity(resistivity_values, filename)
                color_analysis = resistivity_color_analysis(resistivity_values)
                
                return {
                    "success": True,
                    "analysis": analysis,
                    "color_analysis": color_analysis,
                    "message": "‚úÖ Analyse des mat√©riaux termin√©e"
                }
            else:
                return {
                    "success": False,
                    "message": "‚ùå Aucune donn√©e de r√©sistivit√©"
                }
                
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå Erreur analyse: {e}"
            }
    
    def extract_dat_data(self, data: dict, query: str = "") -> dict:
        """Extrait les donn√©es d'un fichier .dat"""
        try:
            content = data.get("content", "")
            
            if not content:
                return {
                    "success": False,
                    "message": "‚ùå Aucun contenu √† extraire"
                }
            
            # Extraction des nombres
            import re
            numbers = re.findall(r'-?\d+\.?\d*', content)
            extracted_values = [float(n) for n in numbers if n]
            
            # Filtrer pour garder les r√©sistivit√©s plausibles (0.001 - 1000000 Ohm.m)
            resistivity_values = [v for v in extracted_values if 0.001 <= v <= 1000000]
            
            return {
                "success": True,
                "resistivity_values": resistivity_values,
                "total_numbers": len(extracted_values),
                "message": f"‚úÖ {len(resistivity_values)} valeurs de r√©sistivit√© extraites"
            }
            
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå Erreur extraction: {e}"
            }
    
    def perform_statistical_analysis(self, data: dict, query: str = "") -> dict:
        """Effectue une analyse statistique des donn√©es"""
        try:
            values = data.get("resistivity_values", [])
            
            if not values:
                return {
                    "success": False,
                    "message": "‚ùå Aucune donn√©e pour analyse statistique"
                }
            
            import numpy as np
            from scipy import stats
            
            arr = np.array(values)
            
            statistics = {
                "Nombre de valeurs": len(arr),
                "Moyenne": np.mean(arr),
                "M√©diane": np.median(arr),
                "√âcart-type": np.std(arr),
                "Minimum": np.min(arr),
                "Maximum": np.max(arr),
                "Q1 (25%)": np.percentile(arr, 25),
                "Q3 (75%)": np.percentile(arr, 75),
                "Skewness": stats.skew(arr),
                "Kurtosis": stats.kurtosis(arr)
            }
            
            # Format pour affichage
            stats_text = "## üìä Statistiques Descriptives\n\n"
            for key, value in statistics.items():
                if isinstance(value, (int, float)):
                    stats_text += f"- **{key}:** {value:.3f}\n"
                else:
                    stats_text += f"- **{key}:** {value}\n"
            
            return {
                "success": True,
                "statistics": statistics,
                "formatted_text": stats_text,
                "message": "‚úÖ Analyse statistique compl√®te"
            }
            
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå Erreur statistiques: {e}"
            }
    
    def perform_web_search(self, data: dict, query: str = "") -> dict:
        """Effectue une recherche web contextuelle"""
        try:
            from langchain_tavily import TavilySearch
            
            # Construire requ√™te intelligente
            search_query = query
            if data.get("extension") == "dat":
                search_query += " ERT electrical resistivity tomography interpretation"
            
            tavily = TavilySearch()
            results = tavily.run(search_query)
            
            return {
                "success": True,
                "results": results,
                "message": "‚úÖ Recherche web termin√©e"
            }
            
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ö†Ô∏è Recherche web non disponible: {e}",
                "results": ""
            }
    
    def create_generic_plot(self, data: dict, query: str = "") -> dict:
        """Cr√©e un graphique g√©n√©rique"""
        try:
            values = data.get("resistivity_values", [])
            
            if not values:
                return {
                    "success": False,
                    "message": "‚ùå Aucune donn√©e √† visualiser"
                }
            
            import matplotlib.pyplot as plt
            import numpy as np
            import io
            import base64
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            # Graphique 1: Courbe
            ax1.plot(values, marker='o', linestyle='-', linewidth=2)
            ax1.set_title("√âvolution des valeurs")
            ax1.set_xlabel("Index")
            ax1.set_ylabel("Valeur")
            ax1.grid(True, alpha=0.3)
            
            # Graphique 2: Histogramme
            ax2.hist(values, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
            ax2.set_title("Distribution")
            ax2.set_xlabel("Valeur")
            ax2.set_ylabel("Fr√©quence")
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Convertir en base64
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')
            buffer.seek(0)
            plot_data = buffer.getvalue()
            buffer.close()
            plt.close()
            
            plot_base64 = base64.b64encode(plot_data).decode()
            plot_html = f'<img src="data:image/png;base64,{plot_base64}" style="max-width: 100%;" />'
            
            return {
                "success": True,
                "plot": plot_html,
                "message": "‚úÖ Graphique g√©n√©r√©"
            }
            
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå Erreur graphique: {e}"
            }
    
    def show_color_mapping(self, data: dict, query: str = "") -> dict:
        """Affiche la carte de correspondance couleurs-r√©sistivit√©"""
        try:
            color_table = get_water_resistivity_color_table()
            
            return {
                "success": True,
                "color_mapping": color_table,
                "message": "‚úÖ Carte de couleurs ERT"
            }
            
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå Erreur: {e}"
            }
    
    def analyze_dat_structure(self, data: dict, query: str = "") -> dict:
        """Analyse automatiquement la structure d'un fichier .DAT"""
        try:
            file_path = data.get("physical_path", "")
            if not file_path:
                return {
                    "success": False,
                    "message": "‚ùå Aucun chemin de fichier fourni"
                }
            
            # Utiliser l'analyseur intelligent
            structure = analyze_dat_file(file_path)
            
            if structure:
                return {
                    "success": True,
                    "structure": structure,
                    "message": f"‚úÖ Structure analys√©e: {structure.file_type} | Confiance: {structure.confidence}%"
                }
            else:
                return {
                    "success": False,
                    "message": "‚ùå Impossible d'analyser la structure du fichier"
                }
                
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå Erreur analyse structure: {e}"
            }
    
    def parse_survey_depth_data(self, data: dict, query: str = "") -> dict:
        """Parseur sp√©cialis√© pour fichiers survey-point/depth/data - DISABLED"""
        return {
            "success": False,
            "message": "‚ö†Ô∏è Parseur survey-depth d√©sactiv√© (calculs incorrects). Utilisez multi_freq_parser √† la place."
        }
    
    def parse_multi_freq_data(self, data: dict, query: str = "") -> dict:
        """
        Parseur multi-fr√©quences pour fichiers .dat compl√©mentaires
        Calcule les coordonn√©es spatiales correctes (X, Y, Z)
        """
        try:
            # V√©rifier si des fichiers multiples sont disponibles dans session_state
            if 'multi_freq_data' in st.session_state and st.session_state.multi_freq_data is not None:
                parser = st.session_state.get('multi_freq_parser')
                df = st.session_state.multi_freq_data
                coords_df = st.session_state.get('multi_freq_coords')
                
                return {
                    "success": True,
                    "data": df,
                    "coordinates": coords_df,
                    "parser": parser,
                    "metadata": parser.metadata if parser else {},
                    "message": f"‚úÖ Donn√©es multi-fr√©quences disponibles: {len(df)} mesures, {len(parser.frequencies)} fr√©quences"
                }
            
            # Sinon, essayer de parser un seul fichier
            file_path = data.get("physical_path", "")
            if not file_path or not os.path.exists(file_path):
                return {
                    "success": False,
                    "message": "‚ùå Aucun fichier disponible. Uploadez des fichiers .dat dans l'interface multi-fr√©quences."
                }
            
            # Parser le fichier
            from multi_freq_ert_parser import MultiFreqERTParser
            parser = MultiFreqERTParser()
            df = parser.parse_multiple_files([file_path])
            
            if df.empty:
                return {
                    "success": False,
                    "message": "‚ùå Aucune donn√©e extraite. V√©rifiez le format du fichier."
                }
            
            # G√©n√©rer coordonn√©es
            coords_df = parser.get_coordinates_corrected()
            
            # Sauvegarder dans session state
            st.session_state['multi_freq_data'] = df
            st.session_state['multi_freq_coords'] = coords_df
            st.session_state['multi_freq_parser'] = parser
            
            return {
                "success": True,
                "data": df,
                "coordinates": coords_df,
                "parser": parser,
                "metadata": parser.metadata,
                "frequencies": parser.frequencies,
                "survey_points": parser.survey_points,
                "message": f"‚úÖ Fichier pars√©: {len(df)} mesures, {len(parser.frequencies)} fr√©quences, {len(parser.survey_points)} survey points"
            }
            
        except Exception as e:
            import traceback
            return {
                "success": False,
                "message": f"‚ùå Erreur parseur multi-fr√©quences: {e}",
                "traceback": traceback.format_exc()
            }
    
    def execute_tools(self, tool_ids: list, data: dict, query: str = "") -> dict:
        """
        Ex√©cute plusieurs outils et retourne les r√©sultats combin√©s
        """
        results = {
            "tools_used": [],
            "outputs": {},
            "success": True
        }
        
        for tool_id in tool_ids:
            if tool_id in self.tools:
                tool_info = self.tools[tool_id]
                st.info(f"üîß Utilisation de l'outil: **{tool_info['name']}**")
                
                # Ex√©cuter l'outil
                output = tool_info["function"](data, query)
                
                results["tools_used"].append(tool_info["name"])
                results["outputs"][tool_id] = output
                
                if not output.get("success", False):
                    results["success"] = False
        
        return results

# Instance globale du gestionnaire d'outils
kibali_tools = KibaliToolsManager()

# Instance globale du moteur de visualisation
viz_engine = VisualizationEngine()

# Instance globale de l'auto-visualiseur
auto_viz = AutoVisualizer()


def llm_with_tools(prompt: str, file_data: dict = None, qwen_llm=None, code_agent=None) -> dict:
    """
    LLM Qwen qui analyse la question et d√©cide intelligemment d'utiliser les outils
    
    Args:
        prompt: Question de l'utilisateur
        file_data: Donn√©es du fichier upload√©
        qwen_llm: Instance du mod√®le Qwen
        code_agent: Instance de l'agent de code
    
    Returns:
        dict: {
            'response': str,  # R√©ponse textuelle du LLM
            'needs_code': bool,  # Besoin d'ex√©cuter du code
            'needs_viz': bool,  # Besoin de visualisations
            'code_params': dict  # Param√®tres pour l'agent de code
        }
    """
    # Pr√©parer le contexte pour Qwen
    context = f"""Tu es Kibali, expert en g√©ophysique et analyse ERT.

FICHIER ANALYS√â: {file_data.get('filename', 'N/A') if file_data else 'Aucun'}

QUESTION: {prompt}

INSTRUCTIONS:
1. Analyse la question et r√©ponds de fa√ßon naturelle et conversationnelle
2. Si tu as besoin d'extraire des donn√©es pr√©cises du fichier, indique [USE_CODE_AGENT]
3. Si tu veux cr√©er des visualisations, indique [USE_VISUALIZATION]
4. Sois concis et p√©dagogique

R√©ponds maintenant:"""
    
    try:
        # Utiliser Qwen pour g√©n√©rer la r√©ponse avec LIMITE AUGMENT√âE
        if qwen_llm:
            from langchain.schema import HumanMessage
            # Configuration pour g√©n√©rer des r√©ponses LONGUES et D√âTAILL√âES
            response = qwen_llm.invoke(
                [HumanMessage(content=context)],
                config={"max_tokens": 3000, "temperature": 0.7}  # 3000 tokens
            )
            llm_response = response.content if hasattr(response, 'content') else str(response)
        else:
            # Fallback si Qwen n'est pas disponible
            llm_response = f"Pour r√©pondre √† votre question '{prompt}', j'ai besoin d'analyser le fichier en d√©tail."
        
        # D√©tecter les besoins d'outils dans la r√©ponse
        needs_code = '[USE_CODE_AGENT]' in llm_response or any(word in prompt.lower() for word in [
            'profondeur', 'valeur', 'extraire', 'calculer', 'statistique', 'pr√©cis', 'exact'
        ])
        
        needs_viz = '[USE_VISUALIZATION]' in llm_response or any(word in prompt.lower() for word in [
            'graphique', 'profil', 'coupe', 'visualis', 'affiche', 'montre', 'sch√©ma'
        ])
        
        # Nettoyer la r√©ponse des marqueurs
        clean_response = llm_response.replace('[USE_CODE_AGENT]', '').replace('[USE_VISUALIZATION]', '').strip()
        
        return {
            'response': clean_response,
            'needs_code': needs_code,
            'needs_viz': needs_viz,
            'code_params': {
                'action': 'extract' if 'profondeur' in prompt.lower() or 'valeur' in prompt.lower() else 'analyze',
                'query': prompt
            }
        }
    
    except Exception as e:
        print(f"‚ùå Erreur LLM: {e}")
        return {
            'response': f"Je peux vous aider avec '{prompt}'. Laissez-moi analyser les donn√©es...",
            'needs_code': True,
            'needs_viz': False,
            'code_params': {'action': 'analyze', 'query': prompt}
        }


def handle_chat_enhanced(message, history, agent, model_choice, vectordb, graph, pois, web_enabled, mode="humain", mode_prompt=None):
    """
    Conversation naturelle avec le LLM principal qui d√©cide lui-m√™me d'utiliser les outils
    Le LLM contr√¥le tout, les agents sont des outils optionnels
    
    Args:
        message: Question de l'utilisateur
        history: Historique de conversation
        file_context: Contexte du fichier upload√© (optionnel)
        web_enabled: Activer recherche web
    
    Returns:
        R√©ponse conversationnelle du LLM
    """
    # Construire le contexte complet pour le LLM
    system_prompt = """Tu es Kibali, un assistant IA expert en g√©ophysique et analyse de donn√©es ERT.

üéØ **TON R√îLE**:
- Avoir des conversations naturelles et fluides avec l'utilisateur
- R√©pondre directement aux questions sans r√©p√©ter la m√™me analyse
- Utiliser ta connaissance pour expliquer, interpr√©ter, contextualiser
- D√©cider toi-m√™me quand tu as besoin d'outils pour compl√©ter ta r√©ponse

üõ†Ô∏è **OUTILS DISPONIBLES** (√† utiliser SI N√âCESSAIRE):
1. **code_agent** : G√©n√©rer et ex√©cuter du code Python pour analyses complexes
   - Usage: Quand tu as besoin de calculs pr√©cis, extractions de donn√©es, analyses statistiques
   - Exemple: "Je vais g√©n√©rer du code pour extraire les profondeurs exactes"

2. **web_search** : Rechercher des informations actuelles sur internet
   - Usage: Pour contextualiser avec des r√©f√©rences scientifiques, normes, √©tudes
   - Exemple: "Je vais chercher les normes de r√©sistivit√© pour ce type de sol"

3. **visualization** : Cr√©er des graphiques professionnels
   - Usage: Pour illustrer les donn√©es, cr√©er des profils, sections, histogrammes
   - Exemple: "Je vais cr√©er un profil de r√©sistivit√© pour visualiser ces donn√©es"

üìù **R√àGLES DE CONVERSATION**:
1. **R√©ponds directement** √† la question pos√©e, ne refais pas l'analyse compl√®te √† chaque fois
2. **Utilise le contexte** : R√©f√©rence les donn√©es du fichier d√©j√† analys√©
3. **Sois conversationnel** : Parle naturellement, pas en bullet points robotiques
4. **Appelle les outils** seulement quand n√©cessaire, pas syst√©matiquement
5. **Explique ce que tu fais** : "Je vais utiliser l'outil X pour Y"

üí° **EXEMPLES DE BONNES R√âPONSES**:

Question: "Donne moi les profondeurs exactes"
‚ùå Mauvais: *refaire toute l'analyse du fichier*
‚úÖ Bon: "D'apr√®s l'analyse pr√©c√©dente, les profondeurs vont de 0m √† 15m avec des mesures tous les 0.5m. Voici les profondeurs exactes: [liste]. Veux-tu que je cr√©e un profil vertical pour mieux visualiser ces donn√©es ?"

Question: "C'est quoi cette zone de faible r√©sistivit√© ?"
‚ùå Mauvais: *refaire extraction compl√®te*
‚úÖ Bon: "La zone de faible r√©sistivit√© (< 10 ohm-m) que nous avons d√©tect√©e correspond probablement √† de l'argile satur√©e en eau ou √† une nappe phr√©atique. Ces valeurs sont typiques pour..."
"""

    # Ajouter le contexte du fichier si disponible
    if file_context:
        system_prompt += f"""

üìÅ **FICHIER ANALYS√â**:
- Nom: {file_context.get('filename', 'N/A')}
- Type: {file_context.get('extension', 'N/A')}
- Donn√©es disponibles: {file_context.get('summary', 'R√©sistivit√©s, profondeurs, positions')}
"""

    # G√©n√©rer la r√©ponse avec le LLM
    try:
        # Utiliser Ollama (local)
        from openai import OpenAI
        
        try:
            client = OpenAI(
                base_url="http://localhost:11434/v1",
                api_key="ollama"
            )
            
            # Pr√©parer les messages
            messages = [{"role": "system", "content": system_prompt}]
            
            # Ajouter l'historique r√©cent
            for msg in history[-5:]:
                messages.append({"role": msg['role'], "content": msg['content']})
            
            # Ajouter le message actuel
            messages.append({"role": "user", "content": message})
            
            response = client.chat.completions.create(
                model="llama3.2:latest",
                messages=messages,
                temperature=0.7,
                max_tokens=1500
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"‚ö†Ô∏è Ollama non disponible: {e}")
            # Fallback: r√©ponse basique intelligente
            return generate_smart_fallback_response(message, history, file_context)
    
    except Exception as e:
        print(f"‚ùå Erreur LLM: {e}")
        return f"Je suis d√©sol√©, j'ai rencontr√© une erreur. Peux-tu reformuler ta question ? Erreur: {e}"


def generate_smart_fallback_response(message: str, history: list, file_context: dict) -> str:
    """G√©n√®re une r√©ponse intelligente sans LLM en analysant la question"""
    message_lower = message.lower()
    
    # D√©tection de questions sp√©cifiques
    if any(word in message_lower for word in ['profondeur', 'depth', 'profond']):
        return """D'apr√®s le fichier analys√©, voici les informations sur les profondeurs :

Les mesures ont √©t√© effectu√©es √† diff√©rentes profondeurs. Pour obtenir les valeurs exactes et pr√©cises, je peux g√©n√©rer du code Python qui va extraire toutes les profondeurs du fichier. 

Souhaites-tu que je fasse cette extraction d√©taill√©e ?"""
    
    elif any(word in message_lower for word in ['r√©sistivit√©', 'resistivity', 'valeur']):
        return """Concernant les valeurs de r√©sistivit√©, le fichier contient plusieurs mesures. Je peux te donner diff√©rents types d'informations :

- Les valeurs brutes compl√®tes
- Les statistiques (min, max, moyenne, m√©diane)
- La distribution par zones g√©ologiques
- Un profil vertical

Qu'est-ce qui t'int√©resse particuli√®rement ?"""
    
    elif any(word in message_lower for word in ['graph', 'visualis', 'profil', 'coupe']):
        return """Je peux cr√©er plusieurs types de visualisations pour ces donn√©es :

üìä **Disponibles**:
- Profil vertical de r√©sistivit√©
- Coupe 2D avec colormap
- Histogramme par zones
- Colonne g√©ologique

Quel type de graphique veux-tu que je g√©n√®re ?"""
    
    else:
        # R√©ponse g√©n√©rique conversationnelle
        return f"""Je comprends ta question sur "{message}". 

Pour te donner une r√©ponse pr√©cise, peux-tu me dire exactement ce que tu cherches ? Par exemple :
- Des valeurs num√©riques sp√©cifiques ?
- Une interpr√©tation g√©ologique ?
- Une visualisation ?
- Une comparaison avec d'autres donn√©es ?

Je suis l√† pour t'aider de fa√ßon conversationnelle et naturelle ! üòä"""


def handle_chat_enhanced(message, history, agent, model_choice, vectordb, graph, pois, web_enabled, mode="humain", mode_prompt=None):
    # AJOUT M√âMOIRE VECTORIELLE: Charger la base chat
    chat_vectordb, _ = load_chat_vectordb()
    if not message.strip():
        return ""
    
    # üéØ ENRICHISSEMENT AUTOMATIQUE DU CONTEXTE FICHIER
    # Si un fichier est upload√©, toujours l'ajouter au contexte de la conversation
    file_context_prompt = ""
    
    # üìä CONTEXTE MULTI-FR√âQUENCES ERT
    multi_freq_context = ""
    if 'multi_freq_data' in st.session_state and st.session_state.multi_freq_data is not None:
        parser = st.session_state.get('multi_freq_parser')
        if parser:
            multi_freq_context = f"""

üì° **DONN√âES ERT MULTI-FR√âQUENCES CHARG√âES**
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìÅ Fichiers fusionn√©s: {parser.metadata['num_files']}
üì° Fr√©quences: {', '.join([f'{f:.1f} MHz' for f in parser.frequencies])}
üìç Survey points: {parser.metadata['num_survey_points']}
üìè Profondeurs: {parser.metadata['depth_range'][0]:.1f}m √† {parser.metadata['depth_range'][1]:.1f}m
üìä Total mesures: {parser.metadata['total_measurements']:,}
üî¨ R√©sistivit√©s: {parser.metadata['resistivity_range'][0]:.2f} - {parser.metadata['resistivity_range'][1]:.2f} Œ©¬∑m

üí° **Tu peux acc√©der aux DataFrames**:
- `st.session_state.multi_freq_data` : Donn√©es compl√®tes
- `st.session_state.multi_freq_coords` : Coordonn√©es spatiales (X, Y, Z)
- `st.session_state.multi_freq_parser` : Objet parseur avec toutes les m√©thodes

üé® **M√©thodes de visualisation disponibles**:
- `parser.create_2d_section_by_frequency(freq)` : Coupe 2D pour une fr√©quence
- `parser.create_3d_volume()` : Volume 3D interactif
- `parser.create_frequency_comparison()` : Comparaison multi-fr√©quences
- `parser.get_coordinates_corrected()` : Coordonn√©es spatiales corrig√©es

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
    
    if 'uploaded_file_data' in st.session_state and st.session_state.uploaded_file_data:
        file_data = st.session_state.uploaded_file_data
        file_context_prompt = f"""
üìé **CONTEXTE FICHIER ACTIF** (Toujours pr√©sent dans la conversation)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìÅ Fichier: {file_data['filename']}
üìÇ Chemin physique: {file_data.get('physical_path', 'N/A')}
üìä Type: {file_data.get('extension', 'N/A')}
üíæ Taille: {file_data['size']} octets ({file_data['size']/1024:.2f} KB)
{multi_freq_context}

üìù **M√©tadonn√©es extraites**:
{file_data.get('metadata', 'Aucune m√©tadonn√©e')}

üìÑ **Aper√ßu du contenu**:
{file_data.get('content', 'Contenu non disponible')[:500]}...

üî¨ **CAPACIT√âS SCIENTIFIQUES DISPONIBLES**:
- Analyse ERT (Electrical Resistivity Tomography) : r√©sistivit√© √©lectrique, profondeurs, formations g√©ologiques
- Interpr√©tation g√©ologique : classification des couches, identification des mat√©riaux
- Recherches web en temps r√©el : pour contextualiser les donn√©es avec des r√©f√©rences scientifiques
- G√©n√©ration de code Python : pour analyses avanc√©es, visualisations, calculs statistiques
- Extraction de donn√©es binaires : formats .dat, .bin, structures hexad√©cimales
- Calculs statistiques : moyennes, m√©dianes, √©carts-types, distributions
- Visualisations : graphiques, courbes de r√©sistivit√©, cartes de profondeur

ÔøΩ **OUTILS DE VISUALISATION DISPONIBLES** (utilise-les spontan√©ment !):
- create_resistivity_profile() : Profils verticaux de r√©sistivit√© interactifs avec Plotly
- create_2d_resistivity_section() : Coupes 2D avec colormaps g√©ologiques
- create_geological_column() : Colonnes stratigraphiques avec l√©gendes
- create_cross_section_diagram() : Coupes transversales annot√©es
- create_histogram_with_zones() : Histogrammes avec zones g√©ologiques
- create_depth_statistics_chart() : Statistiques multi-courbes par profondeur
- create_legend_table() : Tables de l√©gendes HTML styl√©es
- create_annotated_diagram() : Sch√©mas avec annotations OpenCV
- Tous les graphiques sont t√©l√©chargeables en PNG, PDF, ou HTML interactif

ÔøΩüí° **INSTRUCTIONS POUR TES R√âPONSES**:
1. **Contextualise toujours** : Fais r√©f√©rence au fichier dans tes explications
2. **Sois p√©dagogique** : Explique les concepts scientifiques de fa√ßon accessible
3. **Utilise ta connaissance** : Apporte du contexte g√©ologique, physique, chimique
4. **Fais des recherches** : Si besoin, cherche sur internet pour enrichir ton analyse
5. **G√©n√®re du code** : Pour r√©pondre aux questions complexes, cr√©e et ex√©cute du code Python
6. **Cr√©e des visualisations** : Utilise spontan√©ment les outils graphiques pour illustrer tes explications
7. **Reste naturel** : Parle comme un expert humain, pas comme un robot
8. **Anticipe les besoins** : Propose des analyses suppl√©mentaires pertinentes
9. **Sois pr√©cis** : Donne des valeurs num√©riques, des unit√©s, des ordres de grandeur
10. **Rends tout t√©l√©chargeable** : Chaque graphique doit avoir un bouton de t√©l√©chargement

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

**Question de l'utilisateur**: {message}
"""
        # Ajouter le contexte enrichi au message original (ne pas remplacer compl√®tement)
        # IMPORTANT: Extraire UNIQUEMENT la vraie question utilisateur pour le code agent
        if "[QUESTION DE L'UTILISATEUR]" in message:
            # Extraire la question entre les marqueurs
            parts = message.split("[QUESTION DE L'UTILISATEUR]")
            if len(parts) > 1:
                question_part = parts[1].split("[INSTRUCTION]")[0].strip()
                original_message = question_part
            else:
                original_message = message
        else:
            original_message = message
        
        message = f"{file_context_prompt}\n\n{original_message}"
    else:
        original_message = message
    
    # ü§ñ CODE AGENT D√âSACTIV√â - KIBALI CONVERSE NATURELLEMENT
    # L'agent de code g√©n√©rait toujours les m√™mes templates
    # Kibali analyse maintenant directement avec ses capacit√©s conversationnelles
    
    # Ne pas utiliser le code agent - laisser Kibali converser naturellement
    if False:  # D√©sactiv√©
        pass
    
    # KIBALI ANALYSE DIRECTEMENT
    # Kibali va r√©pondre naturellement √† toutes les questions
    if True:  # Toujours actif
        # Pr√©parer le message pour Kibali avec le contexte du fichier
        kibali_message = original_message
        
        # Si un fichier est charg√©, ajouter son contexte
        if 'uploaded_file_data' in st.session_state and st.session_state.uploaded_file_data:
            physical_path = st.session_state.uploaded_file_data.get('physical_path')
            filename = st.session_state.uploaded_file_data.get('filename', 'fichier')
            
            if physical_path and os.path.exists(physical_path):
                # Lire un √©chantillon du fichier pour contexte
                try:
                    with open(physical_path, 'r', encoding='utf-8', errors='ignore') as f:
                        file_preview = f.read(2000)  # Premiers 2000 caract√®res
                    
                    # Extraire quelques nombres pour analyse rapide
                    import re
                    numbers = re.findall(r'[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?', file_preview)
                    sample_values = [float(x) for x in numbers[:50] if x]  # 50 premiers nombres
                    
                    if sample_values:
                        file_info = f"""
ÔøΩ CONTEXTE DU FICHIER '{filename}':
- Chemin: {physical_path}
- √âchantillon de valeurs: {len(sample_values)} valeurs extraites
- Min: {min(sample_values):.2f}, Max: {max(sample_values):.2f}, Moyenne: {sum(sample_values)/len(sample_values):.2f}
- Aper√ßu du contenu:
{file_preview[:500]}
...

QUESTION DE L'UTILISATEUR:
{original_message}
"""
                        kibali_message = file_info
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur lecture fichier: {e}")
        
        # Laisser Kibali r√©pondre naturellement avec le contexte
        message = kibali_message  # Remplacer le message par celui enrichi avec le contexte fichier
        response = None  # Kibali va g√©n√©rer sa r√©ponse plus bas
    
    # üé® D√âTECTION DE DEMANDE DE GRAPHIQUE (Agent Graphique)
    # Si l'utilisateur demande un graphique, coupe, profil, visualisation
    if any(kw in original_message.lower() for kw in ['graphique', 'coupe', 'profil', 'visuali', 'courbe', 'histogram', 'plot', 'section']):
        if 'uploaded_file_data' in st.session_state and st.session_state.uploaded_file_data:
            # Charger l'agent graphique si n√©cessaire
            if st.session_state.graph_agent is None:
                with st.spinner("üé® Chargement de l'agent de g√©n√©ration de graphiques..."):
                    try:
                        st.session_state.graph_agent = GraphGenerationAgent()
                        st.session_state.graph_agent_loaded = True
                        print("‚úÖ Agent graphique charg√©")
                    except Exception as e:
                        print(f"‚ùå Erreur chargement agent graphique: {e}")
                        st.session_state.graph_agent = None
            
            # Charger le moteur de visualisation avanc√© (PyGIMLI + OpenCV + Matplotlib)
            if 'advanced_viz_engine' not in st.session_state:
                with st.spinner("üöÄ Chargement du moteur de visualisation avanc√©..."):
                    try:
                        st.session_state.advanced_viz_engine = AdvancedVisualizationEngine()
                        print("‚úÖ Moteur avanc√© charg√© (PyGIMLI + OpenCV + Matplotlib)")
                    except Exception as e:
                        print(f"‚ùå Erreur chargement moteur avanc√©: {e}")
                        st.session_state.advanced_viz_engine = None
            
            # Si l'agent est charg√©, g√©n√©rer le graphique
            if st.session_state.graph_agent is not None:
                with st.status("üé® G√©n√©ration du graphique...", expanded=True) as status:
                    try:
                        # Lire les donn√©es du fichier
                        physical_path = st.session_state.uploaded_file_data.get('physical_path')
                        
                        st.write("üìÇ Lecture du fichier...")
                        with open(physical_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        
                        # Extraire les donn√©es
                        st.write("üîç Extraction des donn√©es...")
                        import re, numpy as np
                        numbers = re.findall(r'[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?', content)
                        all_values = [float(x) for x in numbers if x]
                        
                        if len(all_values) == 0:
                            st.error("‚ùå Aucune donn√©e num√©rique trouv√©e")
                        else:
                            st.write(f"‚úÖ {len(all_values)} valeurs extraites")
                            
                            # D√©terminer le type de graphique
                            request_info = st.session_state.graph_agent.understand_request(
                                original_message,
                                {'values': all_values, 'filename': st.session_state.uploaded_file_data.get('filename')}
                            )
                            
                            st.write(f"üéØ Type d√©tect√©: {request_info['graph_type']}")
                            
                            # G√©n√©rer le graphique appropri√©
                            if request_info['graph_type'] == '3d_volume':
                                # Volume 3D
                                st.write("üé® Cr√©ation du volume 3D interactif...")
                                
                                # Pr√©parer donn√©es pour 3D
                                resistivity = np.array(all_values)
                                n = len(resistivity)
                                
                                # Cr√©er grille 3D automatique
                                n_side = int(np.cbrt(n)) + 1
                                st.write(f"   Grille 3D: {n_side}√ó{n_side}√ó{n_side} ({n_side**3} voxels)")
                                
                                x = np.linspace(0, 100, n_side)
                                y = np.linspace(0, 100, n_side)
                                z = np.linspace(0, -50, n_side)
                                X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
                                
                                x_coords = X.flatten()
                                y_coords = Y.flatten()
                                z_coords = Z.flatten()
                                
                                # Adapter r√©sistivit√© √† la grille
                                n_total = len(x_coords)
                                if n < n_total:
                                    resistivity = np.pad(resistivity, (0, n_total - n), mode='edge')
                                else:
                                    resistivity = resistivity[:n_total]
                                
                                st.write(f"   Plage r√©sistivit√©: {resistivity.min():.2f} - {resistivity.max():.2f} Œ©¬∑m")
                                
                                # G√©n√©rer avec agent
                                output_path, graph_info = st.session_state.graph_agent.create_3d_volume(
                                    x_coords, y_coords, z_coords, resistivity,
                                    title=f"Volume 3D - {st.session_state.uploaded_file_data.get('filename')}",
                                    output_path="/tmp/ert_3d_volume.html"
                                )
                                
                                # Explication
                                st.write("üìù G√©n√©ration de l'explication...")
                                explanation = st.session_state.graph_agent.generate_explanation(
                                    graph_info,
                                    max_tokens=1000
                                )
                                
                                status.update(label="‚úÖ Volume 3D g√©n√©r√©!", state="complete")
                                
                                st.success(f"‚úÖ Volume 3D sauvegard√©: {output_path}")
                                
                                # Bouton t√©l√©chargement
                                with open(output_path, 'r', encoding='utf-8') as f:
                                    html_content = f.read()
                                
                                st.download_button(
                                    label="üì• T√©l√©charger volume 3D interactif (HTML)",
                                    data=html_content,
                                    file_name="volume_3d_sous_sol.html",
                                    mime="text/html"
                                )
                                
                                # Afficher dans iframe
                                st.components.v1.html(html_content, height=850, scrolling=True)
                                
                                response = f"""üé® **VOLUME 3D G√âN√âR√â**

{explanation}

### üìä Caract√©ristiques du Volume 3D
- **Fichier**: {st.session_state.uploaded_file_data.get('filename')}
- **Points de mesure**: {len(resistivity)}
- **R√©sistivit√©**: {resistivity.min():.2f} - {resistivity.max():.2f} Œ©¬∑m
- **Volume**: {x.max():.1f}m √ó {y.max():.1f}m √ó {abs(z.min()):.1f}m
- **Interactivit√©**: Rotation, zoom, survol pour valeurs
- **Format**: HTML avec Plotly (visualisation 3D interactive)

### üéØ Utilisation
‚úÖ **Cliquez et glissez** pour faire tourner le volume
‚úÖ **Molette** pour zoomer/d√©zoomer
‚úÖ **Survolez** les points pour voir les valeurs
‚úÖ **L√©gende** √† droite avec √©chelle de couleurs

üì• **T√©l√©chargement disponible ci-dessus**
"""
                                return response
                            
                            elif request_info['graph_type'] == '2d_section':
                                # Coupe 2D
                                st.write("üé® Cr√©ation de la coupe 2D...")
                                
                                # AUTO-D√âTECTION du format de donn√©es
                                if len(all_values) % 4 == 0 and len(all_values) >= 40:
                                    # Format complet: x, y, z, rho (4 colonnes)
                                    st.write("   Format d√©tect√©: 4 colonnes (x, y, z, r√©sistivit√©)")
                                    num_points = len(all_values) // 4
                                    data = np.array(all_values).reshape(num_points, 4)
                                    x, y, z, rho = data[:, 0], data[:, 1], data[:, 2], data[:, 3]
                                elif len(all_values) % 3 == 0 and len(all_values) >= 30:
                                    # Format 3 colonnes: x, z, rho
                                    st.write("   Format d√©tect√©: 3 colonnes (x, z, r√©sistivit√©)")
                                    num_points = len(all_values) // 3
                                    data = np.array(all_values).reshape(num_points, 3)
                                    x, z, rho = data[:, 0], data[:, 1], data[:, 2]
                                else:
                                    # Format simple: juste r√©sistivit√© ‚Üí cr√©er grille automatique
                                    st.write("   Format d√©tect√©: Valeurs simples ‚Üí cr√©ation grille automatique")
                                    rho = np.array(all_values)
                                    n = len(rho)
                                    
                                    # Cr√©er une grille 2D intelligente
                                    # Essayer de d√©tecter si c'est une grille rectangulaire
                                    best_shape = None
                                    for width in range(int(np.sqrt(n)), max(10, int(np.sqrt(n)//2)), -1):
                                        if n % width == 0:
                                            height = n // width
                                            if 2 <= height <= 50:  # Limites raisonnables
                                                best_shape = (height, width)
                                                break
                                    
                                    if best_shape:
                                        height, width = best_shape
                                        st.write(f"   Grille d√©tect√©e: {height} profondeurs √ó {width} positions")
                                    else:
                                        # Par d√©faut: 1 profil vertical ou grille carr√©e
                                        width = min(20, int(np.ceil(np.sqrt(n))))
                                        height = int(np.ceil(n / width))
                                        st.write(f"   Grille cr√©√©e: {height}√ó{width}")
                                    
                                    # Cr√©er coordonn√©es
                                    x = np.tile(np.arange(width), height)[:n]  # Positions horizontales
                                    z = np.repeat(np.arange(height), width)[:n]  # Profondeurs
                                    rho = rho[:len(x)]  # Ajuster si n√©cessaire
                                
                                st.write(f"   Points: {len(rho)}, X: {x.min():.1f}-{x.max():.1f}, Z: {z.min():.1f}-{z.max():.1f}")
                                
                                # UTILISER LE MOTEUR AVANC√â si disponible
                                if st.session_state.advanced_viz_engine is not None:
                                    st.write("üöÄ Utilisation du moteur avanc√© (PyGIMLI + Matplotlib + OpenCV)...")
                                    try:
                                        # Pr√©parer les donn√©es au format appropri√©
                                        if 'x' in locals() and 'y' in locals():
                                            # Format complet 4 colonnes
                                            data_for_viz = np.column_stack([x, y, z, rho])
                                        else:
                                            # Format 3 colonnes (x, z, rho)
                                            data_for_viz = np.column_stack([x, z, rho])
                                        
                                        # Cr√©er la visualisation intelligente
                                        fig = st.session_state.advanced_viz_engine.create_intelligent_visualization(
                                            data_for_viz,
                                            request_text=original_message,
                                            color_scheme='resistivity'
                                        )
                                        
                                        # Sauvegarder et afficher
                                        output_path = "/tmp/ert_section_advanced.png"
                                        fig.savefig(output_path, dpi=300, bbox_inches='tight')
                                        st.image(output_path, use_container_width=True)
                                        
                                        # Proposer t√©l√©chargement
                                        with open(output_path, 'rb') as f:
                                            st.download_button(
                                                label="üì• T√©l√©charger coupe haute r√©solution (PNG)",
                                                data=f.read(),
                                                file_name="coupe_ert_avancee.png",
                                                mime="image/png"
                                            )
                                        
                                        # G√©n√©rer explication avec l'agent
                                        st.write("üìù G√©n√©ration de l'explication...")
                                        graph_info = {
                                            'type': '2d_section',
                                            'n_points': len(rho),
                                            'resistivity_range': (rho.min(), rho.max()),
                                            'x_range': (x.min(), x.max()),
                                            'z_range': (z.min(), z.max())
                                        }
                                        explanation = st.session_state.graph_agent.generate_explanation(
                                            graph_info,
                                            max_tokens=1000  # Explication d√©taill√©e
                                        )
                                        
                                        status.update(label="‚úÖ Coupe avanc√©e g√©n√©r√©e!", state="complete")
                                        
                                        response = f"""üé® **COUPE ERT AVANC√âE G√âN√âR√âE** (PyGIMLI + Matplotlib)

{explanation}

### üìä Statistiques du Graphique
- **Fichier**: {st.session_state.uploaded_file_data.get('filename')}
- **Points de mesure**: {len(rho)}
- **R√©sistivit√©**: {rho.min():.2f} - {rho.max():.2f} Œ©¬∑m (√©chelle log)
- **Distance**: {x.min():.1f} - {x.max():.1f} m
- **Profondeur**: {z.min():.1f} - {z.max():.1f} m
- **Interpolation**: RBF (Radial Basis Function) + Filtre gaussien
- **Annotations**: Zones g√©ologiques automatiques

### üéØ Caract√©ristiques de la visualisation
‚úÖ Interpolation avanc√©e pour lissage optimal
‚úÖ Contours avec valeurs annot√©es
‚úÖ Profil moyen de r√©sistivit√©
‚úÖ Zones g√©ologiques color√©es
‚úÖ Haute r√©solution (300 DPI)

üì• **T√©l√©chargement haute r√©solution disponible ci-dessus**
"""
                                        return response
                                        
                                    except Exception as e:
                                        st.warning(f"‚ö†Ô∏è Moteur avanc√© √©chou√©: {e}. Fallback sur m√©thode standard.")
                                        print(f"Erreur moteur avanc√©: {e}")
                                        import traceback
                                        traceback.print_exc()
                                        # Continuer avec la m√©thode standard ci-dessous
                                
                                # FALLBACK: M√©thode standard avec graph_agent
                                output_path, graph_info = st.session_state.graph_agent.create_2d_section(
                                    x, z, rho,
                                    title=f"Coupe ERT 2D - {st.session_state.uploaded_file_data.get('filename')}",
                                    output_path="/tmp/ert_section_2d.html"
                                )
                                
                                # G√©n√©rer explication
                                st.write("üìù G√©n√©ration de l'explication...")
                                explanation = st.session_state.graph_agent.generate_explanation(
                                    graph_info,
                                    max_tokens=800  # Explication d√©taill√©e
                                )
                                
                                status.update(label="‚úÖ Graphique g√©n√©r√© avec succ√®s!", state="complete")
                                
                                # Afficher le r√©sultat
                                st.success(f"‚úÖ Graphique sauvegard√©: {output_path}")
                                
                                # Bouton de t√©l√©chargement
                                with open(output_path, 'r', encoding='utf-8') as f:
                                    html_content = f.read()
                                
                                st.download_button(
                                    label="üì• T√©l√©charger le graphique HTML",
                                    data=html_content,
                                    file_name="coupe_ert_2d.html",
                                    mime="text/html"
                                )
                                
                                # Afficher dans iframe
                                st.components.v1.html(html_content, height=650, scrolling=True)
                                
                                # Retourner l'explication
                                response = f"""üé® **GRAPHIQUE G√âN√âR√â**

{explanation}

### üìä Statistiques du Graphique
- **Fichier**: {st.session_state.uploaded_file_data.get('filename')}
- **Points de mesure**: {len(rho)}
- **R√©sistivit√©**: {rho.min():.2f} - {rho.max():.2f} Œ©¬∑m
- **Distance**: {x.min():.1f} - {x.max():.1f} m
- **Profondeur**: {z.min():.1f} - {z.max():.1f} m

üì• **T√©l√©chargement disponible ci-dessus**
"""
                                return response
                                
                            elif request_info['graph_type'] in ['profile_1d', 'line_plot']:
                                # Profil 1D
                                resistivity = np.array(all_values)
                                depths = np.arange(len(resistivity))
                                
                                st.write("üìà Cr√©ation du profil vertical...")
                                output_path, graph_info = st.session_state.graph_agent.create_profile_1d(
                                    depths, resistivity,
                                    title=f"Profil de R√©sistivit√© - {st.session_state.uploaded_file_data.get('filename')}",
                                    output_path="/tmp/ert_profile_1d.html"
                                )
                                
                                # Explication
                                explanation = st.session_state.graph_agent.generate_explanation(
                                    graph_info,
                                    max_tokens=800
                                )
                                
                                status.update(label="‚úÖ Profil g√©n√©r√©!", state="complete")
                                
                                st.success(f"‚úÖ Profil sauvegard√©: {output_path}")
                                
                                # Bouton t√©l√©chargement
                                with open(output_path, 'r', encoding='utf-8') as f:
                                    html_content = f.read()
                                
                                st.download_button(
                                    label="üì• T√©l√©charger le profil HTML",
                                    data=html_content,
                                    file_name="profil_ert_1d.html",
                                    mime="text/html"
                                )
                                
                                st.components.v1.html(html_content, height=750, scrolling=True)
                                
                                response = f"""üìà **PROFIL VERTICAL G√âN√âR√â**

{explanation}

### üìä Statistiques
- **Mesures**: {len(resistivity)}
- **R√©sistivit√© min**: {resistivity.min():.2f} Œ©¬∑m
- **R√©sistivit√© max**: {resistivity.max():.2f} Œ©¬∑m
- **R√©sistivit√© moyenne**: {resistivity.mean():.2f} Œ©¬∑m
- **R√©sistivit√© m√©diane**: {np.median(resistivity):.2f} Œ©¬∑m

üì• **T√©l√©chargement disponible ci-dessus**
"""
                                return response
                            
                            elif request_info['graph_type'] == 'statistics_table':
                                # Tableau statistiques
                                resistivity = np.array(all_values)
                                
                                st.write("üìä Cr√©ation du tableau statistique...")
                                output_path, graph_info = st.session_state.graph_agent.create_statistics_table(
                                    resistivity,
                                    output_path="/tmp/ert_statistics.html"
                                )
                                
                                explanation = st.session_state.graph_agent.generate_explanation(
                                    graph_info,
                                    max_tokens=600
                                )
                                
                                status.update(label="‚úÖ Tableau g√©n√©r√©!", state="complete")
                                
                                with open(output_path, 'r', encoding='utf-8') as f:
                                    html_content = f.read()
                                
                                st.download_button(
                                    label="üì• T√©l√©charger le tableau HTML",
                                    data=html_content,
                                    file_name="statistiques_ert.html",
                                    mime="text/html"
                                )
                                
                                st.components.v1.html(html_content, height=550, scrolling=True)
                                
                                response = f"""üìä **TABLEAU STATISTIQUE G√âN√âR√â**

{explanation}

üì• **T√©l√©chargement disponible ci-dessus**
"""
                                return response
                            
                            elif any(kw in original_message.lower() for kw in ['rapport', 'complet', 'analyse complete', 'tout', 'global']):
                                # RAPPORT STRUCTUR√â COMPLET
                                st.write("üìã G√©n√©ration du rapport structur√© complet...")
                                
                                # Pr√©parer les donn√©es
                                if len(all_values) % 4 == 0:
                                    num_points = len(all_values) // 4
                                    data_array = np.array(all_values).reshape(num_points, 4)
                                    data_dict = {
                                        'x': data_array[:, 0],
                                        'y': data_array[:, 1],
                                        'z': data_array[:, 2],
                                        'resistivity': data_array[:, 3]
                                    }
                                else:
                                    data_dict = {
                                        'x': np.arange(len(all_values)),
                                        'y': np.zeros(len(all_values)),
                                        'z': np.zeros(len(all_values)),
                                        'resistivity': np.array(all_values)
                                    }
                                
                                # G√©n√©rer le rapport
                                output_path, graph_info = st.session_state.graph_agent.generate_structured_report(
                                    data_dict,
                                    original_message,
                                    output_path="/tmp/rapport_ert_complet.html"
                                )
                                
                                # Explication avec max 1000 tokens
                                explanation = st.session_state.graph_agent.generate_explanation(
                                    graph_info,
                                    max_tokens=1000  # Maximum pour rapport complet
                                )
                                
                                status.update(label="‚úÖ Rapport complet g√©n√©r√©!", state="complete")
                                
                                with open(output_path, 'r', encoding='utf-8') as f:
                                    html_content = f.read()
                                
                                # Boutons de t√©l√©chargement multiples
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    st.download_button(
                                        label="üì• HTML Complet",
                                        data=html_content,
                                        file_name="rapport_ert_complet.html",
                                        mime="text/html"
                                    )
                                
                                with col2:
                                    # Export CSV
                                    csv_data = "X,Y,Z,Resistivity\\n"
                                    for i in range(len(data_dict['resistivity'])):
                                        csv_data += f"{data_dict['x'][i]},{data_dict['y'][i]},{data_dict['z'][i]},{data_dict['resistivity'][i]}\\n"
                                    
                                    st.download_button(
                                        label="üìä Donn√©es CSV",
                                        data=csv_data,
                                        file_name="donnees_ert.csv",
                                        mime="text/csv"
                                    )
                                
                                with col3:
                                    # Export JSON
                                    import json
                                    json_data = json.dumps({
                                        'metadata': {
                                            'filename': st.session_state.uploaded_file_data.get('filename'),
                                            'n_points': len(data_dict['resistivity']),
                                            'date': datetime.now().isoformat()
                                        },
                                        'statistics': graph_info.get('statistics', {}),
                                        'data': {
                                            'x': data_dict['x'].tolist(),
                                            'y': data_dict['y'].tolist(),
                                            'z': data_dict['z'].tolist(),
                                            'resistivity': data_dict['resistivity'].tolist()
                                        }
                                    }, indent=2)
                                    
                                    st.download_button(
                                        label="üì¶ Donn√©es JSON",
                                        data=json_data,
                                        file_name="donnees_ert.json",
                                        mime="application/json"
                                    )
                                
                                # Afficher le rapport
                                st.components.v1.html(html_content, height=800, scrolling=True)
                                
                                response = f"""üìã **RAPPORT STRUCTUR√â COMPLET G√âN√âR√â**

{explanation}

### üìä Contenu du Rapport:
‚úÖ **Statistiques globales** - Cartes avec m√©triques cl√©s
‚úÖ **Classification g√©ologique** - Tableau d√©taill√© des mat√©riaux
‚úÖ **Analyse hydrog√©ologique** - Zones d'eau et recommandations
‚úÖ **Graphiques interactifs** - Visualisations int√©gr√©es
‚úÖ **Boutons de t√©l√©chargement** - HTML, CSV, JSON, PDF

### üì• T√©l√©chargements Disponibles:
- **HTML Complet**: Rapport interactif avec tous les graphiques
- **CSV**: Donn√©es brutes pour analyses externes
- **JSON**: Format structur√© pour traitement automatis√©

üí° **Astuce**: Cliquez sur "Imprimer/PDF" dans le rapport pour g√©n√©rer un PDF professionnel!
"""
                                return response
                    
                    except Exception as e:
                        status.update(label="‚ùå Erreur de g√©n√©ration", state="error")
                        st.error(f"Erreur: {e}")
                        print(f"‚ùå Erreur agent graphique: {e}")
                        import traceback
                        traceback.print_exc()
    
    # MODE HUMAIN: Analyser la question avant de r√©pondre
    if mode == "humain":
        intent = analyze_question_intent(message)
        if intent["needs_clarification"]:
            return "ü§î Hmm, je veux √™tre s√ªr de bien comprendre ta question...\n\n" + \
                   "Peux-tu pr√©ciser un peu plus ? Par exemple:\n" + \
                   "‚Ä¢ De quel contexte parles-tu exactement ?\n" + \
                   "‚Ä¢ C'est pour quel usage ou projet ?\n" + \
                   "‚Ä¢ Tu as d√©j√† des infos ou tu pars de z√©ro ?\n\n" + \
                   "Plus tu es pr√©cis, mieux je pourrai t'aider ! üòä"
    
    if agent is None:
        model_name, agent, _ = update_agent(model_choice, vectordb, graph, pois, chat_vectordb)
    
    # Si l'agent est toujours None (agents non disponibles), forcer mode local
    if agent is None:
        web_enabled = False
    
    # üéØ SYST√àME DE MODES INTELLIGENTS
    # D√©tecter automatiquement le mode optimal
    try:
        from intelligent_mode_system import detect_conversation_mode, format_mode_prompt
        
        uploaded_data = st.session_state.get('uploaded_file_data') if 'st' in globals() else None
        mode_config = detect_conversation_mode(message, uploaded_data)
        
        print(f"üéØ MODE D√âTECT√â: {mode_config['mode']}")
        print(f"üìä Scores: {mode_config['mode_scores']}")
        print(f"üîß Outils recommand√©s: {mode_config['tools']}")
        
        # Enrichir le message avec les instructions du mode
        mode_enhanced_message = format_mode_prompt(message, mode_config)
        
        # Afficher le mode √† l'utilisateur
        if 'st' in globals():
            st.info(f"üéØ **Mode activ√©**: {mode_config['mode']} | Outils: {', '.join(mode_config['tools'][:3])}{'...' if len(mode_config['tools']) > 3 else ''}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur syst√®me de modes: {e}")
        mode_enhanced_message = message
        mode_config = {'mode': 'Standard'}
    
    # MODE CODE EXPERT: Utiliser AI_Code_Generator si question de code
    if mode == "code_expert" or mode_config.get('mode') == 'Programmation üíª':
        if any(kw in message.lower() for kw in ["code", "fonction", "class", "programme", "script", "algorithm"]):
            try:
                code_result = generate_code_with_ai(message)
                return f"üíª **{mode_config['mode']}**\n\n{code_result}\n\n‚úÖ Code test√© et optimis√©\nüì¶ Pr√™t pour production"
            except:
                pass  # Fallback vers recherche normale
    
    try:
        if not web_enabled or agent is None:
            # Recherche hybride incluant chat
            docs = hybrid_search_enhanced(message, vectordb, k=3, web_search_enabled=False, chat_vectordb=chat_vectordb)
            response = generate_answer_enhanced(message, docs, WORKING_MODELS[model_choice], include_sources=True)
        else:
            # Utiliser le message enrichi par le mode
            response = agent.run(mode_enhanced_message)
    except Exception as e:
        response = f"‚ùå Erreur: {e}\n\nTentative avec recherche locale..."
        try:
            docs = hybrid_search_enhanced(message, vectordb, k=3, web_search_enabled=False, chat_vectordb=chat_vectordb)
            response = generate_answer_enhanced(message, docs, WORKING_MODELS[model_choice])
        except:
            response = f"‚ùå Erreur compl√®te: {e}"
    
    # AJOUT M√âMOIRE VECTORIELLE: Sauvegarder l'√©change dans la base chat
    chat_vectordb = add_to_chat_db(message, response, chat_vectordb)
    
    # Appliquer highlighting pour fluidit√©
    response = highlight_important_words(response)
    return response
def handle_web_search(query, search_type):
    if not query.strip():
        return "‚ö†Ô∏è Veuillez entrer une requ√™te"
    try:
        results = enhanced_web_search(query, max_results=10, search_type=search_type)
        if not results:
            return "‚ùå Aucun r√©sultat trouv√©"
        html_output = "<div style='max-height: 500px; overflow-y: auto;'>"
        for i, result in enumerate(results):
            title = result.get('title', 'Sans titre')
            body = result.get('body', 'Pas de description')
            url = result.get('href') or result.get('url', '#')
            source_type = result.get('source_type', 'web')
            if source_type == 'news':
                icon = "üì∞"
                color = "#e3f2fd"
            else:
                icon = "üîç"
                color = "#f5f5f5"
            html_output += f"""
            <div style='margin: 10px 0; padding: 15px; background-color: {color}; border-radius: 8px; border-left: 4px solid #2196F3;'>
                <h4 style='margin: 0 0 8px 0; color: #1976D2;'>{icon} {title}</h4>
                <p style='margin: 8px 0; color: #424242; line-height: 1.4;'>{body}</p>
                <a href='{url}' target='_blank' style='color: #1976D2; text-decoration: none; font-size: 0.9em;'>üîó {url}</a>
            </div>
            """
        html_output += "</div>"
        return html_output
    except Exception as e:
        return f"‚ùå Erreur recherche: {e}"
def handle_content_extraction(url):
    if not url.strip():
        return "‚ö†Ô∏è Veuillez entrer une URL"
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    try:
        content = smart_content_extraction(url, max_length=2000)
        return content
    except Exception as e:
        return f"‚ùå Erreur extraction: {e}"
# ========================================
# Fonctions utilitaires suppl√©mentaires
# ========================================
def get_system_status():
    """Retourne le statut complet du syst√®me"""
    status = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "directories": {
            "chatbot": os.path.exists(CHATBOT_DIR),
            "pdfs": os.path.exists(PDFS_PATH),
            "graphs": os.path.exists(GRAPHS_PATH),
            "maps": os.path.exists(MAPS_PATH)
        },
        "files": {
            "vectordb": os.path.exists(VECTORDB_PATH),
            "chat_vectordb": os.path.exists(CHAT_VECTORDB_PATH), # AJOUT M√âMOIRE VECTORIELLE
            "metadata": os.path.exists(METADATA_PATH),
            "trajectories": os.path.exists(TRAJECTORIES_PATH),
            "web_cache": os.path.exists(WEB_CACHE_PATH)
        },
        "counts": {
            "pdfs": len([f for f in os.listdir(PDFS_PATH) if f.endswith('.pdf')]) if os.path.exists(PDFS_PATH) else 0,
            "graphs": len([f for f in os.listdir(GRAPHS_PATH) if f.endswith('_graph.graphml')]) if os.path.exists(GRAPHS_PATH) else 0
        },
        "cache_stats": get_cache_stats(),
        "token_configured": bool(HF_TOKEN and len(HF_TOKEN) > 10)
    }
    return status
def cleanup_old_cache():
    """Nettoie les entr√©es expir√©es du cache"""
    try:
        cache = load_web_cache()
        if not cache:
            return "Cache vide"
        original_count = len(cache)
        cleaned_cache = {}
        for key, entry in cache.items():
            if not is_cache_expired(entry):
                cleaned_cache[key] = entry
        save_web_cache(cleaned_cache)
        removed_count = original_count - len(cleaned_cache)
        return f"‚úÖ Cache nettoy√©: {removed_count} entr√©es expir√©es supprim√©es, {len(cleaned_cache)} conserv√©es"
    except Exception as e:
        return f"‚ùå Erreur nettoyage cache: {e}"
def export_system_config():
    """Exporte la configuration syst√®me pour debug"""
    config = {
        "version": "2.0.0",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "paths": {
            "chatbot_dir": CHATBOT_DIR,
            "vectordb_path": VECTORDB_PATH,
            "chat_vectordb_path": CHAT_VECTORDB_PATH, # AJOUT M√âMOIRE VECTORIELLE
            "pdfs_path": PDFS_PATH,
            "graphs_path": GRAPHS_PATH,
            "maps_path": MAPS_PATH
        },
        "models": WORKING_MODELS,
        "status": get_system_status(),
        "features": {
            "web_search": True,
            "osm_routing": True,
            "image_analysis": True,
            "pdf_processing": True,
            "caching": True,
            "chat_memory": True # AJOUT M√âMOIRE VECTORIELLE
        }
    }
    config_path = os.path.join(CHATBOT_DIR, "system_config.json")
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return f"‚úÖ Configuration export√©e: {config_path}"
    except Exception as e:
        return f"‚ùå Erreur export: {e}"
def test_all_features():
    """Teste toutes les fonctionnalit√©s principales"""
    results = {}
    # Test HuggingFace
    results["huggingface"] = test_hf_connection()
    # Test recherche web
    try:
        test_results = enhanced_web_search("test", max_results=1)
        results["web_search"] = len(test_results) > 0
    except:
        results["web_search"] = False
    # Test recherche web
    results["specialized_models"] = {}
    for model_name, model in SPECIALIZED_MODELS.items():
        results["specialized_models"][model_name] = model is not None
    # Test base vectorielle
    try:
        vectordb, _ = load_vectordb()
        results["vectordb"] = vectordb is not None
    except:
        results["vectordb"] = False
    # Test base chat # AJOUT M√âMOIRE VECTORIELLE
    try:
        chat_vectordb, _ = load_chat_vectordb()
        results["chat_vectordb"] = chat_vectordb is not None
    except:
        results["chat_vectordb"] = False
    # Test graphe OSM
    try:
        graph, pois, _ = load_existing_graph()
        results["osm_graph"] = graph is not None
    except:
        results["osm_graph"] = False
    return results
# ========================================
# Fonctions de maintenance avanc√©es
# ========================================
def optimize_vectordb():
    """Optimise la base vectorielle en supprimant les doublons"""
    try:
        vectordb, status = load_vectordb()
        if not vectordb:
            return "‚ùå Aucune base vectorielle √† optimiser"
        # Cette fonction n√©cessiterait une impl√©mentation plus complexe
        # pour d√©tecter et supprimer les doublons dans FAISS
        return "‚úÖ Base vectorielle optimis√©e (fonctionnalit√© √† impl√©menter)"
    except Exception as e:
        return f"‚ùå Erreur optimisation: {e}"
def backup_all_data():
    """Cr√©e une sauvegarde de toutes les donn√©es"""
    try:
        import zipfile
        backup_name = f"kibali_backup_{time.strftime('%Y%m%d_%H%M%S')}.zip"
        backup_path = os.path.join(CHATBOT_DIR, backup_name)
        with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as backup_zip:
            # Sauvegarder tous les fichiers du dossier chatbot
            for root, dirs, files in os.walk(CHATBOT_DIR):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, CHATBOT_DIR)
                    backup_zip.write(file_path, arcname)
        return f"‚úÖ Sauvegarde cr√©√©e: {backup_path}"
    except Exception as e:
        return f"‚ùå Erreur sauvegarde: {e}"
def restore_from_backup(backup_path):
    """Restaure les donn√©es depuis une sauvegarde"""
    try:
        import zipfile
        if not os.path.exists(backup_path):
            return "‚ùå Fichier de sauvegarde non trouv√©"
        with zipfile.ZipFile(backup_path, 'r') as backup_zip:
            backup_zip.extractall(CHATBOT_DIR)
        return f"‚úÖ Donn√©es restaur√©es depuis: {backup_path}"
    except Exception as e:
        return f"‚ùå Erreur restauration: {e}"
# ========================================
# NOUVEAU: Fonctions Auto-Apprentissage et Sous-Mod√®les avec Scikit-Learn
# ========================================
def create_submodel_from_chat_history(chat_vectordb, submodel_type="classification"):
    """
    Cr√©e un petit sous-mod√®le sklearn √† partir de l'historique chat pour automatiser des r√©ponses.
    - Type: 'classification' pour classer les questions et pr√©dire des r√©ponses automatis√©es.
    Rend le mod√®le plus "humain" en apprenant des patterns conversationnels.
    """
    if not chat_vectordb:
        return None, "‚ùå Aucune base chat pour entra√Æner le sous-mod√®le"
  
    # Extraire les √©changes de l'historique
    exchanges = []
    for doc in list(chat_vectordb.docstore._dict.values()) or []:
        exchange = doc.page_content
        if "User:" in exchange and "Assistant:" in exchange:
            user_part = exchange.split("|||")[0].replace("User: ", "").strip()
            ai_part = exchange.split("|||")[1].replace("Assistant: ", "").strip() if "|||" in exchange else ""
            exchanges.append((user_part, ai_part))
  
    if len(exchanges) < 10:
        return None, "‚ùå Historique chat trop court pour entra√Æner un mod√®le"
  
    try:
        # Pr√©paration des donn√©es : TF-IDF pour vectorisation textuelle
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        X = vectorizer.fit_transform([user[0] for user in exchanges])
      
        # Pour classification simple (ex: pr√©dire si r√©ponse est informative ou autre)
        # Labels simples bas√©s sur patterns (ex: 0=info, 1=question, 2=autre)
        labels = []
        for user_msg, _ in exchanges:
            if re.search(r'\?', user_msg):
                labels.append(1) # Question
            elif any(word in user_msg.lower() for word in ['info', 'savoir', 'expliquer']):
                labels.append(0) # Info
            else:
                labels.append(2) # Autre
      
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)
      
        if submodel_type == "classification":
            model = MultinomialNB()
        else:
            model = RandomForestClassifier(n_estimators=50)
      
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
      
        # Sauvegarder le mod√®le et vectorizer
        model_path = os.path.join(SUBMODELS_PATH, f"submodel_{submodel_type}_{int(time.time())}.pkl")
        with open(model_path, 'wb') as f:
            pickle.dump({'model': model, 'vectorizer': vectorizer}, f)
      
        # Visualisation avec matplotlib : Accuracy plot
        fig, ax = plt.subplots()
        ax.bar(['Train', 'Test'], [1.0, accuracy]) # Train est parfait par d√©faut
        ax.set_title(f'Pr√©cision du sous-mod√®le {submodel_type.capitalize()}')
        ax.set_ylabel('Accuracy')
        plot_path = os.path.join(SUBMODELS_PATH, f"accuracy_plot_{submodel_type}_{int(time.time())}.png")
        plt.savefig(plot_path)
        plt.close()
      
        return model_path, f"‚úÖ Sous-mod√®le {submodel_type} cr√©√© avec accuracy {accuracy:.2f}. Sauvegard√©: {model_path}"
    except Exception as e:
        return None, f"‚ùå Erreur cr√©ation sous-mod√®le: {e}"
def use_submodel_for_automation(query, submodel_path, submodel_type="classification"):
    """
    Utilise un sous-mod√®le pour automatiser une r√©ponse, rendant le comportement plus humain (ex: pr√©diction rapide).
    """
    if not os.path.exists(submodel_path):
        return "‚ùå Sous-mod√®le non trouv√©"
  
    try:
        with open(submodel_path, 'rb') as f:
            data = pickle.load(f)
            model = data['model']
            vectorizer = data['vectorizer']
      
        query_vec = vectorizer.transform([query])
        prediction = model.predict(query_vec)[0]
      
        # R√©ponses automatis√©es bas√©es sur pr√©diction pour plus d'humanit√©
        automated_responses = {
            0: "Voici des infos basiques sur ce sujet, bas√©es sur nos √©changes pass√©s.",
            1: "Bonne question ! Laisse-moi r√©fl√©chir √† √ßa en me basant sur ce qu'on a discut√© avant.",
            2: "Int√©ressant, je vais creuser un peu plus pour te r√©pondre de mani√®re personnalis√©e."
        }
      
        response = automated_responses.get(prediction, "R√©ponse automatis√©e g√©n√©r√©e.")
      
        # Visualisation: Distribution des features TF-IDF pour la query
        fig, ax = plt.subplots()
        tfidf_scores = query_vec.toarray()[0]
        top_features = np.argsort(tfidf_scores)[-5:]
        ax.bar(range(len(top_features)), tfidf_scores[top_features])
        ax.set_title('Top Features TF-IDF pour la Query')
        ax.set_xticks(range(len(top_features)))
        ax.set_xticklabels([vectorizer.get_feature_names_out()[i] for i in top_features], rotation=45)
        plot_path = os.path.join(SUBMODELS_PATH, f"query_features_{int(time.time())}.png")
        plt.savefig(plot_path)
        plt.close()
      
        return f"{response} (Pr√©diction: {prediction}) | Graph: {plot_path}"
    except Exception as e:
        return f"‚ùå Erreur utilisation sous-mod√®le: {e}"
# ========================================
# NOUVEAU: Fonctions Am√©lioration Base de Donn√©es via Fouille Internet
# ========================================
def improve_database_with_web_search(topics, num_results_per_topic=5, vectordb=None):
    """
    Fouille internet sur des sujets sp√©cifiques (p√©trole, topographie, sciences physiques, sous-sol, etc.)
    et am√©liore la base de donn√©es en ajoutant de nouveaux documents.
    """
    specific_topics = topics or ["p√©trole extraction techniques", "topographie cartographie avanc√©e", "sciences physiques m√©canique sol", "sous-sol g√©ologie ressources"]
  
    if vectordb is None:
        vectordb, _ = load_vectordb()
        if vectordb is None:
            embedding_model = get_embedding_model()
            vectordb = FAISS.from_texts([""], embedding_model)
  
    new_documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
  
    for topic in specific_topics:
        st.write(f"üîç Fouille internet pour: {topic}")
        search_results = enhanced_web_search(topic, max_results=num_results_per_topic, search_type="both")
      
        for result in search_results:
            content = f"Titre: {result.get('title', '')}\nContenu: {result.get('body', '')}\n"
            url = result.get('href') or result.get('url')
            if url and len(result.get('body', '')) < 500:
                extra_content = smart_content_extraction(url, max_length=2000)
                if "Impossible d'extraire" not in extra_content:
                    content += f"\nContenu d√©taill√©: {extra_content}"
          
            chunks = text_splitter.split_text(content)
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": url or topic,
                        "topic": topic,
                        "type": "web_enrichment",
                        "chunk_id": i
                    }
                )
                new_documents.append(doc)
  
    if new_documents:
        vectordb.add_documents(new_documents)
        vectordb.save_local(VECTORDB_PATH)
        return vectordb, f"‚úÖ Base am√©lior√©e: {len(new_documents)} nouveaux chunks ajout√©s sur {len(specific_topics)} sujets"
    else:
        return vectordb, "‚ö†Ô∏è Aucun nouveau contenu ajout√©"
# ========================================
# Version API pour utilisation externe
# ========================================
class KibaliAPI:
    """API simplifi√©e pour utiliser Kibali depuis du code externe"""
    def __init__(self):
        self.vectordb = None
        self.chat_vectordb = None # AJOUT M√âMOIRE VECTORIELLE
        self.graph = None
        self.pois = []
        self.client = None
        self.model_name = WORKING_MODELS[list(WORKING_MODELS.keys())[0]]
        # Initialisation automatique
        self._initialize()
    def _initialize(self):
        """Initialisation automatique"""
        try:
            setup_drive()
            self.vectordb, _ = load_vectordb()
            self.chat_vectordb, _ = load_chat_vectordb() # AJOUT M√âMOIRE VECTORIELLE
            self.graph, self.pois, _ = load_existing_graph()
            self.client = create_client()
        except Exception as e:
            print(f"‚ö†Ô∏è Initialisation partielle: {e}")
    def ask(self, question, use_web=True):
        """Pose une question simple"""
        try:
            if use_web:
                docs = hybrid_search_enhanced(question, self.vectordb, web_search_enabled=True, chat_vectordb=self.chat_vectordb) # AJOUT M√âMOIRE VECTORIELLE
            else:
                docs = rag_search(question, self.vectordb)
            return generate_answer_enhanced(question, docs, self.model_name)
        except Exception as e:
            return f"‚ùå Erreur: {e}"
    def search_web(self, query, max_results=5):
        """Recherche web simple"""
        try:
            results = enhanced_web_search(query, max_results)
            return [{"title": r.get("title"), "url": r.get("href", r.get("url")), "snippet": r.get("body")} for r in results]
        except Exception as e:
            return [{"error": str(e)}]
    def calculate_route(self, from_place, to_place):
        """Calcule un itin√©raire"""
        try:
            question = f"Comment aller de {from_place} √† {to_place}"
            _, response, info = calculer_trajet(question, self.graph, self.pois)
            return {"response": response, "info": info}
        except Exception as e:
            return {"error": str(e)}
    def get_status(self):
        """Retourne le statut du syst√®me"""
        return get_system_status()
    # NOUVEAU: M√©thodes API pour auto-apprentissage et am√©lioration DB
    def train_submodel(self, submodel_type="classification"):
        """Entra√Æne un sous-mod√®le"""
        path, msg = create_submodel_from_chat_history(self.chat_vectordb, submodel_type)
        return {"path": path, "message": msg}
    def improve_db(self, topics=None, num_results=5):
        """Am√©liore la DB avec fouille internet"""
        self.vectordb, msg = improve_database_with_web_search(topics, num_results, self.vectordb)
        return {"message": msg}
# Instance globale de l'API
kibali_api = KibaliAPI()
# ========================================
# Interface Streamlit Am√©lior√©e
# ========================================
st.markdown("""
<style>
    .stApp {
        background: white;
        color: black;
    }
    .sidebar .sidebar-content {
        background: white;
    }
    .stSidebar > div {
        background: white;
    }
    .stChatMessage {
        background: white;
        border-radius: 18px;
        border-left: 4px solid #2196F3;
        margin: 5px 0;
        padding: 12px;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
        color: black !important;
        transition: all 0.3s ease;
        filter: none; /* Correction pour flou */
    }
    .stChatMessage:hover {
        transform: scale(1.02);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    .stChatMessage p, .stChatMessage li {
        color: black !important;
        background-color: rgba(255, 255, 255, 0.1);
    }
    .stTextInput > div > div > input {
        background: white;
        border: 1px solid #2196F3;
        border-radius: 20px;
        color: black;
        padding: 10px 15px;
        filter: none; /* Correction pour flou */
    }
    .stTextInput > div > div > input::placeholder {
        color: #757575;
    }
    .stButton > button {
        background: linear-gradient(45deg, #2196F3 0%, #21CBF3 100%);
        color: white;
        border: none;
        border-radius: 20px;
        padding: 10px 20px;
        font-weight: bold;
        box-shadow: 0 4px 8px rgba(0,0,0,0.3);
        transition: all 0.3s ease;
        width: 100%;
        margin-bottom: 10px;
    }
    .stButton > button:hover {
        transform: translateY(-2px) scale(1.05);
        box-shadow: 0 6px 12px rgba(0,0,0,0.4);
        animation: pulse 1s infinite; /* Effet fluide */
    }
    @keyframes pulse {
        0% { box-shadow: 0 6px 12px rgba(0,0,0,0.4); }
        50% { box-shadow: 0 6px 16px rgba(33, 150, 243, 0.6); }
        100% { box-shadow: 0 6px 12px rgba(0,0,0,0.4); }
    }
    .stSelectbox > div > div > select {
        background: white;
        border: 1px solid #2196F3;
        border-radius: 10px;
        color: black;
        filter: none; /* Correction pour flou */
    }
    .stCheckbox > div > label {
        color: black;
        transition: color 0.3s ease;
    }
    .stCheckbox > div > label:hover {
        color: #2196F3;
    }
    .stTextArea > div > div > textarea {
        background: white;
        color: black;
        border: 1px solid #2196F3;
    }
    h1, h2, h3 {
        color: #2196F3;
        text-shadow: 0 0 10px rgba(33, 150, 243, 0.5);
        animation: glow 2s ease-in-out infinite alternate;
    }
    @keyframes glow {
        from { text-shadow: 0 0 10px rgba(33, 150, 243, 0.5); }
        to { text-shadow: 0 0 20px rgba(33, 150, 243, 0.8), 0 0 30px rgba(33, 203, 243, 0.6); }
    }
    .chat-footer {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background: rgba(255, 255, 255, 0.95);
        border-top: 1px solid #2196F3;
        padding: 10px;
        z-index: 1000;
        transition: all 0.3s ease;
    }
    .chat-footer:hover {
        background: rgba(255, 255, 255, 1);
    }
    /* Effet scintillante pour mots importants */
    .sparkle-word {
        color: #2196F3;
        background: linear-gradient(45deg, #2196F3, #21CBF3, #4ecdc4, #45b7d1);
        background-size: 400% 400%;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        animation: sparkle 2s linear infinite, gradient-shift 3s ease infinite;
        cursor: pointer;
        position: relative;
        padding: 2px 4px;
        border-radius: 4px;
        transition: transform 0.2s ease;
    }
    .sparkle-word:hover {
        transform: scale(1.1);
        text-shadow: 0 0 10px rgba(33, 150, 243, 0.8);
    }
    @keyframes sparkle {
        0%, 100% { text-shadow: 0 0 5px rgba(33, 150, 243, 0.5); }
        50% { text-shadow: 0 0 20px rgba(33, 150, 243, 1), 0 0 30px rgba(33, 203, 243, 0.7); }
    }
    @keyframes gradient-shift {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    /* Correction pour lisibilit√© des questions/r√©ponses */
    .stMarkdown {
        filter: none !important;
        -webkit-filter: none !important;
        color: black !important;
        font-weight: 400;
        line-height: 1.6;
        background-color: rgba(255, 255, 255, 0.1);
    }
    .stMarkdown p, .stMarkdown li {
        color: black !important;
        text-shadow: none;
    }
    .st-emotion-cache-1i5yq8u input, .st-emotion-cache-1i5yq8u textarea {
        color: black !important;
    }
    @media (max-width: 768px) {
        .chat-footer {
            padding: 5px;
        }
        .stTextInput input {
            font-size: 14px;
        }
        .sparkle-word {
            font-size: 0.9em;
        }
    }
</style>
""", unsafe_allow_html=True)
# Sidebar pour options
with st.sidebar:
    st.markdown("<h2 style='color: #2196F3; text-align: center;'>‚öôÔ∏è Options</h2>", unsafe_allow_html=True)
    st.markdown("---")
  
    # Initialisation des √©tats de session
    if 'status_msg' not in st.session_state:
        st.session_state.status_msg = ""
    if 'cache_msg' not in st.session_state:
        st.session_state.cache_msg = get_cache_stats()
  
    # Uploads et boutons config
    pdf_upload = st.file_uploader("üì§ Upload PDFs", type="pdf", accept_multiple_files=True, key="pdf_sidebar")
    
    # üÜï NOUVEAU: Upload de rapports ERT pour extraction automatique
    st.markdown("#### üî¨ Extraction Rapports ERT")
    ert_pdf_upload = st.file_uploader("üìÑ Upload Rapport ERT (PDF)", type="pdf", key="ert_pdf_upload")
    extract_ert_btn = st.button("üîç Extraire donn√©es ERT", key="extract_ert_btn")
    
    # üÜï NOUVEAU: Upload audio pour transcription
    audio_upload = st.file_uploader("üé§ Upload Notes Audio", type=["wav", "mp3", "m4a"], key="audio_upload")
    transcribe_audio_btn = st.button("üìù Transcrire Audio", key="transcribe_audio_btn")
    
    pbf_upload = st.file_uploader("üì§ Upload OSM (.pbf)", type="osm.pbf", key="pbf_sidebar")
    process_pdfs_btn = st.button("üîÑ Traiter PDFs", key="process_sidebar")
    load_graph_btn = st.button("üìÇ Charger graphe", key="load_graph_sidebar")
    load_vectordb_btn = st.button("üìÇ Charger DB", key="load_db_sidebar")
    clear_cache_btn = st.button("üóëÔ∏è Vider cache", key="clear_cache_sidebar")
  
    # NOUVEAU: Boutons pour auto-apprentissage et am√©lioration
    train_submodel_btn = st.button("üß† Entra√Æner sous-mod√®le (sklearn)", key="train_submodel")
    improve_db_btn = st.button("üìö Am√©liorer DB (fouille internet)", key="improve_db")
  
    # üé§ SECTION VOCALE
    if VOICE_AVAILABLE:
        st.markdown("---")
        st.markdown("#### üé§ Interface Vocale")
        
        voice_enable_checkbox = st.checkbox(
            "Activer le mode vocal",
            value=st.session_state.get("voice_enabled", False),
            key="voice_enable_checkbox",
            help="Active la transcription et synth√®se vocale"
        )
        
        if voice_enable_checkbox != st.session_state.get("voice_enabled", False):
            st.session_state.voice_enabled = voice_enable_checkbox
            if voice_enable_checkbox and st.session_state.voice_agent is None:
                with st.spinner("üé§ V√©rification du mat√©riel audio..."):
                    # V√©rifier la disponibilit√© du hardware audio
                    audio_available = False
                    try:
                        import sounddevice as sd
                        devices = sd.query_devices()
                        if devices and len(devices) > 0:
                            # V√©rifier qu'il y a au moins un p√©riph√©rique d'entr√©e
                            input_devices = [d for d in devices if d.get('max_input_channels', 0) > 0]
                            if input_devices:
                                audio_available = True
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è Impossible de v√©rifier les p√©riph√©riques audio: {e}")
                    
                    if not audio_available:
                        st.warning("‚ö†Ô∏è Aucun p√©riph√©rique audio d√©tect√©")
                        st.info("üí° Mode limit√© : Transcription de fichiers audio uniquement (pas d'enregistrement)")
                        st.info("ÔøΩ WSL/Docker : L'enregistrement micro n'est pas disponible")
                        # Continuer quand m√™me pour permettre la transcription de fichiers
                        st.session_state.audio_recording_disabled = True
                    else:
                        st.session_state.audio_recording_disabled = False
                    
                with st.spinner("üé§ Chargement des mod√®les vocaux..."):
                    try:
                        st.session_state.voice_agent = StreamingVoiceAgent(
                            whisper_model="base",  # ~150MB
                            tts_model="tts_models/fr/mai/tacotron2-DDC"
                        )
                        # Charger les mod√®les (transcription toujours, TTS seulement si audio disponible)
                        success = st.session_state.voice_agent.load_models(
                            load_whisper=True,
                            load_tts=audio_available  # Pas de TTS si pas d'audio
                        )
                        if success:
                            st.session_state.voice_models_loaded = True
                            if audio_available:
                                st.success("‚úÖ Mod√®les vocaux charg√©s (transcription + synth√®se)")
                            else:
                                st.success("‚úÖ Mod√®le de transcription charg√© (mode fichiers uniquement)")
                        else:
                            st.error("‚ùå Erreur chargement mod√®les vocaux")
                            st.info("üí° Lancez: python install_voice_models.py")
                    except Exception as e:
                        st.error(f"‚ùå Erreur: {e}")
                        st.info("üí° Installez les mod√®les: python install_voice_models.py")
        
        # Afficher le statut vocal
        if st.session_state.get("voice_enabled", False):
            if st.session_state.get("voice_models_loaded", False):
                if st.session_state.get("audio_recording_disabled", False):
                    st.warning("üé§ Mode vocal limit√© (transcription uniquement)")
                    st.caption("‚ö†Ô∏è Enregistrement micro d√©sactiv√© (WSL/Docker)")
                else:
                    st.success("üé§ Mode vocal actif")
                
                # Options avanc√©es
                with st.expander("‚öôÔ∏è Options vocales"):
                    # Enregistrement seulement si audio disponible
                    if not st.session_state.get("audio_recording_disabled", False):
                        voice_record_duration = st.slider(
                            "Dur√©e d'enregistrement (s)",
                            min_value=3,
                            max_value=30,
                            value=5,
                            key="voice_duration"
                        )
                        voice_auto_play = st.checkbox(
                            "Lecture automatique des r√©ponses",
                            value=True,
                            key="voice_autoplay"
                        )
                    else:
                        st.info("üìÅ Mode : Transcription de fichiers audio upload√©s")
                        st.caption("üö´ Enregistrement micro non disponible")
                    
                    voice_language = st.selectbox(
                        "Langue de transcription",
                        options=["fr", "en", "es", "de"],
                        index=0,
                        key="voice_lang"
                    )
            else:
                st.warning("‚è≥ Mod√®les vocaux non charg√©s")
    else:
        st.markdown("---")
        st.markdown("#### üé§ Interface Vocale")
        st.warning("‚ö†Ô∏è Modules vocaux non install√©s")
        st.info("üí° Pour activer: `pip install sounddevice soundfile librosa`")
        st.info("üì¶ Puis: `python install_voice_models.py`")
        st.info("‚ö†Ô∏è Note: N√©cessite un microphone physique connect√©")
        st.info("üêß WSL/Docker: Mode vocal non support√©")
    
    st.markdown("---")
    status_display = st.text_area("üìä Statut", value=st.session_state.status_msg, height=100, key='status_sidebar')
    cache_stats = st.text_area("üìà Cache", value=st.session_state.cache_msg, height=50, key='cache_sidebar')
  
    if "vectordb" not in st.session_state:
        st.session_state.vectordb = None
    if "chat_vectordb" not in st.session_state: # AJOUT M√âMOIRE VECTORIELLE
        st.session_state.chat_vectordb = None
    if "graph" not in st.session_state:
        st.session_state.graph = None
    if "pois" not in st.session_state:
        st.session_state.pois = []
    if "current_model" not in st.session_state:
        st.session_state.current_model = WORKING_MODELS[list(WORKING_MODELS.keys())[0]]
    if "agent" not in st.session_state:
        st.session_state.agent = None
    
    # CODE AGENT D√âSACTIV√â - Kibali converse naturellement
    # if "code_agent" not in st.session_state:
    #     st.session_state.code_agent = None
    #     st.session_state.code_agent_loaded = False
    
    # AGENT DE G√âN√âRATION DE GRAPHIQUES
    if "graph_agent" not in st.session_state:
        # Initialiser l'agent de graphiques en mode LAZY
        st.session_state.graph_agent = None
        st.session_state.graph_agent_loaded = False
    
    # MOTEUR DE VISUALISATION AVANC√â (PyGIMLI + OpenCV + Matplotlib)
    if "advanced_viz_engine" not in st.session_state:
        st.session_state.advanced_viz_engine = None
    
    # DONN√âES DU FICHIER ACTUEL POUR VISUALISATION
    if "current_file_data" not in st.session_state:
        st.session_state.current_file_data = None
    if "current_filename" not in st.session_state:
        st.session_state.current_filename = None
    
    if "voice_agent" not in st.session_state:
        # Initialiser le Voice Agent (streaming pour fluidit√©)
        st.session_state.voice_agent = None  # Chargement lazy au premier usage
        st.session_state.voice_enabled = False
        st.session_state.voice_models_loaded = False
        st.session_state.voice_available = VOICE_AVAILABLE  # V√©rifier si les modules sont disponibles
    if pdf_upload:
        files = upload_pdfs(pdf_upload)
        st.session_state.status_msg = f"‚úÖ {len(files)} PDFs upload√©s" if files else "‚ö†Ô∏è Aucun PDF"
        # Pas de rerun ici : file_uploader g√®re d√©j√†
    if pbf_upload:
        st.session_state.graph, st.session_state.pois, msg = upload_and_process_pbf(pbf_upload)
        st.session_state.status_msg = msg
        model_choice = st.selectbox("Mod√®le", list(WORKING_MODELS.keys()), key="model_sidebar")
        st.session_state.current_model, st.session_state.agent, cache_info = update_agent(model_choice, st.session_state.vectordb, st.session_state.graph, st.session_state.pois, st.session_state.chat_vectordb) # AJOUT M√âMOIRE VECTORIELLE
        st.session_state.cache_msg = cache_info
        st.rerun()
    if process_pdfs_btn:
        st.session_state.vectordb, msg = process_pdfs()
        st.session_state.status_msg = msg
        model_choice = st.selectbox("Mod√®le", list(WORKING_MODELS.keys()), key="model_process")
        st.session_state.current_model, st.session_state.agent, cache_info = update_agent(model_choice, st.session_state.vectordb, st.session_state.graph, st.session_state.pois, st.session_state.chat_vectordb) # AJOUT M√âMOIRE VECTORIELLE
        st.session_state.cache_msg = cache_info
        st.rerun()
    if load_graph_btn:
        st.session_state.graph, st.session_state.pois, msg = load_existing_graph()
        st.session_state.status_msg = msg
        model_choice = st.selectbox("Mod√®le", list(WORKING_MODELS.keys()), key="model_load_graph")
        st.session_state.current_model, st.session_state.agent, cache_info = update_agent(model_choice, st.session_state.vectordb, st.session_state.graph, st.session_state.pois, st.session_state.chat_vectordb) # AJOUT M√âMOIRE VECTORIELLE
        st.session_state.cache_msg = cache_info
        st.rerun()
    if load_vectordb_btn:
        st.session_state.vectordb, msg = load_vectordb()
        st.session_state.status_msg = msg
        model_choice = st.selectbox("Mod√®le", list(WORKING_MODELS.keys()), key="model_load_db")
        st.session_state.chat_vectordb, _ = load_chat_vectordb() # AJOUT M√âMOIRE VECTORIELLE: Charger chat db
        st.session_state.current_model, st.session_state.agent, cache_info = update_agent(model_choice, st.session_state.vectordb, st.session_state.graph, st.session_state.pois, st.session_state.chat_vectordb)
        st.session_state.cache_msg = cache_info
        st.rerun()
    if clear_cache_btn:
        msg = handle_clear_cache()
        st.session_state.status_msg = msg
        st.session_state.cache_msg = get_cache_stats()
        st.rerun()
  
    # NOUVEAU: Gestion des boutons auto-apprentissage et am√©lioration
    if train_submodel_btn:
        st.session_state.chat_vectordb, _ = load_chat_vectordb()
        submodel_path, msg = create_submodel_from_chat_history(st.session_state.chat_vectordb)
        st.session_state.status_msg = msg
        if submodel_path:
            st.write(f"Utiliser: use_submodel_for_automation('query', '{submodel_path}')")
        st.rerun()
  
    if improve_db_btn:
        topics_input = st.text_input("Sujets (s√©par√©s par ,)", value="p√©trole,topographie,sciences physiques,sous-sol", key="topics_input")
        topics = [t.strip() for t in topics_input.split(",")]
        st.session_state.vectordb, msg = improve_database_with_web_search(topics)
        st.session_state.status_msg = msg
        st.rerun()
    
    # üÜï NOUVEAU: Gestion extraction PDF ERT
    if extract_ert_btn and ert_pdf_upload:
        # Sauvegarder temporairement le PDF
        temp_pdf_path = f"/tmp/ert_report_{int(time.time())}.pdf"
        with open(temp_pdf_path, "wb") as f:
            f.write(ert_pdf_upload.getvalue())
        
        # Extraire donn√©es
        extraction_results = extract_ert_report_from_pdf(temp_pdf_path)
        
        # Afficher r√©sultats
        st.success(f"‚úÖ Extraction termin√©e!")
        st.write(f"üìä **Images extraites**: {len(extraction_results['images'])}")
        st.write(f"üìù **L√©gendes**: {len(extraction_results['captions'])}")
        st.write(f"üî¢ **Valeurs r√©sistivit√©**: {len(extraction_results['resistivity_values'])}")
        
        if extraction_results['resistivity_values']:
            st.write(f"üìà **Plage r√©sistivit√©**: {min(extraction_results['resistivity_values']):.4f} - {max(extraction_results['resistivity_values']):.2f} Œ©¬∑m")
            
            # Analyser min√©raux automatiquement
            mineral_report = analyze_minerals_from_resistivity(
                extraction_results['resistivity_values'], 
                ert_pdf_upload.name
            )
            st.text_area("üî¨ Rapport Min√©ralogique", mineral_report, height=400)
            
            # üÜï CR√âER TABLEAU DE CORRESPONDANCES
            st.markdown("### üìä Tableau de Correspondances R√©elles")
            fig_corr, df_corr, rapport_corr = create_real_mineral_correspondence_table(
                extraction_results['resistivity_values'],
                ert_pdf_upload.name
            )
            
            if fig_corr and df_corr is not None:
                st.pyplot(fig_corr)
                plt.close(fig_corr)
                
                # Corriger les pourcentages de confiance
                df_corr_display = df_corr.copy()
                if 'Confiance' in df_corr_display.columns:
                    if df_corr_display['Confiance'].max() <= 1:
                        df_corr_display['Confiance (%)'] = (df_corr_display['Confiance'] * 100).round(1)
                    else:
                        df_corr_display['Confiance (%)'] = df_corr_display['Confiance'].round(1)
                    df_corr_display = df_corr_display.drop('Confiance', axis=1)
                
                # Organiser en plusieurs tableaux si n√©cessaire
                total_rows = len(df_corr_display)
                if total_rows > 20:
                    st.markdown("#### üìã Donn√©es Tabulaires - Organis√©es par Profondeur")
                    
                    depth_col = 'Profondeur (m)' if 'Profondeur (m)' in df_corr_display.columns else df_corr_display.columns[0]
                    df_sorted = df_corr_display.sort_values(depth_col)
                    
                    quantiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
                    depth_ranges = df_sorted[depth_col].quantile(quantiles).values
                    
                    for i in range(5):
                        min_depth = depth_ranges[i]
                        max_depth = depth_ranges[i+1]
                        
                        if i == 4:
                            mask = (df_sorted[depth_col] >= min_depth) & (df_sorted[depth_col] <= max_depth)
                        else:
                            mask = (df_sorted[depth_col] >= min_depth) & (df_sorted[depth_col] < max_depth)
                        
                        df_section = df_sorted[mask]
                        
                        if len(df_section) > 0:
                            with st.expander(f"üìä Tableau {i+1}/5 - Profondeur: {min_depth:.1f} √† {max_depth:.1f} m ({len(df_section)} d√©tections)", expanded=(i==0)):
                                st.dataframe(
                                    df_section,
                                    use_container_width=True,
                                    column_config={
                                        "Confiance (%)": st.column_config.NumberColumn(
                                            "Confiance (%)",
                                            format="%.1f%%"
                                        ),
                                        "R√©sistivit√© mesur√©e (Œ©¬∑m)": st.column_config.NumberColumn(
                                            "R√©sistivit√© mesur√©e (Œ©¬∑m)",
                                            format="%.6f"
                                        ),
                                        "Profondeur (m)": st.column_config.NumberColumn(
                                            "Profondeur (m)",
                                            format="%.1f"
                                        )
                                    },
                                    height=min(400, len(df_section) * 35 + 38)
                                )
                else:
                    st.dataframe(
                        df_corr_display,
                        use_container_width=True,
                        column_config={
                            "Confiance (%)": st.column_config.NumberColumn(
                                "Confiance (%)",
                                format="%.1f%%"
                            ),
                            "R√©sistivit√© mesur√©e (Œ©¬∑m)": st.column_config.NumberColumn(
                                "R√©sistivit√© mesur√©e (Œ©¬∑m)",
                                format="%.6f"
                            ),
                            "Profondeur (m)": st.column_config.NumberColumn(
                                "Profondeur (m)",
                                format="%.1f"
                            )
                        }
                    )
                
                st.text_area("üìù Rapport D√©taill√©", rapport_corr, height=400)
                
                # T√©l√©chargement CSV
                csv_data = df_corr_display.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• T√©l√©charger Correspondances (CSV)",
                    data=csv_data,
                    file_name=f"{ert_pdf_upload.name}_correspondances.csv",
                    mime="text/csv"
                )
            
            # üÜï G√âN√âRER COUPES ERT PROFESSIONNELLES
            st.markdown("### üé® Coupes ERT Professionnelles (5 Graphiques)")
            
            # Option mode grand format
            col_btn1, col_btn2 = st.columns([1, 1])
            with col_btn1:
                use_fullsize_pdf = st.checkbox("üñºÔ∏è Mode GRAND FORMAT PDF (30√ó36 pouces)", value=False, 
                                              help="Graphiques haute r√©solution pour impression A0/A1", key="fullsize_pdf")
            
            fig_ert, grid_data, rapport_ert = create_ert_professional_sections(
                extraction_results['resistivity_values'],
                ert_pdf_upload.name,
                full_size=use_fullsize_pdf
            )
            
            if fig_ert is not None:
                # Affichage responsive
                st.pyplot(fig_ert, use_container_width=True)
                
                # Rapport
                st.text_area("üìä Rapport ERT", rapport_ert, height=300)
                
                # Boutons de t√©l√©chargement en colonnes
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    # PNG haute r√©solution
                    import io
                    buf_png = io.BytesIO()
                    fig_ert.savefig(buf_png, format='png', dpi=300, bbox_inches='tight')
                    buf_png.seek(0)
                    st.download_button(
                        label="üì• PNG 300 DPI",
                        data=buf_png,
                        file_name=f"{ert_pdf_upload.name}_ert_300dpi.png",
                        mime="image/png",
                        key="dl_png_pdf"
                    )
                
                with col2:
                    # PDF vectoriel
                    buf_pdf = io.BytesIO()
                    fig_ert.savefig(buf_pdf, format='pdf', bbox_inches='tight')
                    buf_pdf.seek(0)
                    st.download_button(
                        label="üìÑ PDF Vectoriel",
                        data=buf_pdf,
                        file_name=f"{ert_pdf_upload.name}_ert.pdf",
                        mime="application/pdf",
                        key="dl_pdf_pdf"
                    )
                
                with col3:
                    # Grille de donn√©es
                    if grid_data:
                        import pickle
                        grid_pickle = pickle.dumps(grid_data)
                        st.download_button(
                            label="ÔøΩ Grille PKL",
                            data=grid_pickle,
                            file_name=f"{ert_pdf_upload.name}_grid.pkl",
                            mime="application/octet-stream",
                            key="dl_grid_pdf"
                        )
                
                plt.close(fig_ert)
        
        st.session_state.status_msg = f"‚úÖ PDF ERT extrait: {len(extraction_results['images'])} images"
        
    # üÜï NOUVEAU: Gestion transcription audio
    if transcribe_audio_btn and audio_upload:
        temp_audio_path = f"/tmp/audio_{int(time.time())}.{audio_upload.name.split('.')[-1]}"
        with open(temp_audio_path, "wb") as f:
            f.write(audio_upload.getvalue())
        
        transcription = process_audio_transcription(temp_audio_path)
        
        if transcription:
            st.text_area("üìù Transcription", transcription, height=200)
            st.session_state.status_msg = f"‚úÖ Audio transcrit: {len(transcription)} caract√®res"

# Main area - Chat principal
st.title("üó∫Ô∏è Kibali Analyst üåü - Assistant IA Avanc√©")
main_container = st.container()
with main_container:
    # Onglets pour autres fonctionnalit√©s - AJOUT TAB ERTEST
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "üó∫Ô∏è Trajets", 
        "üì∏ Analyse Image", 
        "üåê Recherche Web", 
        "üí¨ Chat", 
        "üìä Status",
        "üåä ERTest - Analyse R√©sistivit√© Avanc√©e"  # NOUVEAU TAB
    ])
    with tab1:
        st.markdown("""
        ### Calcul de trajets
        **Exemples:** "Comment aller de l'√©cole √† l'h√¥pital ?"
        """)
        trajectory_input = st.text_area("üó∫Ô∏è Question de trajet", key="traj_input")
        if st.button("üöÄ Calculer trajet", key="calc_traj"):
            carte_buf, reponse, traj_info = calculer_trajet(trajectory_input, st.session_state.graph, st.session_state.pois)
            st.text_area("üìã D√©tails", reponse, key="traj_details")
            if carte_buf:
                carte_buf.seek(0)
                st.image(Image.open(carte_buf), key="traj_map")
            if traj_info:
                if st.button("üíæ Sauvegarder trajet", key="save_traj"):
                    save_trajectory(trajectory_input, reponse, traj_info)
                    st.write("‚úÖ Trajet sauvegard√©")
    with tab2:
        st.markdown("""
        ### Analyse d'images
        Upload une image pour analyse d√©taill√©e, annotations, graphiques et am√©lioration IA.
        """)
        image_upload = st.file_uploader("üì§ Upload Image", type=["jpg", "png"], key="img_upload")
        if image_upload and st.button("üîç Analyser", key="analyze_img"):
            analysis_data, proc_images, tables_str = process_image(image_upload.getvalue())
            improved_analysis = improve_analysis_with_llm(analysis_data, st.session_state.current_model)
            st.image(proc_images, caption=proc_images, width=400) # Responsive width
            st.markdown(tables_str, unsafe_allow_html=True)
            st.text_area("Analyse Am√©lior√©e (IA)", improved_analysis, key="img_analysis")
    with tab3:
        st.markdown("""
        ### Recherche web avanc√©e avec extraction de contenu
        """)
        web_query = st.text_area("üîç Requ√™te de recherche", key="web_query")
        search_type = st.selectbox("Type de recherche", ["text", "news", "both"], key="search_type")
        if st.button("üîç Rechercher", key="search_btn"):
            results = handle_web_search(web_query, search_type)
            st.markdown(results, unsafe_allow_html=True)
        url_extract = st.text_input("üåê URL √† extraire", key="url_extract")
        if st.button("üìÑ Extraire contenu", key="extract_btn"):
            content = handle_content_extraction(url_extract)
            st.text_area("Contenu extrait", content, key="extracted_content")
    with tab4:
        st.markdown("### ü§ñ Assistant IA Kibali - Modes Sp√©cialis√©s")
        
        # S√©lection du mode de fonctionnement
        st.markdown("#### üéõÔ∏è S√©lection du Mode")
        mode_col1, mode_col2 = st.columns([2, 1])
        
        with mode_col1:
            kibali_mode = st.selectbox(
                "Mode de Kibali",
                ["humain", "scientifique", "code_expert", "doc", "rapide"],
                format_func=lambda x: {
                    "humain": "üßë Mode Humain - Conversationnel et naturel",
                    "scientifique": "üî¨ Mode Scientifique - Pr√©cis, rigoureux, calculs d√©taill√©s",
                    "code_expert": "üíª Mode Code Expert - Programmation avanc√©e (niveau Claude)",
                    "doc": "üìñ Mode Documentation - Dissertations, livres, analyses approfondies",
                    "rapide": "‚ö° Mode Rapide - R√©ponses concises et directes"
                }[x],
                key="kibali_mode_select",
                help="Chaque mode change compl√®tement le comportement de Kibali"
            )
        
        with mode_col2:
            if kibali_mode == "code_expert":
                # V√©rifier si le mod√®le est install√©
                code_model_path = os.path.expanduser("~/.cache/huggingface/code_models")
                model_installed = os.path.exists(code_model_path) and os.listdir(code_model_path)
                
                if model_installed:
                    st.success("‚úÖ DeepSeek-Coder install√©")
                    if st.button("ÔøΩ R√©installer", key="reinstall_codellama"):
                        with st.spinner("T√©l√©chargement..."):
                            install_code_model()
                else:
                    if st.button("üì¶ Installer DeepSeek-Coder", key="install_codellama", help="T√©l√©charge ~1.3GB. Prend 5-10 min"):
                        with st.spinner("T√©l√©chargement du mod√®le de code..."):
                            st.info("üí° Alternative plus rapide: `python download_code_model.py` dans le terminal")
                            install_code_model()
        
        # Description du mode s√©lectionn√©
        mode_descriptions = {
            "humain": """
            üí¨ **Comportement**: Kibali r√©agit comme un humain v√©ritable
            - Pose des questions de clarification si besoin
            - Admet ses doutes et incertitudes
            - Dialogue naturel avec √©motions et r√©flexion
            - Peut refuser de r√©pondre si question trop vague
            """,
            "scientifique": """
            üî¨ **Comportement**: Rigueur scientifique absolue
            - M√©thodologie stricte et v√©rifiable
            - Calculs d√©taill√©s avec toutes les √©tapes
            - Citations de sources pr√©cises
            - Analyse des incertitudes et limitations
            - Validation par calculs crois√©s
            """,
            "code_expert": """
            ÔøΩ **Comportement**: Expert en programmation niveau Claude/GPT-4
            - Utilise AI_Code_Generator (DeepSeek-Coder)
            - Code production-ready test√© et optimis√©
            - Architecture propre (SOLID, design patterns)
            - Tests unitaires automatiques inclus
            - Documentation compl√®te
            - Suggestions d'optimisation
            """,
            "rapide": """
            ‚ö° **Comportement**: Efficacit√© maximale
            - R√©ponses ultra-concises (2-3 phrases)
            - Bullet points pour clart√©
            - Pas de contexte inutile
            - Code: snippets minimaux fonctionnels
            """,
            "doc": """
            üìñ **Comportement**: Expert en r√©daction approfondie
            - Dissertations acad√©miques compl√®tes
            - Livres et documents longs (10-30+ pages)
            - Structure m√©thodique (intro, d√©veloppement, conclusion)
            - Style professionnel et accessible
            - R√©f√©rences bibliographiques
            - Analyses critiques multidimensionnelles
            - Exemples concrets et √©tudes de cas
            - Peut produire 2000+ mots par r√©ponse
            
            **üìö Id√©al pour**:
            ‚Ä¢ Dissertations universitaires
            ‚Ä¢ Rapports professionnels complets
            ‚Ä¢ Manuels et guides d√©taill√©s
            ‚Ä¢ Analyses approfondies multi-facettes
            ‚Ä¢ Livres blancs (white papers)
            ‚Ä¢ Th√®ses et m√©moires (sections)
            """
        }
        
        with st.expander(f"‚ÑπÔ∏è Description du mode: {kibali_mode}", expanded=False):
            st.markdown(mode_descriptions[kibali_mode])
        
        # Options avanc√©es
        st.markdown("#### ‚öôÔ∏è Options")
        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            web_search_toggle = st.checkbox("üåê Recherche web", value=True, key="web_toggle")
        with col_opt2:
            use_submodel = st.checkbox("üß† Sous-mod√®le rapide", value=False, key="use_submodel",
                                      help="R√©ponses instantan√©es via mod√®le auto-appris")
        
        if use_submodel:
            submodel_path_input = st.text_input("Chemin sous-mod√®le (optionnel)", key="submodel_path")
        else:
            submodel_path_input = None
      
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []
        for msg in st.session_state.chat_history:
            with st.chat_message(msg["role"], avatar="‚òÅÔ∏è" if msg["role"] == "user" else "‚≠ê"):
                # Correction pour lisibilit√© : utiliser markdown pour HTML
                if msg["role"] == "user":
                    st.markdown(f"**Question:** {highlight_important_words(msg['content'])}", unsafe_allow_html=True)
                else:
                    st.markdown(highlight_important_words(msg['content']), unsafe_allow_html=True)
        
        # üé§ INTERFACE VOCALE - Boutons d'enregistrement
        if VOICE_AVAILABLE and st.session_state.get("voice_enabled", False) and st.session_state.get("voice_models_loaded", False):
            st.markdown("---")
            
            # D√©sactiver l'enregistrement si pas de mat√©riel audio
            recording_disabled = st.session_state.get("audio_recording_disabled", False)
            
            if recording_disabled:
                st.info("üìÅ Mode transcription uniquement - Uploadez des fichiers audio pour les transcrire")
                st.caption("üö´ Enregistrement micro non disponible (WSL/Docker)")
            else:
                col_voice1, col_voice2, col_voice3 = st.columns([1, 1, 1])
                
                with col_voice1:
                    if st.button("üé§ Enregistrer Question", key="voice_record_btn", use_container_width=True):
                        st.session_state.voice_recording = True
                
                with col_voice2:
                    if st.button("üîä R√©p√©ter Derni√®re R√©ponse", key="voice_repeat_btn", use_container_width=True):
                        if st.session_state.chat_history:
                            last_response = st.session_state.chat_history[-1]
                            if last_response["role"] == "assistant":
                                with st.spinner("üîä Synth√®se vocale..."):
                                    voice_agent = st.session_state.voice_agent
                                    audio_path = voice_agent.synthesize_speech(
                                        last_response["content"],
                                        play=st.session_state.get("voice_autoplay", True)
                                    )
                                    if audio_path:
                                        st.success("‚úÖ Audio g√©n√©r√©!")
                                        # T√©l√©chargement optionnel
                                        with open(audio_path, 'rb') as f:
                                            st.download_button(
                                                "üíæ T√©l√©charger Audio",
                                                f.read(),
                                                file_name="kibali_response.wav",
                                                mime="audio/wav"
                                            )
                
                with col_voice3:
                    voice_status = "üü¢ Actif" if st.session_state.get("voice_models_loaded", False) else "üî¥ Inactif"
                    st.info(f"Vocal: {voice_status}")
            
            # Gestion de l'enregistrement vocal
            if st.session_state.get("voice_recording", False):
                st.info(f"üé§ Parlez maintenant ({st.session_state.get('voice_duration', 5)}s)...")
                
                voice_agent = st.session_state.voice_agent
                duration = st.session_state.get('voice_duration', 5)
                language = st.session_state.get('voice_lang', 'fr')
                
                # Enregistrer audio (avec gestion des erreurs)
                try:
                    audio = voice_agent.record_audio(duration=duration)
                    
                    if len(audio) > 0:
                        with st.spinner("üìù Transcription en cours..."):
                            # Transcrire
                            transcription = voice_agent.transcribe_audio(
                                audio_array=audio,
                                language=language
                            )
                            
                            if transcription:
                                st.success(f"‚úÖ Transcription: {transcription}")
                                
                                # Utiliser la transcription comme prompt
                                prompt = transcription
                                st.session_state.voice_recording = False
                                
                                # Traiter la question vocale
                                with st.chat_message("user", avatar="‚òÅÔ∏è"):
                                    st.markdown(f"**Question (vocale):** {highlight_important_words(prompt)}", unsafe_allow_html=True)
                                
                                with st.chat_message("assistant", avatar="‚≠ê"):
                                    with st.spinner("ü§ñ Kibali r√©fl√©chit..."):
                                        # G√©n√©rer r√©ponse
                                        mode_prompt = get_mode_specific_prompt(kibali_mode)
                                        response = handle_chat_enhanced(
                                            prompt, 
                                            st.session_state.chat_history, 
                                            st.session_state.agent,
                                            list(WORKING_MODELS.keys())[0],
                                            st.session_state.vectordb,
                                            st.session_state.graph,
                                            st.session_state.pois,
                                            web_search_toggle,
                                            mode=kibali_mode,
                                            mode_prompt=mode_prompt
                                        )
                                        
                                        response = apply_mode_behavior(response, prompt, kibali_mode)
                                        st.markdown(highlight_important_words(response), unsafe_allow_html=True)
                                        
                                        # Synth√®se vocale de la r√©ponse (si disponible)
                                        if st.session_state.get("voice_autoplay", True):
                                            try:
                                                with st.spinner("üîä Synth√®se vocale..."):
                                                    audio_path = voice_agent.synthesize_speech(
                                                        response,
                                                        play=False  # Ne pas jouer automatiquement
                                                    )
                                                    if audio_path:
                                                        st.audio(audio_path)
                                            except Exception as e:
                                                st.warning(f"‚ö†Ô∏è Synth√®se vocale non disponible: {str(e)}")
                            else:
                                st.error("‚ùå √âchec de la transcription")
                                st.session_state.voice_recording = False
                    else:
                        st.error("‚ùå Erreur d'enregistrement audio")
                        st.info("üí° V√©rifiez qu'un microphone est connect√© et configur√©")
                        st.session_state.voice_recording = False
                        
                except Exception as e:
                    st.error(f"‚ùå Erreur enregistrement: {str(e)}")
                    st.info("üí° Le mode vocal n√©cessite un microphone fonctionnel et PortAudio")
                    st.session_state.voice_recording = False
            
            st.markdown("---")
        
        # ========================================
        # ZONE D'UPLOAD DE FICHIERS (Style ChatGPT)
        # ========================================
        st.markdown("### üìé T√©l√©verser un fichier pour analyse")
        
        col_upload, col_info = st.columns([3, 1])
        
        with col_upload:
            uploaded_files = st.file_uploader(
                "üìÇ Glissez un ou plusieurs fichiers ici (multi-s√©lection pour .dat compl√©mentaires)",
                type=None,  # Accepte tous les types
                help="Images, PDF, CSV, JSON, fichiers ERT multi-fr√©quences (.dat), etc. Pour les fichiers .dat: uploadez tous les fichiers compl√©mentaires en une fois pour fusion automatique.",
                key="chat_file_upload",
                accept_multiple_files=True  # üÜï MULTI-FICHIERS ACTIV√â
            )
        
        with col_info:
            if uploaded_files:
                if len(uploaded_files) == 1:
                    st.success(f"‚úÖ {uploaded_files[0].name}")
                    file_size_mb = uploaded_files[0].size / (1024 * 1024)
                    st.caption(f"Taille: {file_size_mb:.2f} MB")
                else:
                    st.success(f"‚úÖ {len(uploaded_files)} fichiers")
                    total_size = sum(f.size for f in uploaded_files) / (1024 * 1024)
                    st.caption(f"Total: {total_size:.2f} MB")
                    
                    # Afficher liste des fichiers
                    with st.expander("üìã Fichiers upload√©s"):
                        for uf in uploaded_files:
                            st.write(f"‚Ä¢ {uf.name} ({uf.size/(1024*1024):.2f} MB)")
        
        # Traiter les fichiers upload√©s
        if uploaded_files is not None and len(uploaded_files) > 0:
            # Stocker les fichiers dans session_state pour usage ult√©rieur
            if 'uploaded_file_data' not in st.session_state:
                st.session_state.uploaded_file_data = None
            
            if st.button("üîç Analyser ces fichiers", type="primary", use_container_width=True):
                with st.spinner("ü§ñ Kibali analyse vos fichiers en profondeur..."):
                    import tempfile
                    temp_dir = tempfile.gettempdir()
                    
                    # üÜï D√âTECTION MULTI-FICHIERS .dat POUR FUSION
                    dat_files = [f for f in uploaded_files if f.name.lower().endswith('.dat')]
                    other_files = [f for f in uploaded_files if not f.name.lower().endswith('.dat')]
                    
                    # Si plusieurs fichiers .dat ‚Üí Fusion multi-fr√©quences automatique
                    if len(dat_files) > 1:
                        st.info(f"üî¨ **{len(dat_files)} fichiers .dat d√©tect√©s ‚Üí Fusion multi-fr√©quences automatique**")
                        
                        # Sauvegarder tous les fichiers .dat
                        dat_paths = []
                        for dat_file in dat_files:
                            temp_path = os.path.join(temp_dir, dat_file.name)
                            with open(temp_path, 'wb') as f:
                                f.write(dat_file.getvalue())
                            dat_paths.append(temp_path)
                            st.write(f"   ‚Ä¢ {dat_file.name}")
                        
                        # Parser avec fusion
                        try:
                            from multi_freq_ert_parser import MultiFreqERTParser
                            parser = MultiFreqERTParser()
                            df = parser.parse_multiple_files(dat_paths)
                            
                            if not df.empty and len(df) > 0:
                                st.success(f"‚úÖ **FUSION R√âUSSIE !**")
                                st.write(f"üìä {len(df)} mesures ERT fusionn√©es")
                                st.write(f"üì° {len(parser.frequencies)} fr√©quences d√©tect√©es")
                                st.write(f"üìç {len(parser.survey_points)} survey points")
                                
                                # Afficher structure
                                with st.expander("üìã Structure fusionn√©e (10 premi√®res lignes)"):
                                    st.dataframe(df[['project', 'survey_point', 'depth', 'frequency_MHz', 'resistivity']].head(10))
                                
                                # Coordonn√©es
                                coords_df = parser.get_coordinates_corrected()
                                with st.expander("üìê Coordonn√©es spatiales (X, Y, Z)"):
                                    st.dataframe(coords_df[['x', 'y', 'z', 'resistivity', 'frequency_MHz']].head(10))
                                
                                # Sauvegarder en session
                                st.session_state['multi_freq_data'] = df
                                st.session_state['multi_freq_coords'] = coords_df
                                st.session_state['multi_freq_parser'] = parser
                                
                                # Cr√©er donn√©es pour le chat
                                file_data = {
                                    'filename': f"fusion_{len(dat_files)}_fichiers.dat",
                                    'physical_path': dat_paths[0],  # Premier fichier comme r√©f√©rence
                                    'all_paths': dat_paths,
                                    'size': sum(f.size for f in dat_files),
                                    'extension': 'dat',
                                    'metadata': {
                                        "type": "ERT_MULTI_FREQ_FUSION",
                                        "num_files": len(dat_files),
                                        "num_measurements": len(df),
                                        "num_frequencies": len(parser.frequencies),
                                        "num_survey_points": len(parser.survey_points),
                                        "frequencies_MHz": [float(f) for f in sorted(parser.frequencies)[:20]],
                                        "depth_range": (float(parser.metadata['depth_range'][0]), float(parser.metadata['depth_range'][1])),
                                        "resistivity_range": (float(parser.metadata['resistivity_range'][0]), float(parser.metadata['resistivity_range'][1]))
                                    },
                                    'content': f"""Donn√©es ERT Multi-Fr√©quences (FUSION de {len(dat_files)} fichiers):
‚Ä¢ {len(df)} mesures totales
‚Ä¢ {len(parser.frequencies)} fr√©quences (de {min(parser.frequencies):.2f} √† {max(parser.frequencies):.2f} MHz)
‚Ä¢ {len(parser.survey_points)} survey points
‚Ä¢ Profondeurs: {parser.metadata['depth_range'][0]:.1f} √† {parser.metadata['depth_range'][1]:.1f} m
‚Ä¢ R√©sistivit√©s: {parser.metadata['resistivity_range'][0]:.2f} √† {parser.metadata['resistivity_range'][1]:.2f} Œ©¬∑m

Structure valid√©e:
project | survey_point | depth | frequency_MHz | resistivity

Coordonn√©es spatiales (X,Y,Z) disponibles pour visualisation."""
                                }
                                
                                st.session_state.uploaded_file_data = file_data
                                
                                # Message de Kibali
                                kibali_analysis = f"""üéØ **Analyse de fusion multi-fr√©quences ERT**

J'ai fusionn√© **{len(dat_files)} fichiers .dat compl√©mentaires** avec succ√®s !

**üìä R√©sum√©:**
‚Ä¢ **{len(df):,}** mesures ERT fusionn√©es
‚Ä¢ **{len(parser.frequencies)}** fr√©quences d√©tect√©es ({min(parser.frequencies):.0f} - {max(parser.frequencies):.0f} MHz)
‚Ä¢ **{len(parser.survey_points)}** survey points
‚Ä¢ Profondeurs: **{parser.metadata['depth_range'][0]:.1f}** √† **{parser.metadata['depth_range'][1]:.1f}** m

**‚úÖ Structure correcte valid√©e:**
```
project | survey_point | depth | frequency_MHz | resistivity
```

**üìê Syst√®me de coordonn√©es:**
‚Ä¢ X = survey_point √ó 10m (espacement horizontal)
‚Ä¢ Y = 0m (ligne unique)
‚Ä¢ Z = profondeur absolue (valeur positive)

**üí° Tu peux maintenant me demander:**
‚Ä¢ "Montre-moi les coordonn√©es spatiales"
‚Ä¢ "G√©n√®re une visualisation 2D par fr√©quence"
‚Ä¢ "Analyse les variations de r√©sistivit√©"
‚Ä¢ "Export en Excel"
"""
                                
                                with st.chat_message("assistant", avatar="‚≠ê"):
                                    st.markdown(kibali_analysis, unsafe_allow_html=True)
                                
                                # Sauvegarder dans historique
                                st.session_state.chat_history.append({
                                    "role": "user",
                                    "content": f"üìé {len(dat_files)} fichiers .dat upload√©s pour fusion"
                                })
                                st.session_state.chat_history.append({
                                    "role": "assistant",
                                    "content": kibali_analysis
                                })
                                
                                st.success("‚úÖ Fusion termin√©e ! Pose-moi toutes tes questions sur ces donn√©es ERT.")
                            
                            else:
                                st.error("‚ùå Aucune donn√©e ERT d√©tect√©e dans ces fichiers")
                        
                        except Exception as e:
                            st.error(f"‚ùå Erreur lors de la fusion: {e}")
                            st.info("üí° Les fichiers seront analys√©s individuellement...")
                    
                    # Si 1 seul fichier .dat ou autres fichiers
                    if len(dat_files) <= 1 or other_files:
                        # Utiliser le premier fichier (logique originale)
                        uploaded_file = uploaded_files[0] if len(uploaded_files) == 1 else (dat_files[0] if dat_files else other_files[0])
                        
                        # Sauvegarder le fichier physiquement
                        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                        
                        # √âcrire le fichier
                        with open(temp_file_path, 'wb') as f:
                            f.write(uploaded_file.getvalue())
                        # √âcrire le fichier
                        with open(temp_file_path, 'wb') as f:
                            f.write(uploaded_file.getvalue())
                        
                        # Stocker imm√©diatement le chemin physique pour le AI Code Agent
                        # AVANT l'analyse, pour que le path soit disponible d√®s le premier message
                        st.session_state.uploaded_file_data = {
                            'filename': uploaded_file.name,
                            'physical_path': temp_file_path,
                            'size': uploaded_file.size
                        }
                        
                        # Analyse compl√®te et intelligente par Kibali
                        kibali_analysis, file_data = analyze_uploaded_file_realtime(
                            uploaded_file,
                            st.session_state.chat_history,
                            st.session_state.agent,
                            st.session_state.vectordb,
                            st.session_state.graph,
                            st.session_state.pois,
                            web_search_toggle
                        )
                        
                        # Ajouter le chemin physique aux donn√©es d'analyse
                        file_data['physical_path'] = temp_file_path
                        
                        # Mettre √† jour avec toutes les donn√©es d'analyse
                        st.session_state.uploaded_file_data = file_data
                        
                        # Afficher l'analyse de Kibali dans le chat
                        with st.chat_message("assistant", avatar="‚≠ê"):
                            st.markdown(kibali_analysis, unsafe_allow_html=True)
                        
                        # Sauvegarder dans l'historique
                        st.session_state.chat_history.append({
                            "role": "user",
                            "content": f"üìé Fichier upload√©: {uploaded_file.name}"
                        })
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "content": kibali_analysis
                        })
                        
                        st.success("‚úÖ Analyse termin√©e ! Pose-moi toutes tes questions sur ce fichier.")
                        st.info("üí° **Kibali est maintenant expert de ce fichier.** Je peux faire des recherches web suppl√©mentaires si besoin !")
        
        st.markdown("---")
        
        if prompt := st.chat_input("Pose une question...", key="chat_input"):
            with st.chat_message("user", avatar="‚òÅÔ∏è"):
                highlighted_prompt = highlight_important_words(prompt)
                st.markdown(f"**Question:** {highlighted_prompt}", unsafe_allow_html=True)
            
            with st.chat_message("assistant", avatar="‚≠ê"):
                spinner_messages = {
                    "humain": "ü§î R√©flexion en cours...",
                    "scientifique": "üî¨ Analyse rigoureuse...",
                    "code_expert": "üíª G√©n√©ration de code optimis√©...",
                    "rapide": "‚ö° Traitement rapide..."
                }
                with st.spinner(spinner_messages.get(kibali_mode, "R√©ponse en cours...")):
                    content_to_save = None
                    
                    # === D√âTECTION ET UTILISATION AUTONOME DES OUTILS ===
                    tools_results = None
                    if 'uploaded_file_data' in st.session_state and st.session_state.uploaded_file_data:
                        # D√©tection automatique des outils n√©cessaires
                        needed_tools = kibali_tools.detect_needed_tools(
                            prompt, 
                            st.session_state.uploaded_file_data
                        )
                        
                        if needed_tools:
                            with st.status("üîß Kibali pr√©pare les outils n√©cessaires...", expanded=True) as status:
                                st.write(f"Outils d√©tect√©s: {', '.join(needed_tools)}")
                                
                                # Ex√©cution des outils
                                tools_results = kibali_tools.execute_tools(
                                    needed_tools,
                                    st.session_state.uploaded_file_data,
                                    prompt
                                )
                                
                                status.update(label="‚úÖ Outils ex√©cut√©s", state="complete")
                    
                    # G√©n√©rer la r√©ponse selon le mode
                    if use_submodel and submodel_path_input:
                        automated = use_submodel_for_automation(prompt, submodel_path_input)
                        # Appliquer le comportement du mode
                        automated = apply_mode_behavior(automated, prompt, kibali_mode)
                        st.markdown(highlight_important_words(automated), unsafe_allow_html=True)
                        content_to_save = automated
                    else:
                        # Enrichir le prompt avec le contexte du fichier upload√© ET r√©sultats des outils
                        enriched_prompt = prompt
                        if 'uploaded_file_data' in st.session_state and st.session_state.uploaded_file_data:
                            # Ajouter r√©sultats des outils au contexte
                            tools_context = ""
                            if tools_results and tools_results.get("success"):
                                tools_context = "\n[R√âSULTATS DES OUTILS UTILIS√âS]\n"
                                for tool_id, output in tools_results["outputs"].items():
                                    tools_context += f"\n**Outil: {tool_id}**\n"
                                    tools_context += f"Message: {output.get('message', '')}\n"
                                    
                                    # Ajouter donn√©es sp√©cifiques
                                    if 'statistics' in output:
                                        tools_context += f"Statistiques: {output['statistics']}\n"
                                    if 'resistivity_values' in output:
                                        tools_context += f"Valeurs extraites: {len(output['resistivity_values'])} points\n"
                                    if 'analysis' in output:
                                        tools_context += f"Analyse: {output['analysis'][:500]}...\n"
                            
                            file_context = f"""
[CONTEXTE FICHIER UPLOAD√â]
Fichier: {st.session_state.uploaded_file_data['filename']}
Chemin physique: {st.session_state.uploaded_file_data.get('physical_path', st.session_state.uploaded_file_data['filename'])}
Type: .{st.session_state.uploaded_file_data['extension']}
Taille: {st.session_state.uploaded_file_data['size']} octets
M√©tadonn√©es: {st.session_state.uploaded_file_data['metadata']}
Contenu extrait: {st.session_state.uploaded_file_data['content'][:1000]}

{tools_context}

[QUESTION DE L'UTILISATEUR]
{prompt}

[INSTRUCTION]
Tu es un assistant IA conversationnel et naturel. Utilise les r√©sultats des outils ci-dessus pour donner une r√©ponse fluide, conversationnelle et compl√®te.

R√àGLES IMPORTANTES:
1. R√©ponds de mani√®re naturelle et conversationnelle, comme un expert humain
2. Explique les r√©sultats de fa√ßon p√©dagogique et accessible
3. Utilise ta connaissance scientifique pour ajouter du contexte et des interpr√©tations
4. Si pertinent, fais des recherches web pour compl√©ter l'analyse avec des donn√©es actuelles
5. Structure ta r√©ponse de fa√ßon logique mais pas robotique
6. Pose des questions de clarification si quelque chose n'est pas clair
7. Adapte ton langage au contexte scientifique tout en restant accessible

Si tu dois g√©n√©rer du code Python pour analyser le fichier, utilise le chemin physique: {st.session_state.uploaded_file_data.get('physical_path', st.session_state.uploaded_file_data['filename'])}
"""
                            enriched_prompt = file_context
                        
                        # Modifier le prompt syst√®me selon le mode avant d'appeler l'agent
                        mode_prompt = get_mode_specific_prompt(kibali_mode)
                        
                        # G√©n√©rer r√©ponse avec agent (avec contexte fichier + outils)
                        response = handle_chat_enhanced(
                            enriched_prompt, 
                            st.session_state.chat_history, 
                            st.session_state.agent, 
                            list(WORKING_MODELS.keys())[0], 
                            st.session_state.vectordb, 
                            st.session_state.graph, 
                            st.session_state.pois, 
                            web_search_toggle, 
                            mode=kibali_mode, 
                            mode_prompt=mode_prompt
                        )
                        
                        # Afficher les r√©sultats visuels des outils (graphiques, tableaux)
                        if tools_results and tools_results.get("success"):
                            for tool_id, output in tools_results["outputs"].items():
                                # Afficher graphiques
                                if 'plot' in output and output['plot']:
                                    st.markdown(output['plot'], unsafe_allow_html=True)
                                
                                # Afficher tables
                                if 'color_table' in output and output['color_table']:
                                    st.markdown(output['color_table'], unsafe_allow_html=True)
                                
                                # Afficher analyses textuelles
                                if 'formatted_text' in output and output['formatted_text']:
                                    st.markdown(output['formatted_text'])
                                
                                # Afficher mapping couleurs
                                if 'color_mapping' in output and output['color_mapping']:
                                    with st.expander("üìä L√©gende des couleurs ERT"):
                                        st.markdown(output['color_mapping'], unsafe_allow_html=True)
                        
                        # Appliquer le comportement du mode
                        response = apply_mode_behavior(response, prompt, kibali_mode)
                        st.markdown(highlight_important_words(response), unsafe_allow_html=True)
                        content_to_save = response
                        
                        # üé® G√âN√âRATION AUTOMATIQUE DE VISUALISATIONS
                        # Si des donn√©es sont disponibles, g√©n√©rer automatiquement des graphiques
                        if tools_results and tools_results.get("success"):
                            # Pr√©parer les donn√©es pour l'auto-visualiseur
                            viz_data = {
                                'filename': st.session_state.uploaded_file_data.get('filename', 'Analyse'),
                                'resistivity_values': []
                            }
                            
                            # Extraire les valeurs de r√©sistivit√© des outils
                            for tool_id, output in tools_results["outputs"].items():
                                if 'resistivity_values' in output:
                                    viz_data['resistivity_values'].extend(output['resistivity_values'])
                                if 'statistics' in output:
                                    viz_data['statistics'] = output['statistics']
                            
                            # G√©n√©rer et afficher automatiquement les visualisations
                            if viz_data['resistivity_values']:
                                auto_viz.auto_generate_and_display(prompt, viz_data)
                        
                        # Si mode doc et PDF g√©n√©r√©, afficher bouton de t√©l√©chargement
                        if kibali_mode == "doc":
                            print(f"[DEBUG PDF] Mode doc activ√©")
                            print(f"[DEBUG PDF] generated_pdfs existe: {'generated_pdfs' in st.session_state}")
                            if 'generated_pdfs' in st.session_state:
                                print(f"[DEBUG PDF] Nombre de PDFs: {len(st.session_state.generated_pdfs)}")
                                
                            if 'generated_pdfs' in st.session_state and st.session_state.generated_pdfs:
                                latest_pdf = st.session_state.generated_pdfs[-1]
                                print(f"[DEBUG PDF] Dernier PDF: {latest_pdf['path']}")
                                
                                # V√©rifier que le fichier existe
                                if os.path.exists(latest_pdf['path']):
                                    st.success(f"ÔøΩ PDF g√©n√©r√©: {latest_pdf['filename']}")
                                    
                                    # Lire le fichier PDF
                                    with open(latest_pdf['path'], 'rb') as pdf_file:
                                        pdf_bytes = pdf_file.read()
                                    
                                    print(f"[DEBUG PDF] Fichier lu: {len(pdf_bytes)} bytes")
                                    
                                    # Bouton de t√©l√©chargement
                                    st.download_button(
                                        label="üì• T√©l√©charger le PDF",
                                        data=pdf_bytes,
                                        file_name=latest_pdf['filename'],
                                        mime="application/pdf",
                                        key=f"download_pdf_{latest_pdf['timestamp']}"
                                    )
                                    
                                    st.info(f"ÔøΩ {latest_pdf['word_count']} mots | Format: A4 | Police: Helvetica")
                                else:
                                    st.error(f"‚ùå Erreur: Fichier PDF introuvable: {latest_pdf['path']}")
                            else:
                                print(f"[DEBUG PDF] Aucun PDF disponible dans session_state")
            
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            st.session_state.chat_history.append({"role": "assistant", "content": content_to_save})
    with tab5:
        st.markdown("### Statut syst√®me")
        st.json(get_system_status())
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TAB 6 : ERTEST - ANALYSE R√âSISTIVIT√â AVANC√âE (IMPORT√â)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    with tab6:
        st.header("üåä ERTest - Analyse R√©sistivit√© Avanc√©e Ravensgate")
        st.markdown("""
        ### Module d'analyse ERT complet import√© d'ERTest.py
        
        Cette section int√®gre toutes les fonctionnalit√©s du syst√®me ERTest :
        - üå°Ô∏è Calculateur de r√©glage temp√©rature Ravensgate
        - üìä Analyse de fichiers .dat (Sonic Water Level Meter)
        - üåç Pseudo-sections ERT 2D/3D avec colormap personnalis√©e
        - ü™® Stratigraphie compl√®te (sols + eaux + min√©raux)
        - üî¨ Inversion PyGIMLI avanc√©e
        
        **Colormap Eau Prioritaire** : Rouge (mer) ‚Üí Jaune (sal√©e) ‚Üí Vert (douce) ‚Üí Bleu (pure)
        """)
        
        # Charger ERTest.py comme module
        import sys
        import importlib.util
        
        ertest_path = "/home/belikan/KIbalione8/SETRAF/ERTest.py"
        
        try:
            # Charger le module ERTest
            spec = importlib.util.spec_from_file_location("ertest_module", ertest_path)
            ertest_module = importlib.util.module_from_spec(spec)
            
            # Ex√©cuter le module dans son propre namespace
            with st.spinner("üîÑ Chargement du module ERTest..."):
                spec.loader.exec_module(ertest_module)
                st.success("‚úÖ Module ERTest charg√© avec succ√®s !")
                
        except Exception as e:
            st.error(f"‚ùå Erreur lors du chargement d'ERTest : {e}")
            st.exception(e)

# Fin du tab6 - retour au code principal d'ERT.py
st.markdown("### üìä Informations Syst√®me")
setup_drive()
st.write(f"üöÄ Kibali üåü - Assistant IA Avanc√© avec Recherche Web")
st.write(f"üìÅ Dossier unifi√©: {CHATBOT_DIR}")
st.write(f"üîë Token HF configur√©: {HF_TOKEN[:10]}...")
st.write(f"üåê Recherche web int√©gr√©e")
existing_graphs = [f for f in os.listdir(GRAPHS_PATH) if f.endswith('_graph.graphml')] if os.path.exists(GRAPHS_PATH) else []
existing_pdfs = [f for f in os.listdir(PDFS_PATH) if f.endswith('.pdf')] if os.path.exists(PDFS_PATH) else []
st.write(f"üìä √âtat initial:")
st.write(f" üó∫Ô∏è Graphes OSM: {len(existing_graphs)}")
st.write(f" üìÑ PDFs: {len(existing_pdfs)}")
st.write(f" üíæ Base vectorielle: {'‚úÖ' if os.path.exists(VECTORDB_PATH) else '‚ùå'}")
st.write(f" üß† M√©moire chat: {'‚úÖ' if os.path.exists(CHAT_VECTORDB_PATH) else '‚ùå'}") # AJOUT M√âMOIRE VECTORIELLE
st.write(f" üåê Cache web: {'‚úÖ' if os.path.exists(WEB_CACHE_PATH) else '‚ùå'}")
st.write(f" üìà {get_cache_stats()}")
st.write("\n" + "="*60)
st.write("üéâ KIBALI üåü - SYST√àME CHARG√â AVEC SUCC√àS")
st.write("="*60)
st.write(f"üìÖ Version: 2.0.0 - {time.strftime('%Y-%m-%d %H:%M:%S')}")
st.write(f"üîë Token HF: {'‚úÖ Configur√©' if HF_TOKEN else '‚ùå Manquant'}")
st.write(f"üìÅ Dossier: {CHATBOT_DIR}")
st.write(f"üåê Recherche web: ‚úÖ Activ√©e")
st.write(f"üíæ Cache intelligent: ‚úÖ Activ√©")
st.write(f"üß† M√©moire vectorielle chat: ‚úÖ Activ√©e") # AJOUT M√âMOIRE VECTORIELLE
st.write(f"ü§ñ Auto-apprentissage sklearn: ‚úÖ Activ√© (sous-mod√®les dans {SUBMODELS_PATH})")
st.write(f"üìö Am√©lioration DB via fouille: ‚úÖ Activ√©e (sujets p√©trole, topographie, etc.)")
st.write("\nüìö FONCTIONNALIT√âS PRINCIPALES:")
st.write(" üí¨ Chat RAG avec recherche web intelligent")
st.write(" üß† M√©moire des conversations pour fluidit√©") # AJOUT M√âMOIRE VECTORIELLE
st.write(" üó∫Ô∏è Calcul de trajets OSM")
st.write(" üì∏ Analyse d'images avec IA")
st.write(" üåê Extraction de contenu web")
st.write(" üíæ Gestion unifi√©e des donn√©es")
st.write(" ü§ñ Sous-mod√®les sklearn pour automatismes humains")
st.write(" üìö Fouille auto internet pour enrichir DB (p√©trole, topographie, sciences physiques, sous-sol)")
st.write("\nüöÄ UTILISATION:")
st.write(" Interface: Ex√©cutez les cellules suivantes")
st.write(" API: kibali_api.ask('votre question')")
st.write(" Auto-apprentissage: kibali_api.train_submodel()")
st.write(" Am√©lioration DB: kibali_api.improve_db(['p√©trole'])")
st.write(" Tests: test_all_features()")
st.write("\n‚öôÔ∏è MAINTENANCE:")
st.write(" Status: get_system_status()")
st.write(" Nettoyage: cleanup_old_cache()")
st.write(" Sauvegarde: backup_all_data()")
st.write("="*60)

# ========================================
# INT√âGRATION PYGIMLI POUR INVERSION ERT
# ========================================

def run_pygimli_inversion(dat_data: dict, electrode_spacing: float = 1.0, 
                         max_depth: float = None, n_layers: int = 20) -> dict:
    """
    Ex√©cute une inversion ERT compl√®te avec PyGIMLI
    
    Args:
        dat_data: Dictionnaire avec les donn√©es du fichier .dat
        electrode_spacing: Espacement entre √©lectrodes (m)
        max_depth: Profondeur maximale d'investigation (m)
        n_layers: Nombre de couches pour le mod√®le
    
    Returns:
        dict: R√©sultats de l'inversion avec mod√®les et figures
    """
    if not PYGIMLI_AVAILABLE:
        return {"error": "PyGIMLI non disponible. Installez avec: pip install pygimli"}
    
    try:
        import pygimli as pg
        import numpy as np
        
        # Extraire les donn√©es du fichier .dat
        survey_points = dat_data.get('survey_point', [])
        depths = dat_data.get('depth', [])
        data_values = dat_data.get('data', [])
        
        if not survey_points or not depths or not data_values:
            return {"error": "Donn√©es .dat incompl√®tes"}
        
        # Cr√©er un profil ERT 2D
        # Supposons un profil lin√©aire avec espacement √©lectrode
        n_electrodes = len(np.unique(survey_points))
        electrode_positions = np.linspace(0, (n_electrodes-1) * electrode_spacing, n_electrodes)
        
        # Cr√©er la g√©om√©trie ERT
        scheme = pg.physics.ert.createERTData(elecs=electrode_positions)
        
        # Simuler des mesures (en r√©alit√©, utiliserait les vraies donn√©es de r√©sistivit√© apparente)
        # Pour d√©monstration, cr√©er des donn√©es synth√©tiques bas√©es sur les mesures .dat
        rho_apparent = np.array(data_values)
        
        # Normaliser et ajuster les donn√©es
        rho_apparent = np.clip(rho_apparent, 0.1, 10000)  # Plage r√©aliste
        
        # Cr√©er le vecteur de donn√©es
        scheme.set('rhoa', rho_apparent)
        
        # Cr√©er le maillage d'inversion
        world = pg.createWorld(start=[electrode_positions[0], 0], 
                              end=[electrode_positions[-1], -max_depth if max_depth else -50])
        
        mesh = pg.createMesh(world, quality=34, area=0.1)
        
        # Inversion ERT
        inv = pg.physics.ert.ERTInversion(sr=False, verbose=False)
        model = inv.run(scheme, mesh)
        
        # Extraire les r√©sultats
        rho_true = model  # R√©sistivit√© vraie
        
        return {
            "success": True,
            "rho_true": rho_true,
            "mesh": mesh,
            "scheme": scheme,
            "electrode_positions": electrode_positions,
            "n_electrodes": n_electrodes
        }
        
    except Exception as e:
        return {"error": f"Erreur PyGIMLI: {str(e)}"}

def create_pygimli_sections(dat_data: dict, inversion_results: dict, 
                          output_dir: str = None) -> dict:
    """
    Cr√©e 4 coupes ERT invers√©es avec PyGIMLI
    
    Args:
        dat_data: Donn√©es du fichier .dat
        inversion_results: R√©sultats de l'inversion PyGIMLI
        output_dir: R√©pertoire de sortie pour les figures
    
    Returns:
        dict: 4 figures matplotlib des coupes invers√©es
    """
    if not PYGIMLI_AVAILABLE or "error" in inversion_results:
        return {"error": inversion_results.get("error", "PyGIMLI non disponible")}
    
    try:
        import pygimli as pg
        import matplotlib.pyplot as plt
        import numpy as np
        
        rho_true = inversion_results["rho_true"]
        mesh = inversion_results["mesh"]
        
        # Cr√©er les 4 coupes avec diff√©rentes √©chelles de couleur
        figures = {}
        
        # Coupe 1: √âchelle lin√©aire compl√®te
        fig1, ax1 = plt.subplots(figsize=(12, 8))
        pg.show(mesh, rho_true, ax=ax1, cMap='jet_r', 
                cMin=0.1, cMax=1000, logScale=True)
        ax1.set_title('Coupe 1: R√©sistivit√© vraie - √âchelle compl√®te (log)', 
                     fontsize=14, fontweight='bold')
        ax1.set_xlabel('Distance (m)')
        ax1.set_ylabel('Profondeur (m)')
        plt.colorbar(ax1.images[0], ax=ax1, label='R√©sistivit√© (Œ©¬∑m)')
        plt.tight_layout()
        figures['section_1_full_scale'] = fig1
        
        # Coupe 2: Focus sur r√©sistivit√©s basses (conductrices)
        fig2, ax2 = plt.subplots(figsize=(12, 8))
        pg.show(mesh, rho_true, ax=ax2, cMap='Reds', 
                cMin=0.1, cMax=10, logScale=True)
        ax2.set_title('Coupe 2: Zone conductrice (0.1-10 Œ©¬∑m)', 
                     fontsize=14, fontweight='bold')
        ax2.set_xlabel('Distance (m)')
        ax2.set_ylabel('Profondeur (m)')
        plt.colorbar(ax2.images[0], ax=ax2, label='R√©sistivit√© (Œ©¬∑m)')
        plt.tight_layout()
        figures['section_2_conductive'] = fig2
        
        # Coupe 3: Focus sur r√©sistivit√©s moyennes (aquif√®res)
        fig3, ax3 = plt.subplots(figsize=(12, 8))
        pg.show(mesh, rho_true, ax=ax3, cMap='YlGnBu', 
                cMin=10, cMax=100, logScale=False)
        ax3.set_title('Coupe 3: Zone aquif√®re (10-100 Œ©¬∑m)', 
                     fontsize=14, fontweight='bold')
        ax3.set_xlabel('Distance (m)')
        ax3.set_ylabel('Profondeur (m)')
        plt.colorbar(ax3.images[0], ax=ax3, label='R√©sistivit√© (Œ©¬∑m)')
        plt.tight_layout()
        figures['section_3_aquifer'] = fig3
        
        # Coupe 4: Focus sur r√©sistivit√©s √©lev√©es (roches)
        fig4, ax4 = plt.subplots(figsize=(12, 8))
        pg.show(mesh, rho_true, ax=ax4, cMap='Blues', 
                cMin=100, cMax=10000, logScale=True)
        ax4.set_title('Coupe 4: Substrat rocheux (100-10000 Œ©¬∑m)', 
                     fontsize=14, fontweight='bold')
        ax4.set_xlabel('Distance (m)')
        ax4.set_ylabel('Profondeur (m)')
        plt.colorbar(ax4.images[0], ax=ax4, label='R√©sistivit√© (Œ©¬∑m)')
        plt.tight_layout()
        figures['section_4_resistive'] = fig4
        
        return figures
        
    except Exception as e:
        return {"error": f"Erreur cr√©ation coupes: {str(e)}"}

def pygimli_ert_analysis(dat_file_path: str, electrode_spacing: float = 1.0) -> dict:
    """
    Analyse ERT compl√®te avec PyGIMLI pour fichier .dat
    
    Args:
        dat_file_path: Chemin vers le fichier .dat
        electrode_spacing: Espacement entre √©lectrodes (m)
    
    Returns:
        dict: R√©sultats complets avec inversion et visualisations
    """
    try:
        # Charger les donn√©es du fichier .dat
        from multi_freq_ert_parser import MultiFreqERTParser
        parser = MultiFreqERTParser()
        dat_data = parser.parse_file(dat_file_path)
        
        if not dat_data:
            return {"error": "Impossible de parser le fichier .dat"}
        
        # Ex√©cuter l'inversion PyGIMLI
        inversion_results = run_pygimli_inversion(dat_data, electrode_spacing)
        
        if "error" in inversion_results:
            return inversion_results
        
        # Cr√©er les 4 coupes
        sections = create_pygimli_sections(dat_data, inversion_results)
        
        if "error" in sections:
            return sections
        
        return {
            "success": True,
            "dat_data": dat_data,
            "inversion_results": inversion_results,
            "sections": sections,
            "electrode_spacing": electrode_spacing
        }
        
    except Exception as e:
        return {"error": f"Erreur analyse PyGIMLI: {str(e)}"}