#!/usr/bin/env python3
"""
Test Backend de la G√©n√©ration d'Interpr√©tation LLM
Test direct sans Streamlit pour v√©rifier les performances
"""

import os
import sys
import time
import numpy as np

# Configuration des chemins
SETRAF_BASE_PATH = os.path.dirname(os.path.abspath(__file__))
MISTRAL_MODEL_PATH = os.path.join(SETRAF_BASE_PATH, "models/mistral-7b")

print("=" * 70)
print("üß™ TEST BACKEND - G√©n√©ration d'Interpr√©tation LLM")
print("=" * 70)
print()

# V√©rifier que le mod√®le existe
if not os.path.exists(MISTRAL_MODEL_PATH):
    print(f"‚ùå ERREUR : Mod√®le Mistral introuvable dans : {MISTRAL_MODEL_PATH}")
    print("üìÅ Contenu du dossier SETRAF :")
    for item in os.listdir(SETRAF_BASE_PATH):
        print(f"   - {item}")
    sys.exit(1)

print(f"‚úÖ Mod√®le trouv√© : {MISTRAL_MODEL_PATH}")
print()

# √âtape 1 : Charger le LLM
print("ü§ñ √âTAPE 1 : Chargement du LLM Mistral...")
print("-" * 70)

try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
    import torch
    
    start_load = time.time()
    
    # Charger le tokenizer
    print("üìù Chargement du tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MISTRAL_MODEL_PATH,
        local_files_only=True,
        trust_remote_code=True,
        use_fast=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("   ‚úÖ Tokenizer charg√©")
    
    # Optimisations CPU
    print("‚öôÔ∏è  Configuration CPU...")
    torch.set_num_threads(6)
    torch.set_grad_enabled(False)
    print(f"   ‚úÖ Threads CPU : {torch.get_num_threads()}")
    
    # Charger le mod√®le
    print("üîÑ Chargement du mod√®le (cela peut prendre 30-60s)...")
    model = AutoModelForCausalLM.from_pretrained(
        MISTRAL_MODEL_PATH,
        local_files_only=True,
        torch_dtype=torch.float32,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    model = model.to('cpu')
    model.eval()
    print("   ‚úÖ Mod√®le charg√© et pr√™t")
    
    # Cr√©er le pipeline
    print("üîó Cr√©ation du pipeline...")
    llm_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=-1,
        framework="pt",
        batch_size=1
    )
    
    elapsed_load = time.time() - start_load
    print(f"   ‚úÖ Pipeline cr√©√© en {elapsed_load:.1f}s")
    print()
    
except Exception as e:
    print(f"‚ùå ERREUR lors du chargement : {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# √âtape 2 : Pr√©parer les donn√©es de test
print("üìä √âTAPE 2 : Pr√©paration des donn√©es de test")
print("-" * 70)

# Donn√©es g√©ophysiques simul√©es
geophysical_data = {
    'n_spectra': 150000,
    'rho_min': 12.5,
    'rho_max': 850.0,
    'rho_mean': 125.7,
    'rho_std': 85.3,
    'n_imputed': 2500,
    'imputation_method': 'KNN',
    'n_cells': 48000,
    'convergence': 'Optimal (5 iterations)',
    'n_trajectories': 15,
    'avg_ransac_score': 0.87
}

print("Donn√©es de test :")
for key, value in geophysical_data.items():
    print(f"   ‚Ä¢ {key}: {value}")
print()

# √âtape 3 : G√©n√©rer l'interpr√©tation
print("üß† √âTAPE 3 : G√©n√©ration de l'interpr√©tation")
print("-" * 70)

# Pr√©parer le contexte
n_spectra_display = f"{geophysical_data['n_spectra']/1000:.1f}K"
rho_min = geophysical_data['rho_min']
rho_max = geophysical_data['rho_max']
rho_mean = geophysical_data['rho_mean']

if rho_mean < 100:
    geo_type = "argiles/marnes satur√©es"
elif rho_mean < 300:
    geo_type = "sols mixtes argilo-sableux"
elif rho_mean < 600:
    geo_type = "sables/graviers semi-satur√©s"
else:
    geo_type = "roches consolid√©es/substratum"

context = f"""[INST] G√©ophysicien ERT. Analyse EXPRESS en 150 mots max:

DATA: {n_spectra_display} mesures, œÅ={rho_min:.0f}-{rho_max:.0f} Œ©¬∑m (moy:{rho_mean:.0f}), {geo_type}, {geophysical_data.get('n_trajectories', 0)} structures

R√âPONDS EN 3 SECTIONS COURTES:
1. G√âOLOGIE (2 phrases): Nature sous-sol?
2. ACTIONS (2 points): Que faire?
3. IMAGE (1 phrase): Description coupe g√©ologique

Sois BREF et PR√âCIS. [/INST]"""

print("üìù Prompt pr√©par√© :")
print(context)
print()

# G√©n√©rer avec timeout
print("‚è±Ô∏è  Lancement de la g√©n√©ration (timeout: 45s)...")
print("    (Attendez 15-30 secondes pour une g√©n√©ration rapide)")
print()

from concurrent.futures import ThreadPoolExecutor, TimeoutError

def run_inference():
    with torch.inference_mode():
        return llm_pipeline(
            context, 
            max_new_tokens=128,
            do_sample=True,
            temperature=0.7,
            top_p=0.85,
            num_return_sequences=1,
            pad_token_id=llm_pipeline.tokenizer.eos_token_id,
            repetition_penalty=1.15
        )

start_gen = time.time()
response = None

try:
    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(run_inference)
        
        # Afficher la progression
        for i in range(45):
            try:
                response = future.result(timeout=1.0)
                break
            except TimeoutError:
                elapsed = time.time() - start_gen
                print(f"\r    ‚è≥ G√©n√©ration en cours... {elapsed:.0f}s", end='', flush=True)
        
        if response is None:
            print(f"\n    ‚è±Ô∏è  TIMEOUT apr√®s 45 secondes")
            response = None
            
except Exception as e:
    print(f"\n    ‚ùå Erreur : {e}")
    response = None

elapsed_gen = time.time() - start_gen
print()

# √âtape 4 : Analyser le r√©sultat
print()
print("üìä √âTAPE 4 : Analyse du r√©sultat")
print("-" * 70)

if response and len(response) > 0:
    print(f"‚úÖ SUCC√àS ! G√©n√©ration termin√©e en {elapsed_gen:.1f}s")
    print()
    
    generated_text = response[0]['generated_text']
    
    # Extraire la r√©ponse
    if '[/INST]' in generated_text:
        generated_text = generated_text.split('[/INST]')[-1].strip()
    
    print("üéØ INTERPR√âTATION G√âN√âR√âE :")
    print("=" * 70)
    print(generated_text)
    print("=" * 70)
    print()
    
    # Parser les sections
    lines = generated_text.split('\n')
    interpretation = ""
    recommendations = ""
    image_prompt = ""
    current_section = None
    
    for line in lines:
        line_upper = line.upper()
        if 'G√âOLOGIE' in line_upper or 'GEOLOGIE' in line_upper or '1.' in line:
            current_section = 'interp'
        elif 'ACTIONS' in line_upper or 'RECOMMANDATION' in line_upper or '2.' in line:
            current_section = 'reco'
        elif 'PROMPT' in line_upper or 'IMAGE' in line_upper or '3.' in line:
            current_section = 'prompt'
        elif line.strip() and current_section:
            if current_section == 'interp':
                interpretation += line.strip() + " "
            elif current_section == 'reco':
                recommendations += line.strip() + " "
            elif current_section == 'prompt':
                image_prompt += line.strip() + " "
    
    print("üìå Sections extraites :")
    print(f"   ‚Ä¢ Interpr√©tation : {len(interpretation)} caract√®res")
    print(f"   ‚Ä¢ Recommandations : {len(recommendations)} caract√®res")
    print(f"   ‚Ä¢ Prompt image : {len(image_prompt)} caract√®res")
    print()
    
    if interpretation:
        print("üî¨ INTERPR√âTATION :")
        print(interpretation.strip())
        print()
    
    if recommendations:
        print("üéØ RECOMMANDATIONS :")
        print(recommendations.strip())
        print()
    
    if image_prompt:
        print("üñºÔ∏è  PROMPT IMAGE :")
        print(image_prompt.strip())
        print()
    
else:
    print(f"‚ö†Ô∏è  √âCHEC ou TIMEOUT apr√®s {elapsed_gen:.1f}s")
    print()
    print("üìã G√©n√©ration d'un fallback automatique...")
    
    fallback_interp = f"""Analyse g√©ologique automatique bas√©e sur {n_spectra_display} mesures:
    
‚Ä¢ Plage de r√©sistivit√© : {rho_min:.1f} - {rho_max:.1f} Œ©¬∑m (moyenne: {rho_mean:.1f} Œ©¬∑m)
‚Ä¢ {geophysical_data['n_trajectories']} structures g√©ologiques d√©tect√©es
‚Ä¢ Mod√®le 3D construit avec {geophysical_data['n_cells']} cellules

Interpr√©tation simplifi√©e :
{"- Formations argileuses dominantes" if rho_mean < 100 else "- Formations sablo-graveleuses" if rho_mean < 500 else "- Substrat rocheux consolid√©"}
"""
    
    print("üî¨ FALLBACK G√âN√âR√â :")
    print(fallback_interp)
    print()

# R√©sum√© final
print()
print("=" * 70)
print("üìà R√âSUM√â DU TEST")
print("=" * 70)
print(f"‚è±Ô∏è  Temps chargement LLM : {elapsed_load:.1f}s")
print(f"‚è±Ô∏è  Temps g√©n√©ration : {elapsed_gen:.1f}s")
print(f"‚úÖ Statut : {'SUCC√àS' if response else 'TIMEOUT/FALLBACK'}")
print()

if elapsed_gen < 30:
    print("üéâ EXCELLENT ! G√©n√©ration tr√®s rapide (< 30s)")
elif elapsed_gen < 45:
    print("‚úÖ BON ! G√©n√©ration acceptable (< 45s)")
else:
    print("‚ö†Ô∏è  LENT ! G√©n√©ration > 45s (timeout activ√©)")

print()
print("=" * 70)
print("‚úÖ Test termin√© !")
print("=" * 70)
